{"subject": "", "question": "Question: Consider an RSA encryption where the $(p, q)$ are determined as $(53, 61)$. True or false: $(e,d) = (123, 79)$ are valid encoding/decoding exponents.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: To determine if $(e,d) = (123, 79)$ are valid encoding/decoding exponents for RSA encryption with $(p, q) = (53, 61)$, we need to check if the values of $e$ and $d$ satisfy the conditions for RSA encryption.\n\nIn RSA encryption, $e$ is the public exponent and $d$ is the private exponent. These are related by the condition $e \\cdot d \\equiv 1 \\pmod{\\phi(n)}$, where $\\phi(n) = (p-1)(q-1)$.\n\nGiven $p = 53$ and $q = 61$, we have $n = p \\cdot q = 53 \\cdot 61 = 3233$, and $\\phi(n) = (p-1)(q-1) = 52 \\cdot 60 = 3120$.\n\nNow, we can check if $e \\cdot d \\equiv 1 \\pmod{3120}$:\n$123 \\cdot 79 = 97017 \\equiv 1297 \\pmod{3120}$.\n\nSince $1297 \\neq 1$, the condition for valid RSA exponents is not satisfied. Therefore, the correct answer is Option 1: False. ///Answer: A"}
{"subject": "", "question": "Question: Our task is to classify whether an animal is a dog (class 0) or a cat (class 1) based on the following features: \n        \begin{itemize}\n\t        \\item $x_1$: height\n\t        \\item $x_2$: length of whiskers\n\t        \\item $x_3$: thickness of fur\n        \\end{itemize}\n        We perform standard normal scaling on the training features so that they have a mean of zero and standard deviation of 1. We have trained a Logistic Regression model to determine the probability that the animal is a cat, $p(1 | \\mathbf{x,w})$.\n        Our classifier learns that cats have a lower height and longer whiskers than dogs, while the thickness of fur is not relevant to the classification outcome. Which of the following is true about the weights~$\\wv$ learned by the classifier?\n        ?\n\nOptions:\nA. $w_1 < w_2 < w_3$\nB. $w_1 < w_3 < w_2$\nC. $w_2 < w_1 < w_3$\nD. $w_2 < w_3 < w_1$\nE. $w_3 < w_1 < w_2$\nF. $w_3 < w_2 < w_1$\n", "answer": "Explanations: Given that cats have a lower height and longer whiskers than dogs, we can infer that the weight corresponding to height ($w_1$) should be negative (as it is inversely related to the class) and the weight corresponding to the length of whiskers ($w_2$) should be positive (as cats have longer whiskers compared to dogs). Since the thickness of fur is not relevant to the classification outcome, the weight corresponding to this feature ($w_3$) should ideally be close to zero.\n\nTherefore, the correct order of the weights learned by the classifier should be:\n\n$w_1 < w_3 < w_2$\n\nSo, the correct statement is: $w_1 < w_3 < w_2$. ///Answer: E."}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. In order to have zero-knowledge from $\\Sigma$-protocols, we need to add the use of \\ldots?\n\nOptions:\nA. \\ldots an ephemeral key $h$ and a Pedersen commitment.\nB. \\ldots a common reference string.\nC. \\ldots hash functions.\nD. \\ldots none of the above is necessary, zero-knowledge is already contained in $\\Sigma$-protocols.\n", "answer": "Explanations: To determine the false assertion in the given question regarding zero-knowledge from $\\Sigma$-protocols, let's first understand the concept of zero-knowledge proofs and how they relate to $\\Sigma$-protocols.\n\n**Concepts:**\n\n1. **Zero-Knowledge Proofs:** Zero-knowledge proofs are cryptographic protocols in which a prover can convince a verifier of the truth of a statement without revealing any additional information beyond the validity of the statement itself. In a zero-knowledge proof, the verifier learns nothing about the secret other than the fact that the prover knows it.\n\n2. **$\\Sigma$-protocols:** $\\Sigma$-protocols are a type of cryptographic protocol used for proving knowledge of a witness to a statement without revealing the witness itself. They are often used in the construction of zero-knowledge proofs.\n\n**Analysis:**\n\nIn the context of the question, the false assertion is asking about the necessity of additional elements beyond $\\Sigma$-protocols for achieving zero-knowledge. Let's analyze each proposition provided:\n\n1. **An ephemeral key $h$ and a Pedersen commitment:** In some zero-knowledge protocols, ephemeral keys and Pedersen commitments may be used to enhance security or efficiency, but they are not necessary for achieving zero-knowledge from $\\Sigma$-protocols. Zero-knowledge can be achieved using $\\Sigma$-protocols alone.\n\n2. **A common reference string:** Common reference strings are used in certain cryptographic constructions to generate parameters shared between parties. While they can be beneficial in certain scenarios, they are not required for achieving zero-knowledge from $\\Sigma$-protocols. Zero-knowledge can be achieved without a common reference string.\n\n3. **Hash functions:** Hash functions are commonly used in cryptographic protocols for various purposes, including commitment schemes and proof constructions. However, they are not a strict requirement for achieving zero-knowledge from $\\Sigma$-protocols. Zero-knowledge can be achieved without the explicit use of hash functions.\n\n4. **None of the above is necessary, zero-knowledge is already contained in $\\Sigma$-protocols:** This proposition correctly states that zero-knowledge is inherent in $\\Sigma$-protocols themselves. $\\Sigma$-protocols are designed to provide zero-knowledge proofs, and additional elements like ephemeral keys, Pedersen commitments, common reference strings, or hash functions are not strictly necessary for achieving zero-knowledge.\n\n**Conclusion:**\n\nThe false assertion is that additional elements beyond $\\Sigma$-protocols are necessary for achieving zero-knowledge. In reality, zero-knowledge is already contained within $\\Sigma$-protocols, making the statement \"None of the above is necessary, zero-knowledge is already contained in $\\Sigma$-protocols\" the false assertion in this context. ///Answer: D"}
{"subject": "", "question": "Question: The Factoring Problem is equivalent to ...\n\nOptions:\nA. the Computing Square Roots Problem.\nB. the RSA Decryption Problem.\nC. the Discrete Logarithm Problem.\nD. the Computing Generator Problem.\n", "answer": "Explanations: The Factoring Problem is equivalent to:\n- the RSA Decryption Problem.\n\nExplanation: The Factoring Problem and the RSA Decryption Problem are mathematically equivalent because the security of the RSA encryption scheme relies on the difficulty of factoring large composite numbers. If one can efficiently factorize the product of two large prime numbers, they can break the RSA encryption and decrypt the message. This equivalence has significant implications for cryptography and the security of communication systems. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following is wrong regarding Ontologies?\n\nOptions:\nA. We can create more than one ontology that conceptualize the same real-world entities\nB. Ontologies help in the integration of data expressed in different models\nC. Ontologies support domain-specific vocabularies\nD. Ontologies dictate how semi-structured data are serialized\n", "answer": "Explanations: \n{'message_id': 29243, 'chat_id': 10869, 'content': 'Step 1: Understanding the concept of Ontologies\\nOntologies are formal representations of knowledge that define the concepts, relationships, and properties within a specific domain. They are used to structure information and \nprovide a common understanding of a particular subject area.\\n\\nStep 2: Analyzing the statements\\n1. \"We can create more than one ontology that conceptualizes the same real-world entities\" - This statement is correct. Different ontologies can be created to represent the same real-world entities based on different perspectives or requirements.\\n2. \"Ontologies help in the integration of data expressed in different models\" - This statement is correct. Ontologies facilitate data integration by providing a common vocabulary and structure for representing information from diverse sources.\\n3. \"Ontologies support domain-specific vocabularies\" - This statement is correct. Ontologies are designed to support domain-specific terminologies and vocabularies to ensure consistency and interoperability within a particular field.\\n4. \"Ontologies dictate how semi-structured data are serialized\" - This statement is wrong. Ontologies do not dictate how data is serialized; instead, they focus on defining the concepts and relationships within a domain.\\n\\nStep 3: Conclusion\\nAll the statements provided regarding \nontologies are correct except for the statement that says \"Ontologies dictate how semi-structured data are serialized.\" Ontologies play a crucial role in knowledge representation, data integration, and supporting domain-specific vocabularies, but they do not determine how data is serialized.' ///Answer: D"}
{"subject": "", "question": "Question: Consider the exhaustive search of a uniformly distributed key in a set of size $N$. What is the complexity of the exhaustive search with time memory trade-off? (We assume that memory access is constant.)?\n\nOptions:\nA. precomputation: $O(N^{2/3})$, memory: $O(N^{2/3})$, time: $O(N^{2/3})$.\nB. precomputation: $O(N)$, memory: $O(N^{2/3})$, time: $O(N^{2/3})$.\nC. precomputation: $O(1)$, memory: $O(N^{2/3})$, time: $O(N^{2/3})$.\nD. precomputation: $O(N)$, memory: $O(1)$, time: $O(N^{2/3})$.\n", "answer": "Explanations: The concept of time-memory trade-off is a common strategy in computer science to optimize algorithms by using more memory to reduce the time complexity of an operation. In the context of exhaustive search with a uniformly distributed key in a set of size $N$, the trade-off involves precomputing data to reduce the time complexity of the search operation.\n\nIn this scenario, the exhaustive search operation without any precomputation would have a time complexity of $O(N)$, as it would need to check each key in the set one by one until finding the desired key.\n\nBy applying precomputation to store some additional information in memory, we can reduce the time complexity of the search operation. The precomputation involves calculating and storing some data before the search begins to speed up the search process.\n\nGiven that memory access is constant, we can use precomputation to achieve a time complexity of $O(N^{2/3})$ with a memory complexity of $O(N^{2/3})$. This trade-off means that by using more memory to precompute certain values, we can significantly reduce the time it takes to perform the search operation.\n\nTherefore, the correct option from the choices provided is:\nprecomputation: $O(N^{2/3})$, memory: $O(N^{2/3})$, time: $O(N^{2/3})$. ///Answer: A"}
{"subject": "", "question": "Question: Graph coloring is \\ldots?\n\nOptions:\nA. \\ldots $NP$-hard with 1 color.\nB. \\ldots not interesting for cryptographers.\nC. \\ldots an $NP$-complete problem when limited to 3 colors.\nD. \\ldots always possible with 2 colors.\n", "answer": "Explanations: - $NP$-hard with 1 color: Incorrect. Graph coloring with 1 color is trivial and not $NP$-hard.\n- Not interesting for cryptographers: Incorrect. Graph coloring is fundamental in cryptography and computer science.\n- An $NP$-complete problem when limited to 3 colors: Correct. Graph coloring is $NP$-complete when limited to 3 colors.\n- Always possible with 2 colors: Incorrect. Some graphs require more than 2 colors for proper coloring.\n\nThe correct option is that graph coloring is an $NP$-complete problem when limited to 3 colors. ///Answer: C"}
{"subject": "", "question": "Question: Let $G_1, G_2$, be valid generator matrices of dimensions $\\mathbb F^{k_i \times n_i}$, all over the same field $\\mathbb F$. Recall that ``valid'' means that for all $i$, $k_i \\leq n_i$ and $\text{rank}(G_i) = k_i$. True or false:  Assuming $k_1 = k_2$ and $n_1=n_2$, the matrix $G_{7} + G_{8}$ is also a valid generator matrix.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: Given that $G_1$ and $G_2$ are valid generator matrices with dimensions $\\mathbb F^{k_i \\times n_i}$, where $k_i \\leq n_i$ and $\\text{rank}(G_i) = k_i$, and assuming $k_1 = k_2$ and $n_1 = n_2$, we are asked whether the matrix $G_1 + G_2$ is also a valid generator matrix.\n\nTo determine the validity of $G_1 + G_2$ as a generator matrix, we need to consider the properties of generator matrices. \n\nA valid generator matrix is a matrix that has full row rank and contains linearly independent rows. In this case, since $G_1$ and $G_2$ are both valid generator matrices, their sum $G_1 + G_2$ will also have full row rank, as the sum of two matrices cannot decrease the rank.\n\nAdditionally, because $k_1 = k_2$ and $n_1 = n_2$, the number of rows and columns in $G_1$ is the same as in $G_2$, and the sum $G_1 + G_2$ will have the same dimensions.\n\nTherefore, the matrix $G_1 + G_2$ will have full row rank and the same dimensions as $G_1$ and $G_2$, making it a valid generator matrix.\n\nThe correct answer is: {'Correct Answer(s)': 2. True}. ///Answer: B"}
{"subject": "", "question": "Question: When using linear regression, what can cause numerical instabilities? (One or multiple answers)?\n\nOptions:\nA. learning rate too small\nB. degeneracies in the features\nC. too much training data\nD. too little training data\n", "answer": "Explanations: The possible causes of numerical instabilities when using linear regression are:\n\n1. Degeneracies in the features: When the features are highly correlated or redundant, it can lead to numerical instabilities in the model. This is known as multicollinearity, and it can make the matrix inversion process unstable.\n\n2. Learning rate too small: If the learning rate in the gradient descent optimization algorithm is too small, it can lead to slow convergence or even getting stuck in local minima. This can cause numerical instabilities in the training process.\n\nTherefore, the correct answers are:\n- Degeneracies in the features\n- Learning rate too small ///Answer: A"}
{"subject": "", "question": "Question: In which of the following groups is the decisional Diffie-Hellman problem (DDH) believed to be hard?\n\nOptions:\nA. In $\\mathbb{Z}_p$, with a large prime $p$.\nB. In large subgroup of smooth order of a ``regular'' elliptic curve.\nC. In a large subgroup of prime order of $\\mathbb{Z}_p^*$, such that $p$ is a large prime.\nD. In $\\mathbb{Z}_p^*$, with a large prime $p$.\n", "answer": "Explanations: First, let's briefly explain what the Decisional Diffie-Hellman problem (DDH) is. In the DDH problem, given three elements $g, g^a, g^b$ where $a$ and $b$ are randomly chosen from a finite group, the task is to determine whether $g^{ab}$ was computed using $g^a$ and $g^b$. In other words, it asks whether a certain value was derived from a specific pair of values in the group.\n\nNow, let's analyze each of the given options:\n\n1. In $\\mathbb{Z}_p$, with a large prime $p$: \n   This option refers to the group of integers modulo a prime $p$. The DDH problem is believed to be hard in this group because of the difficulty in computing discrete logarithms modulo a large prime.\n\n2. In large subgroup of smooth order of a \"regular\" elliptic curve: \n   Elliptic curve groups are commonly used in cryptographic protocols due to their security properties. The DDH problem is believed to be hard in large subgroups of elliptic curves because of the complexity of solving the discrete logarithm problem on elliptic curves.\n\n3. In a large subgroup of prime order of $\\mathbb{Z}_p^*$, such that $p$ is a large prime: \n   This option refers to the multiplicative group of integers modulo a prime $p$. The DDH problem is believed to be hard in large subgroups of this group because of the difficulty of solving the discrete logarithm problem modulo a large prime.\n\n4. In $\\mathbb{Z}_p^*$, with a large prime $p: \n   This option is similar to option 3, as it also refers to the multiplicative group of integers modulo a prime $p$. The DDH problem is believed to be hard in this group for the same reasons as mentioned in option 3.\n\nIn conclusion, the Decisional Diffie-Hellman problem (DDH) is believed to be hard in all of the given groups: \n- In $\\mathbb{Z}_p$, with a large prime $p.\n- In large subgroup of smooth order of a \"regular\" elliptic curve.\n- In a large subgroup of prime order of $\\mathbb{Z}_p^*$, such that $p$ is a large prime.\n- In $\\mathbb{Z}_p^*$, with a large prime $p. \n\nThis is due to the computational complexity involved in solving the discrete logarithm problem in these groups, making it difficult for an adversary to distinguish between different pairs of elements. ///Answer: A"}
{"subject": "", "question": "Question: Let $n \\geq 2$ be a positive integer, and $M$ a uniformly distributed binary message of length $2n$. Let $P_K(M)$ denote the one-time pad encryption of $M$ with key $K$. Let $K_1$ be a uniformly distributed binary key length $n$. Let $K_2$ be the complement of $K_1$. Let $K_3$ be the reverse of $K_1$. Let $K_i||K_j$ denote the concatenation of the two keys. True or false: Encryption with the key $K_4 = ( K_1|| K_1 ) $, $P_{K_4}(M)$ provides perfect secrecy.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: Let's analyze the encryption process with the key $K_4 = (K_1||K_1)$. \n\nGiven that $K_1$ is a uniformly distributed binary key of length $n$, $K_1||K_1$ is essentially repeating $K_1$ twice. \n\nWhen encrypting the message $M$ using the one-time pad encryption with $K_4$, we would essentially be XOR-ing $M$ with $K_1||K_1$. \n\nSince $K_1$ is repeated twice to form $K_4$, this means that the key used to encrypt the first half of the message is the same as the key used to encrypt the second half of the message. Therefore, the XOR of the first half of the message with the key is the same as the XOR of the second half of the message with the same key.\n\nThis repetition in the key leads to a lack of perfect secrecy because an attacker can exploit the redundancy in the key structure to potentially recover the original message. \n\nTherefore, the statement that encryption with the key $K_4 = (K_1||K_1)$ provides perfect secrecy is: \n\nA: False ///Answer: A."}
{"subject": "", "question": "Question: What is the mean squared error of $f$ for a sample, where $\\textbf{x}$ is an input, $y$ a target and $f(\\textbf{x},W)$ the mapping function  ? \n(One answer)?\n\nOptions:\nA.  $||y - f(\\textbf{x},W)||^2 $ \nB.  $||y - f(\\textbf{x},W)|| $\nC.  $-\\log(P(y=i | \\textbf{x})) = -\\log(\\frac{e^{\\textbf{f}_i(\\textbf{x},W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x},W)}})$ \nD.  $P(y=i |\\textbf{x}) = \\frac{e^{\\textbf{f}_i(\\textbf{x},W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x},W)}}$ \n", "answer": "Explanations: Certainly! Let's break down the answer step by step:\n\n1. Mean Squared Error (MSE) Formula:\nThe mean squared error of $f$ for a sample is calculated using the formula $||y - f(\\textbf{x}, W)||^2$. This formula represents the squared Euclidean distance between the target value $y$ and the predicted value $f(\\textbf{x}, W)$ for the given input $\\textbf{x}$.\n\n2. Explanation of Symbols:\n- $||y - f(\\textbf{x}, W)||^2$: This notation denotes the squared Euclidean norm (or magnitude) of the vector $y - f(\\textbf{x}, W)$. It calculates the squared difference between the target value $y$ and the predicted value $f(\\textbf{x}, W)$.\n- $||y - f(\\textbf{x}, W)||$: This represents the Euclidean norm of the vector $y - f(\\textbf{x}, W)$, which calculates the distance between the target value $y$ and the predicted value $f(\\textbf{x}, W)$. It is not squared, unlike in the MSE formula.\n- $-\\log(P(y=i | \\textbf{x})) = -\\log(\\frac{e^{\\textbf{f}_i(\\textbf{x}, W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x}, W)}})$: This equation is related to the negative log-likelihood in the context of classification tasks, where $P(y=i | \\textbf{x})$ represents the probability that the target class is $i$ given the input $\\textbf{x}$.\n- $P(y=i |\\textbf{x}) = \\frac{e^{\\textbf{f}_i(\\textbf{x}, W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x}, W)}}$: This equation defines the softmax function, which calculates the probability that the target class is $i$ given the input $\\textbf{x}$ based on the scores $\\textbf{f}(\\textbf{x}, W)$ obtained from a model.\n\nBy using the MSE formula, we can evaluate the performance of a regression model like a generalized linear model or a deep neural network by comparing the predicted values to the actual target values for a given sample. ///Answer: A"}
{"subject": "", "question": "Question: When using bootstrapping in Random Forests, the number of different data items used to construct a single tree is:?\n\nOptions:\nA. smaller than the size of the training data set, with high probability\nB. of order square root of the size of the training set, with high probability\nC. the same as the size of the training data set\nD. subject to the outcome of the sampling process, and can be both smaller or larger than the training set\n", "answer": "Explanations: The correct answer is: of order square root of the size of the training set, with high probability\n\nIn Random Forests, bootstrapping involves sampling the training data set with replacement. This means that each tree is constructed using a subset of the training data. The typical practice is to use around the square root of the total number of data items in the training set. This helps to introduce randomness and diversity into each tree, which is crucial for the overall performance of the Random Forest model. ///Answer: B"}
{"subject": "", "question": "Question: Consider a classification problem on linearly separable data. We train an SVM model and a logistic regression model. For logistic regression (LR) we add a small regularization term (penalty on weights) in order to make the optimum well-defined. Each model gives us a margin. Consider a datapoint $\\mathbf{x}_{0}$ that is correctly classified and strictly outside both margins Which one of the following statements is incorrect ?\n\nOptions:\nA. There exists a direction in which we can slightly move $\\mathbf{x}_{0}$ without changing the LR decision boundary after retraining.\nB. $\\mathbf{x}_{0}$ isn't a support vector\nC. There exists a direction in which we can arbitrarily move $\\mathbf{x}_{0}$ without changing the SVM decision boundary after retraining.\nD. If we remove $\\mathbf{x}_{0}$ from the dataset and retrain, this will change the LR decision boundary.\nE. If we remove $\\mathbf{x}_{0}$ from the dataset and retrain, this will not change the SVM decision boundary.\n", "answer": "Explanations: The incorrect statement is: There exists a direction in which we can slightly move $\\mathbf{x}_{0}$ without changing the LR decision boundary after retraining.\n\nExplanation:\nFor logistic regression with a small regularization term, moving a data point outside the margin will likely affect the decision boundary during retraining, as the regularization term penalizes large weights. Thus, moving $\\mathbf{x}_{0}$ even slightly can lead to a change in the LR decision boundary. This is in contrast to SVM, where as long as the data point remains outside the margin, small movements will not change the decision boundary due to the margin maximization objective. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is true for Generative Adversarial Networks (GANs) but not Diffusion Models:\n        ?\n\nOptions:\nA. They can generate new data from input noise.\nB. They use two different models during training.\nC. They use a simple L2 loss function.\nD. They gradually remove noise from inputs using a Markov chain.\n", "answer": "Explanations: First, let's break down the characteristics of both Generative Adversarial Networks (GANs) and Diffusion Models:\n\n1. Generative Adversarial Networks (GANs):\n- They can generate new data from input noise.\n- They use two different models during training.\n- They do not use a simple L2 loss function.\n- They do not gradually remove noise from inputs using a Markov chain.\n\n2. Diffusion Models:\n- They do not generate new data from input noise.\n- They do not use two different models during training.\n- They use a simple L2 loss function.\n- They gradually remove noise from inputs using a Markov chain.\n\nBased on the characteristics listed above for both GANs and Diffusion Models, the unique characteristic that is true for Generative Adversarial Networks (GANs) but not for Diffusion Models is: \"They can generate new data from input noise.\"\n\nTherefore, the correct answer is: \"They can generate new data from input noise.\" ///Answer: A"}
{"subject": "", "question": "Question: In order to have security of $2^{128}$ against key recovery attack for a (secure) symmetric key encryption scheme \\ldots?\n\nOptions:\nA. we should use DES!\nB. the shortest secret key that we can use is $128$ bits long.\nC. the shortest secret key that we can use is $64$ bits long.\nD. the longest secret key that we can use is $256$ bits long.\n", "answer": "Explanations: The correct answer is: 'the shortest secret key that we can use is $128$ bits long.'\n\nExplanation:\n\nIn order to have security of $2^{128}$ against key recovery attacks, the key length should be at least 128 bits long. This is because the security of a symmetric key encryption scheme is typically measured in terms of the number of possible keys that an attacker would have to try in order to recover the key. In this case, $2^{128}$ represents the number of possible keys that an attacker would have to try in order to break the encryption.\n\nUsing a key length shorter than 128 bits would not provide the desired level of security, as an attacker could potentially recover the key by trying all possible keys within a reasonable amount of time. On the other hand, using a key length longer than 128 bits would provide even more security, but it may not be necessary depending on the specific requirements of the application.\n\nTherefore, the shortest secret key that we can use to achieve security of $2^{128}$ against key recovery attacks is 128 bits long. ///Answer: B"}
{"subject": "", "question": "Question: Consider an IR system using a Vector Space model with Okapi BM25 as the weighting scheme (with \\(k=1.5\\) and \\(b=0.75\\)) and operating on a document collection that contains:a document \\(d_1\\), andand a document \\(d_3\\) corresponding to the concatenation of 3 copies of \\(d_1\\).Indicate which of the following statements are true, where \\(\\langle d\\rangle\\) stands for the vector representing document \\(d\\):(Penalty for wrong ticks.)?\n\nOptions:\nA. The cosine similarity between \\(\\langle d_3\\rangle\\) and \\(\\langle d_1\\rangle\\) is equal to 1.\nB. Each component of \\(\\langle d_3\\rangle\\) is strictly larger than the corresponding one in \\(\\langle d_1\\rangle\\).\nC. Each component of \\(\\langle d_3\\rangle\\) is strictly smaller than the corresponding one in \\(\\langle d_1\\rangle\\).\nD. Indexing terms with small term frequency are favored in \\(\\langle d_3\\rangle\\) (w.r.t. \\(\\langle d_1\\rangle\\)).\nE. Indexing terms with large term frequency are favored in \\(\\langle d_3\\rangle\\) (w.r.t. \\(\\langle d_1\\rangle\\)).\n", "answer": "Explanations: A) In the Okapi BM25 weighting scheme with \\(k=1.5\\) and \\(b=0.75\\), the term frequency normalization factor and document length normalization factor help in adjusting the term weights based on the document length. In this case, document \\(d_3\\) is the concatenation of 3 copies of document \\(d_1\\). Since \\(d_3\\) contains 3 times the content of \\(d_1\\), the term frequency of each term in \\(d_3\\) will be 3 times that of \\(d_1\\). However, the document length normalization factor will counterbalance this increase in term frequency.\n\n1) The cosine similarity between \\(\\langle d_3\\rangle\\) and \\(\\langle d_1\\rangle\\) is not equal to 1. The cosine similarity is a measure of similarity between two vectors and it depends on the magnitude of the vectors as well. In this case, the cosine similarity between \\(\\langle d_3\\rangle\\) and \\(\\langle d_1\\rangle\\) will be less than 1 due to the normalization factors.\n\n2) Each component of \\(\\langle d_3\\rangle\\) is not strictly larger than the corresponding one in \\(\\langle d_1\\rangle\\). The term weights in \\(\\langle d_3\\rangle\\) will be adjusted based on the normalization factors, and some weights may be larger while others may be smaller.\n\n3) Each component of \\(\\langle d_3\\rangle\\) is not strictly smaller than the corresponding one in \\(\\langle d_1\\rangle\\). Similar to the previous point, the term weights in \\(\\langle d_3\\rangle\\) will vary based on the normalization factors and may not be strictly smaller.\n\n4) Indexing terms with small term frequency are not favored in \\(\\langle d_3\\rangle\\) (w.r.t. \\(\\langle d_1\\rangle\\)). False. In the Okapi BM25 scheme, terms with higher term frequency are favored as the term frequency component in the BM25 formula increases with higher term frequency.\n\n5) Indexing terms with large term frequency are favored in \\(\\langle d_3\\rangle\\) (w.r.t. \\(\\langle d_1\\rangle\\)). True. Since \\(d_3\\) contains 3 copies of \\(d_1\\), the term frequency of terms in \\(d_3\\) will be higher, and therefore indexing terms with large term frequency are favored in \\(\\langle d_3\\rangle\\) compared to \\(\\langle d_1\\rangle\\). ///Answer: E"}
{"subject": "", "question": "Question: Is Java as a programming language considered memory safe? Why?\n(note: excluding Java Native Interface, the use of \\texttt{sun.misc.unsafe}, or bugs in the JVM)?\n\nOptions:\nA. Yes, because the JVM guarantees spatial and temporal memory safety at all times.\nB. Yes, because you can't access objects by reference in Java.\nC. No, there are Java programs vulnerable to use after free.\nD. No, there are Java programs vulnerable to type confusion.\n", "answer": "Explanations: Yes, because the JVM guarantees spatial and temporal memory safety at all times.  Java as a programming language is considered memory safe because it provides automatic memory management through garbage collection, which helps prevent common memory-related errors such as memory leaks and dangling pointers. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion. The ambiguity issue in the decryption algorithm of the Rabin cryptosystem can be solved by\\dots?\n\nOptions:\nA. encrypting the message twice.\nB. encrypting the message appended to itself.\nC. appending some integrity checks to the message before encryption.\nD. ensuring that the other possible plaintexts make no sense.\n", "answer": "Explanations: To determine the false assertion among the options provided regarding the ambiguity issue in the decryption algorithm of the Rabin cryptosystem, let's analyze each option:\n\n1. \\textbf{Encrypting the message twice:} Encrypting the message twice does not inherently resolve the ambiguity issue in the decryption algorithm of the Rabin cryptosystem. In fact, this may introduce further complications and does not directly address the core problem of ambiguity.\n\n2. \\textbf{Encrypting the message appended to itself:} This method does not effectively resolve the ambiguity issue in the decryption algorithm. It might even exacerbate the problem by potentially increasing the number of possible plaintexts that could match a given ciphertext.\n\n3. \\textbf{Appending some integrity checks to the message before encryption:} Adding integrity checks to the message before encryption can help ensure the validity and integrity of the message but does not directly address the ambiguity issue in the decryption algorithm of the Rabin cryptosystem.\n\n4. \\textbf{Ensuring that the other possible plaintexts make no sense:} This approach is a common technique to mitigate ambiguity in the Rabin cryptosystem. By ensuring that the other possible plaintexts generated during decryption do not make logical or contextual sense, the correct plaintext can be identified reliably.\n\nTherefore, option 1, \"Encrypting the message twice,\" is the false assertion as it does not provide a direct solution to the ambiguity issue in the decryption algorithm of the Rabin cryptosystem. ///Answer: A"}
{"subject": "", "question": "Question: Thang, Jeremie and Tugrulcan have built their own search engines. For a query Q, they got precision scores of 0.6, 0.7, 0.8 respectively. Their F1 scores (calculated by same parameters) are same. Whose search engine has a higher recall on Q?\n\nOptions:\nA. Thang\nB. Jeremie\nC. Tugrulcan\nD. We need more information\n", "answer": "Explanations: To determine whose search engine has a higher recall on query Q, we need to consider the relationship between precision, recall, and the F1 score.\n\nThe F1 score is the harmonic mean of precision and recall, given by the formula:\n\\[ F1 = 2 * \\frac{precision * recall}{precision + recall} \\]\n\nSince the F1 scores for Thang, Jeremie, and Tugrulcan are the same, it implies that the product of precision and recall is the same for all three. This means that if one search engine has a higher precision, it must have a lower recall to keep the product constant.\n\nGiven that Thang has the lowest precision score (0.6) among the three, it is likely that Thang has a higher recall compared to Jeremie (precision 0.7) and Tugrulcan (precision 0.8).\n\nTherefore, the correct answer is:\n\nA. Thang ///Answer: A"}
{"subject": "", "question": "Question: The worst case complexity of an exaustive search (with memory) against DES is\\dots?\n\nOptions:\nA. $1$\nB. $\\frac{2^{64}}{2}$\nC. $2^{56}$\nD. $2^{64}$\n", "answer": "Explanations: Answer: \n\nC) $2^{56}$\n\nExplanation: \n\n1. In an exhaustive search against DES, we try all possible keys to decrypt a ciphertext until we find the correct key. \n2. DES has a key size of 56 bits (even though it uses a 64-bit key, but 8 bits are used for parity, so effectively 56 bits are used for encryption).\n3. The worst-case complexity of an exhaustive search with memory is $2^{n-1}$, where $n$ is the number of bits in the key.\n4. Substituting $n = 56$ into the formula, we get $2^{56-1} = 2^{55} = 2^{56}$ possible keys that need to be tried in the worst-case scenario to find the correct key. \n5. Therefore, the worst-case complexity of an exhaustive search against DES is $2^{56}$ possible keys. \n6. Hence, the correct answer is C) $2^{56}$. ///Answer: C"}
{"subject": "", "question": "Question: (Neural networks) Training only the first layer of a deep neural network using the logistic loss is equivalent to training a logistic regression over a transformed feature space.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: True.\n\nTraining only the first layer of a deep neural network using the logistic loss is equivalent to training a logistic regression over a transformed feature space. This is because the first layer of a neural network without any activation function is essentially a linear transformation of the input features. When using logistic loss for training, the output of the first layer can be directly interpreted as the logits for logistic regression over the transformed features.\n\nTherefore, by training only the first layer with logistic loss, we are essentially learning a logistic regression model over the transformed feature space, which can be seen as equivalent to training a logistic regression over the original feature space. ///Answer: A. True"}
{"subject": "", "question": "Question: Tick the \\textbf{correct} assertion. Let $G$ be a cyclic multiplicative group of order $101$, let $g$ be a generator of $G$, and let $e_G$ be the neutral element of $G$.\n\nOptions:\nA. For any $a\\in G$, the order of $a$ divides the order of $g$.\nB. There is no $a\\in G$ such that \\textsf{order}(a) = |G|.\nC. There is an $a \\in G$ such that $a^{101} \\neq e_G$.\nD. $\\left\\{ e_G \\right\\}$ and $G$ are not the only subgroups of $G$.\n", "answer": "Explanations: The search results provide information on the properties of cyclic multiplicative groups, which will help us determine the correctness of each assertion. The search results give us a better understanding of how the order of elements in cyclic multiplicative groups relates to the order of the generator. The search results provide information on the subgroups of cyclic multiplicative groups, which will help us determine the correctness of the last assertion. The correct assertions are: ///Answer: C."}
{"subject": "", "question": "Question: Consider the following CF grammar \\(G_1\\)\n\n\\( R_1: \\text{S} \\rightarrow \\text{NP VP} \\)\n\\( R_2: \\text{S} \\rightarrow \\text{NP VP PNP} \\)\n\\( R_3: \\text{PNP} \\rightarrow \\text{Prep NP} \\)\n\\( R_4: \\text{NP} \\rightarrow \\text{N} \\)\n\\( R_5: \\text{NP} \\rightarrow \\text{Det N} \\)\n\\( R_6: \\text{NP} \\rightarrow \\text{Det N PNP} \\)\n\\( R_7: \\text{VP} \\rightarrow \\text{V} \\)\n\\( R_8: \\text{VP} \\rightarrow \\text{V NP} \\)\n(where \\(\\text{Det}\\), \\(\\text{N}\\), \\(\\text{Prep}\\) and \\(\\text{V}\\) are the only pre-terminals), complemented by an adequate lexicon \\(L_1\\).If the sequence \\((p_1, p_2, \\dots, p_8)\\) represents a set of probabilistic coefficients for the syntactic rules in \\(G_1\\) (\\(p_i\\) being associated to \\(R_i\\)), indicate which of the following choices correspond to a valid probabilistic extension for the grammar \\(G_1\\).\n(Penalty for wrong ticks.)?\n\nOptions:\nA. (1.00, 0.00, 1.00, 0.00, 1.00, 0.00, 1.00, 0.00)\nB. (0.55, 0.45, 0.60, 0.10, 0.15, 0.75, 0.50, 0.50)\nC. (0.35, 0.65, 1.00, 0.30, 0.25, 0.45, 0.25, 0.75)\nD. I cannot answer because it also depends on the probabilistic coefficients associated to the lexical rules.\nE. None of the other proposed answers.\n", "answer": "Explanations: To determine which choice corresponds to a valid probabilistic extension for the grammar \\(G_1\\), we need to consider the probabilities associated with the rules in the grammar.\n\nLooking at the rules in \\(G_1\\), we can see that \\(R_1\\), \\(R_2\\), \\(R_3\\), \\(R_4\\), \\(R_5\\), \\(R_6\\), \\(R_7\\), and \\(R_8\\) are given probabilities in the choices. Let's analyze each choice:\n\n- (1.00, 0.00, 1.00, 0.00, 1.00, 0.00, 1.00, 0.00)\nThis choice assigns a probability of 0.00 to \\(R_2\\), which is not valid because the rule \\(R_2\\) should have a non-zero probability since it is part of the grammar.\n\n- (0.55, 0.45, 0.60, 0.10, 0.15, 0.75, 0.50, 0.50)\nThis choice assigns probabilities to all rules, but we need to ensure that the probabilities sum up to 1 for each non-terminal. Let's calculate the sums:\n- S: 0.55 + 0.45 = 1.00\n- PNP: 0.60\n- NP: 0.10 + 0.15 + 0.75 = 1.00\n- VP: 0.50 + 0.50 = 1.00\nThe probabilities sum up correctly for each non-terminal, so this choice is valid.\n\n- (0.35, 0.65, 1.00, 0.30, 0.25, 0.45, 0.25, 0.75)\nThis choice also assigns probabilities to all rules, but let's check the sums:\n- S: 0.35 + 0.65 = 1.00\n- PNP: 1.00\n- NP: 0.30 + 0.25 + 0.45 = 1.00\n- VP: 0.25 + 0.75 = 1.00\nThe probabilities sum up correctly for each non-terminal, so this choice is also valid.\n\nTherefore, the valid choices for a probabilistic extension of the grammar \\(G_1\\) are:\n- (0.55, 0.45, 0.60, 0.10, 0.15, 0.75, 0.50, 0.50)\n- (0.35, 0.65, 1.00, 0.30, 0.25, 0.45, 0.25, 0.75) ///Answer: B"}
{"subject": "", "question": "Question: Consider the following context-free grammar \\(G\\) (where \\(\\text{S}\\) is the top-level symbol):\n\n\\(R_{01}: \\text{S} \\rightarrow \\text{NP VP}\\)\n\\(R_{02}: \\text{NP} \\rightarrow \\text{NP0}\\)\n\\(R_{03}: \\text{NP} \\rightarrow \\text{Det NP0}\\)\n\\(R_{04}: \\text{NP0} \\rightarrow \\text{N}\\)\n\\(R_{05}: \\text{NP0} \\rightarrow \\text{Adj N}\\)\n\\(R_{06}: \\text{NP0} \\rightarrow \\text{NP0 PNP}\\)\n\\(R_{07}: \\text{VP} \\rightarrow \\text{V}\\)\n\\(R_{08}: \\text{VP} \\rightarrow \\text{V NP}\\)\n\\(R_{09}: \\text{VP} \\rightarrow \\text{V NP PNP}\\)\n\\(R_{10}: \\text{PNP} \\rightarrow \\text{Prep NP}\\)\n\ncomplemented by the lexicon \\(L\\):\na        : Det\nblue     : Adj, N\ndrink    : N, V\ndrinks   : N, V\nfriends  : N\nfrom     : Prep\ngave     : V\nletter   : N\nmy       : Det\nneighbor : N\nnice     : Adj, N\nof       : Prep\npostman  : N\nran      : V\nthe      : Det\nto       : PrepHow many (syntactic and lexical) rules does the extended Chomsky Normal Form grammar equivalent to \\(G\\) contain, if produced as described in the parsing lecture?\n\nOptions:\nA. the grammar \\(G\\) cannot be converted to extended Chomsky Normal Form\nB. the grammar \\(G\\) already is in extended Chomsky Normal Form\nC. 11 rules\nD. 31 rules\nE. 48 rules\n", "answer": "Explanations: To convert the given Context-Free Grammar (CFG) \\(G\\) into Extended Chomsky Normal Form (ECNF), we need to follow certain rules. \n\nIn the Chomsky Normal Form (CNF), each production rule is of the form \\(A \\rightarrow BC\\) or \\(A \\rightarrow a\\), where \\(A, B, C\\) are non-terminal symbols, and \\(a\\) is a terminal symbol. In the Extended Chomsky Normal Form (ECNF), we also allow rules of the form \\(A \\rightarrow \\epsilon\\), where \\(\\epsilon\\) represents the empty string.\n\nGiven the CFG \\(G\\), we need to convert it to ECNF. Let's analyze each production rule:\n\n1. We see that the rules are already in the form required for CNF. They are either \\(A \\rightarrow BC\\) or \\(A \\rightarrow a\\).\n\n2. There are no rules of the form \\(A \\rightarrow \\epsilon\\), so we do not need to add any additional rules for this.\n\nNow let's count the total number of rules in the given CFG \\(G\\) to determine the number of rules in the ECNF equivalent:\n\n- We have 10 syntactic rules (from \\(R_{01}\\) to \\(R_{10}\\)).\n- We have 23 lexical rules in the lexicon \\(L\\) (each word corresponds to a lexical rule).\n\nTherefore, the total number of rules in the extended Chomsky Normal Form equivalent to \\(G\\) would be \\(10 + 23 = 33\\) rules.\n\nSince none of the provided answer options match the calculated value, it seems that the correct answer might be missing from the options. The closest option is \"31 rules,\" but based on the analysis provided, the correct answer should be 33 rules. ///Answer: D."}
{"subject": "", "question": "Question: Modularity clustering will end up always with a single community at the top level?\n\nOptions:\nA. true\nB. Only for dense graphs\nC. Only for connected graphs\nD. never\n", "answer": "Explanations: Modularity clustering aims to maximize the modularity of the network by partitioning it into communities. In some cases, especially with dense graphs, modularity clustering may end up with a single community at the top level. This is because the algorithm may struggle to find meaningful community structures in densely connected graphs, leading to a single large community as the optimal solution.\nOnly for dense graphs ///Answer: B"}
{"subject": "", "question": "Question: Given the following teleporting matrix (\u0395) for nodes A, B and C:[0\u00a0\u00a0\u00a0 \u00bd\u00a0\u00a0\u00a0 0][0\u00a0\u00a0\u00a0\u00a0 0\u00a0\u00a0\u00a0 0][0\u00a0\u00a0\u00a0 \u00bd\u00a0\u00a0\u00a0 1]and making no assumptions about the link matrix (R), which of the following is correct:(Reminder: columns are the probabilities to leave the respective node.)?\n\nOptions:\nA. A random walker can never reach node A\nB. A random walker can never leave node A\nC. A random walker can always leave node C\nD. A random walker can always leave node B\n", "answer": "Explanations: 3. A random walker can always leave node C\n\nExplanation: The probabilities for leaving node C in the teleporting matrix are [0, 1/2, 1]. This indicates that there is always a probability of 1 for the random walker to leave node C, regardless of their current location. Therefore, it is correct to say that a random walker can always leave node C. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following mitigation(s) can prevent reliable exploitation of\n    the following vulnerable code snippet?\n\n    \\begin{lstlisting}[language=C,style=c]\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid win(void) {\n    system(\"/bin/sh\");\n}\n\nint main(int argc, char *argv[]) {\n    char buf[128];\n    scanf(\"%126[^\\n]\", buf); // Reads up to 126 chars up until newline into buf\n    printf(buf);\n    return 0;\n}\n    \\end{lstlisting}?\n\nOptions:\nA. ASLR for position-independent executables\nB. Fortify source\nC. DEP\nD. Stack canaries\nE. Safe Exception Handling (SEH)\n", "answer": "Explanations: The vulnerable code snippet provided is susceptible to a format string vulnerability. This vulnerability occurs when user input is directly passed to a formatting function like `printf` without proper validation, allowing an attacker to execute arbitrary code by providing format specifiers in the input.\n\nTo prevent reliable exploitation of this vulnerable code snippet, one of the key mitigations that can be applied is **Stack canaries**.\n\nExplanation:\n1. **ASLR for position-independent executables**: Address Space Layout Randomization (ASLR) randomizes the memory layout of a program to make it harder for attackers to predict the location of specific code or data. While ASLR can help in mitigating certain types of exploits, it may not directly prevent the exploitation of the format string vulnerability in the given code snippet.\n\n2. **Fortify source**: Fortify source is a set of compiler features and runtime libraries that offer protection against certain types of vulnerabilities like buffer overflows and format string vulnerabilities. While Fortify source can help in detecting and preventing some vulnerabilities, it may not directly address the format string vulnerability in the provided code.\n\n3. **DEP (Data Execution Prevention)**: DEP helps prevent certain types of exploits by marking certain areas of memory as non-executable. While DEP can help in preventing the execution of injected code in certain scenarios, it may not directly prevent the exploitation of the format string vulnerability in the given code snippet.\n\n4. **Stack canaries**: Stack canaries involve placing a random value before the return address on the stack and checking if this value has been modified when a function returns. If the canary value has been modified, it indicates a buffer overflow or stack corruption, and the program can terminate to prevent exploitation. Stack canaries can help prevent buffer overflow attacks, including the exploitation of vulnerabilities like the format string vulnerability in the provided code snippet.\n\n5. **Safe Exception Handling (SEH)**: Safe Exception Handling is a feature in Windows that helps in handling exceptions and structured exception handling. While SEH can be useful for handling exceptions in Windows programs, it may not directly prevent the exploitation of the format string vulnerability in the given code snippet.\n\nIn conclusion, among the mitigation options provided, **Stack canaries** would be the most effective in preventing reliable exploitation of the format string vulnerability present in the vulnerable code snippet. ///Answer: D"}
{"subject": "", "question": "Question: For any function $f:\\{0,1\\}^p\\rightarrow \\{0,1\\}^q$ and for any $a\\in\\{0,1\\}^p$, we have\\ldots?\n\nOptions:\nA. $\\Sigma _{b\\in \\{0,1\\}^q}\\mathsf{DP}^f(a,b)=1$\nB. $\\Sigma _{b\\in \\{0,1\\}^q}\\mathsf{DP}^f(a,b)=0$\nC. $\\Sigma _{b\\in \\{0,1\\}^q}\\mathsf{DP}^f(a,b)=\\frac{1}{2}$\nD. $\\Sigma _{b\\in \\{0,1\\}^q}\\mathsf{DP}^f(a,b)=\\frac{1}{\\sqrt{2}}$\n", "answer": "Explanations: The correct answer is:\n- $\\Sigma _{b\\in \\{0,1\\}^q}\\mathsf{DP}^f(a,b)=\\frac{1}{2}$\n\nExplanation:\nThe term $\\mathsf{DP}^f(a,b)$ represents the statistical difference between two probability distributions. In this case, it measures the difference between the distribution of outputs when the input is $a$ and the distribution of all possible outputs $b$ when using function $f$.\n\nWhen summing up the statistical differences for all possible outputs $b\\in \\{0,1\\}^q$, we expect the total difference to be equal to $\\frac{1}{2}$ due to the properties of probability distributions. This value indicates that the two distributions are maximally different. ///Answer: C"}
{"subject": "", "question": "Question: Consider a source $S$ with some distribution $P_S$ over the alphabet $\\mathcal{A} = \\{a, b, c, d, e, f\\}$. Bob designs a uniquely decodable code $\\Gamma$ over a code alphabet $\\mathcal{D}$ of size $D$ with following codeword lengths. \begin{center}\begin{tabular}{ |c|c|c|c|c|c|c| } \\hline& $a$ & $b$ & $c$ & $d$ & $e$ & $f$ \\ \\hline$|\\Gamma(\\cdot)|$ & 1 & 1 & 2 & 2 & 3 & 3 \\ \\hline\\end{tabular}\\end{center} Which of the following is true?\n\nOptions:\nA. The average codeword length of the code is necessarily greater than or equal to $H_D(S)$.\nB. $\\Gamma$ can be a binary code, i.e., $D$ can be $2$.\nC. There exists a distribution $P_S$ such that the average codeword length of $\\Gamma(\\cdot)$ is equal to $H_D(S)$.\n", "answer": "Explanations: Let's analyze each statement one by one:\n\n1. The average codeword length of the code is necessarily greater than or equal to $H_D(S)$:\nThe average codeword length of a uniquely decodable code is given by $\\sum_{a \\in \\mathcal{A}} P_S(a) \\cdot |\\Gamma(a)|$. In this case, the average codeword length is $1 \\cdot P_S(a) + 1 \\cdot P_S(b) + 2 \\cdot P_S(c) + 2 \\cdot P_S(d) + 3 \\cdot P_S(e) + 3 \\cdot P_S(f) = P_S(a) + P_S(b) + 2(P_S(c) + P_S(d)) + 3(P_S(e) + P_S(f))$. This average codeword length may or may not be greater than or equal to the entropy $H_D(S)$, so this statement is not necessarily true.\n\n2. $\\Gamma$ can be a binary code, i.e., $D$ can be $2$:\nFor $\\Gamma$ to be a binary code, all codeword lengths must be powers of 2. In this case, the codeword lengths are not powers of 2, so $\\Gamma$ cannot be a binary code. This statement is false.\n\n3. There exists a distribution $P_S$ such that the average codeword length of $\\Gamma(\\cdot)$ is equal to $H_D(S)$:\nThe entropy $H_D(S)$ is given by $-\\sum_{a \\in \\mathcal{A}} P_S(a) \\log_D(P_S(a))$. For the average codeword length to be equal to $H_D(S)$, we need to find a distribution $P_S$ such that the average codeword length calculated in statement 1 is equal to $H_D(S)$. Since the average codeword length depends on the distribution $P_S$, it is possible to find a distribution that satisfies this condition. This statement is true.\n\nIn conclusion, the only true statement among the three is: There exists a distribution $P_S$ such that the average codeword length of $\\Gamma(\\cdot)$ is equal to $H_D(S)$. ///Answer: C"}
{"subject": "", "question": "Question: In an interactive proof system for a language $L$, having $\\beta$-soundness means that\\dots?\n\nOptions:\nA. if we run the protocol with input $x\\not\\in L$, with a \\textbf{malicious prover}, and a \\textbf{honest verifier} the probability that the protocol succeeds is upper-bounded by $\\beta$.\nB. if we run the protocol with input $x\\in L$, with a \\textbf{malicious prover}, and a \\textbf{honest verifier} the probability that the protocol succeeds is upper-bounded by $\\beta$.\nC. if we run the protocol with input $x\\in L$, with a \\textbf{honest prover}, and a \\textbf{malicious verifier} the probability that the protocol succeeds is upper-bounded by $\\beta$.\nD. if we run the protocol with input $x\\in L$, with a \\textbf{honest prover}, and a \\textbf{honest verifier} the probability that the protocol succeeds is upper-bounded by $\\beta$.\n", "answer": "Explanations: 'To determine the correct option for the multiple-choice question regarding $\\\\beta$-soundness in an interactive proof system for a language $L, we need to understand the concept of $\\\\beta$-soundness in this context.\\n\\n1. **Option 0:** \"if we run the protocol with input $x\\\\not\\\\in L$, with a malicious prover, and a honest verifier the probability that the protocol succeeds is upper-bounded by $\\\\beta.\"\\n   - This option describes the scenario where the input is not in $L$, and a malicious prover is involved. However, $\\\\beta$-soundness typically relates to the case when the input is in $L$.\\n\\n2. **Option 1:** \"if we run the protocol with input $x\\\\in L$, with a malicious prover, and a honest verifier the probability that the protocol succeeds is upper-bounded by $\\\\beta.\"\\n   - This option seems more relevant as it considers the case where the input is in $L, and a malicious prover is involved. However, $\\\\beta$-soundness usually refers to bounding the success probability when both parties are honest.\\n\\n3. **Option 2:** \"if we run the protocol with input $x\\\\in L$, with a honest prover, and a malicious verifier the probability that the protocol succeeds is upper-bounded by $\\\\beta.\"\\n   - This option describes a scenario where a malicious verifier is involved. However, $\\\\beta$-soundness typically focuses on scenarios involving a dishonest prover.\\n\\n4. **Option 3:** \"if we run the protocol with input $x\\\\in L$, with a honest prover, and a honest verifier the probability that the protocol succeeds is upper-bounded by $\\\\beta.\"\\n   - This option aligns with the standard definition of $\\\\beta$-soundness in interactive proof systems. It states that even when both parties are honest, the success probability of the protocol is bounded by $\\\\beta.\\n\\nTherefore, **Option 3** is likely to be the correct answer for this multiple-choice question as it accurately describes $\\\\beta$-soundness in an interactive proof system for a language $L.' ///Answer: D"}
{"subject": "", "question": "Question: The output feedback (OFB) mode of operation \\dots?\n\nOptions:\nA. requires its nonce to be public and constant to be secure.\nB. requires its nonce IV to be constant but secret to be secure.\nC. requires its nonce to be new for every plaintext to be secure.\nD. doesn't require any nonce.\n", "answer": "Explanations: The output feedback (OFB) mode of operation requires its nonce to be new for every plaintext to be secure.\n\nExplanation:\n\n1. Requires its nonce to be public and constant to be secure: False. Using a public and constant nonce in OFB mode can lead to security vulnerabilities as it can result in the same keystream being generated for different plaintexts, which can lead to encryption weaknesses.\n\n2. Requires its nonce IV to be constant but secret to be secure: False. While using a constant and secret IV can provide some level of security, it is not the best practice for the OFB mode. It is recommended to use a unique and random nonce for each encryption operation.\n\n3. Requires its nonce to be new for every plaintext to be secure: True. To maintain the security of the encryption, it is essential for the nonce used in the OFB mode to be unique for each plaintext encryption. Reusing the same nonce can result in security vulnerabilities.\n\n4. Doesn't require any nonce: False. The OFB mode of operation requires the use of a nonce to ensure the security of the encryption process. Using a nonce helps in generating a unique keystream for each encryption operation.\n\nTherefore, the correct statement is: Requires its nonce to be new for every plaintext to be secure. ///Answer: C"}
{"subject": "", "question": "Question: Which sets are countable (Multiple answers can be correct) :?\n\nOptions:\nA. The set of string of finite length of first names starting with the letter P\nB. The set of real numbers containing at least 100 3's in their decimal representation\nC. The set of natural numbers containing at least one 3 in their decimal representation\nD. \\(U-C\\) with \\(U\\) an uncountable set and \\(C\\) a countable set\n", "answer": "Explanations: The correct answers are:\n- The set of string of finite length of first names starting with the letter P\n- The set of natural numbers containing at least one 3 in their decimal representation\n\nFor the first set, the set of strings of finite length of first names starting with the letter P is countable because there are only a finite number of first names that start with the letter P.\n\nFor the second set, the set of natural numbers containing at least one 3 in their decimal representation is countable because every natural number has a unique decimal representation, and there are countably many natural numbers.\n\nFor the third set, if \\(U\\) is an uncountable set and \\(C\\) is a countable set, then the set \\(U-C\\) is uncountable. This is because removing a countable set from an uncountable set still leaves an uncountable set. ///Answer: A"}
{"subject": "", "question": "Question: Consider the source $S_1, S_2, \\dots$ such that $S_1$ is uniformly distributed on $\\mathbb{Z}/10\\mathbb{Z}^*$, and for every $n\\geq 1$, $S_{n+1}$ is distributed uniformly on $\\mathbb{Z}/(S_n+1)\\mathbb{Z}^*$. Let $H(\\mathcal{S}) = \\lim_{n\to\\infty} H(S_n)$. True or false: The source is stationary.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: The source is not stationary. In a stationary source, the distribution of the random variable does not change over time. However, in this case, the distribution of $S_{n+1}$ depends on the value of $S_n$, which changes over time. Therefore, the source is not stationary. ///Answer: A"}
{"subject": "", "question": "Question: What is true regarding Fagin's algorithm?\n\nOptions:\nA. It performs a complete scan over the posting files\nB. It provably returns the k documents with the largest aggregate scores\nC. Posting files need to be indexed by TF-IDF weights\nD. It never reads more than (kn)\u00bd entries from a posting list\n", "answer": "Explanations: The correct answer is:\n\nIt never reads more than (kn)\u00bd entries from a posting list\n\nFagin's algorithm is an efficient algorithm for finding the k documents with the largest aggregate scores without performing a complete scan over the posting files. It achieves this by only reading a limited number of entries from the posting lists, specifically no more than (kn)\u00bd entries, where n is the number of documents and k is the number of documents to retrieve. The algorithm does not require the posting files to be indexed by TF-IDF weights to function. ///Answer: D"}
{"subject": "", "question": "Question: A neural network has been trained for multi-class classification using cross-entropy but has not necessarily achieved a global or local minimum on the training set.\n\tThe output of the neural network is $\\mathbf{z}=[z_1,\\ldots,z_d]^\top$ obtained from the penultimate values $\\mathbf{x}=[x_1,\\ldots,x_d]^\top$ via softmax $z_k=\frac{\\exp(x_k)}{\\sum_{i}\\exp(x_i)}$ that can be interpreted as a probability distribution over the $d$ possible classes.\n\tThe cross-entropy is given by $H(\\mathbf{y},\\mathbf{z})=-\\sum_{i=1}^{d} y_i \\ln{z_i}$ where $\\mathbf{y}$ is one-hot encoded meaning the entity corresponding to the true class is 1 and other entities are 0.\n\n\tWe now modify the neural network, either by scaling $\\mathbf{x} \\mapsto \u0007lpha \\mathbf{x}$ where $\u0007lpha \\in \\R_{>0}$ or through a shift $\\mathbf{x} \\mapsto \\mathbf{x} + b\\mathbf{1}$ where $b \\in \\R$.\n\tThe modified $\\mathbf{x}$ values are fed into the softmax to obtain the final output and the network / parameters are otherwise unchanged.\n\tHow do these transformations affect the training accuracy of the network? ?\n\nOptions:\nA. One transformation has no effect, the other one decreases the accuracy in some cases (but never increases it).\nB. One transformation has no effect, the other sometimes increases and sometimes decreases the accuracy.\nC. Neither transformation affects the accuracy.\nD. Both transformations decrease the accuracy in some cases (but never increase it).\nE. Both transformations sometimes increase and sometimes decrease the accuracy.\n", "answer": "Explanations: The correct answer is: \n- One transformation has no effect, the other one decreases the accuracy in some cases (but never increases it).\n\nExplanation:\n1. **Scaling $\\mathbf{x} \\mapsto \\alpha \\mathbf{x}$ where $\\alpha \\in \\mathbb{R}_{>0}$**: Scaling the input values by a positive factor $\\alpha$ will not change the relative probabilities output by the softmax function. The softmax function normalizes the values, so scaling them will not affect the classification decisions. Therefore, this transformation has no effect on the training accuracy.\n\n2. **Shift $\\mathbf{x} \\mapsto \\mathbf{x} + b\\mathbf{1}$ where $b \\in \\mathbb{R}$**: Shifting the input values by a constant $b$ can change the relative magnitudes of the softmax outputs. This shift can alter the decision boundaries of the network, potentially leading to incorrect classifications and decreasing the training accuracy in some cases. It's important to note that this transformation will not increase the accuracy since it can only introduce additional errors by shifting the decision boundaries.\n\nIn conclusion, while scaling the input values has no impact on the network's accuracy, shifting the values can sometimes lead to decreased accuracy due to changes in decision boundaries caused by the shift. ///Answer: A"}
{"subject": "", "question": "Question: Let \\(A = \\{a, b, c, d, ..., z\\}\\) be the set of lower cased English letters.\n\nLet \\(S = \\{a, b, ab, cd, ae, xy, ord, ...\\}\\) be the set of all strings using \\(A\\) as an alphabet.\n\nGiven \\(s\\in S\\), \\(N(s)\\) is the number of vowels in \\(s\\).\n\nFor example,\\(N(algrzqi) = 2\\), \\(N(bebebe) = 3\\).\n\nWe say \\((s, t)\\) belongs to relation \\(R\\) if \\(N(s) \\leq N(t)\\).\n\nWhich of the following statements are true (more than one answer can be correct) ?\n\nOptions:\nA. \\(R\\) is reflexive. \nB. \\(R\\) is transitive.\nC. \\(R\\) is symmetric.\nD. \\(R\\) is not an equivalence relation.\n", "answer": "Explanations: To determine the properties of relation \\(R\\), let's analyze each statement:\n\n- \\(R\\) is reflexive:\nFor a relation to be reflexive, every element in the set must be related to itself. In this case, for any string \\(s\\), \\(N(s) \\leq N(s)\\) is always true because the number of vowels in a string is always less than or equal to itself. Therefore, \\(R\\) is reflexive.\n\n- \\(R\\) is transitive:\nFor a relation to be transitive, if \\(N(s) \\leq N(t)\\) and \\(N(t) \\leq N(u)\\), then it must be the case that \\(N(s) \\leq N(u)\\). In this scenario, if the number of vowels in string \\(s\\) is less than or equal to the number of vowels in string \\(t\\), and the number of vowels in string \\(t\\) is less than or equal to the number of vowels in string \\(u\\), then it follows that the number of vowels in string \\(s\\) is less than or equal to the number of vowels in string \\(u\\). Therefore, \\(R\\) is transitive.\n\n- \\(R\\) is symmetric:\nFor a relation to be symmetric, if \\(N(s) \\leq N(t)\\), then it must also be true that \\(N(t) \\leq N(s)\\). In this case, it is not necessarily true that if the number of vowels in string \\(s\\) is less than or equal to the number of vowels in string \\(t\\), then the number of vowels in string \\(t\\) is less than or equal to the number of vowels in string \\(s\\). Therefore, \\(R\\) is not symmetric.\n\n- \\(R\\) is not an equivalence relation:\nFor a relation to be an equivalence relation, it must be reflexive, transitive, and symmetric. We have already established that \\(R\\) is reflexive and transitive, but it is not symmetric. Since \\(R\\) does not satisfy all the properties required for an equivalence relation, it is not an equivalence relation.\n\nTherefore, the correct statements are:\n- \\(R\\) is reflexive.\n- \\(R\\) is transitive.\n- \\(R\\) is not an equivalence relation. ///Answer: A"}
{"subject": "", "question": "Question: Let $f_{\\mathrm{MLP}}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be an $L$-hidden layer multi-layer perceptron (MLP) such that $$ f_{\\mathrm{MLP}}(\\mathbf{x})=\\mathbf{w}^{\\top} \\sigma\\left(\\mathbf{W}_{L} \\sigma\\left(\\mathbf{W}_{L-1} \\ldots \\sigma\\left(\\mathbf{W}_{1} \\mathbf{x}\\right)\\right)\\right) $$ with $\\mathbf{w} \\in \\mathbb{R}^{M}, \\mathbf{W}_{1} \\in \\mathbb{R}^{M \\times d}$ and $\\mathbf{W}_{\\ell} \\in \\mathbb{R}^{M \\times M}$ for $\\ell=2, \\ldots, L$, and $\\sigma$ is an entry-wise activation function. Also, let $f_{\\mathrm{CNN}}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ be an $L^{\\prime}$-hidden layer convolutional neural network (CNN) such that $$ f_{\\mathrm{CNN}}(\\mathbf{x})=\\mathbf{w}^{\\top} \\sigma\\left(\\mathbf{w}_{L^{\\prime}} \\star \\sigma\\left(\\mathbf{w}_{L^{\\prime}-1} \\star \\ldots \\sigma\\left(\\mathbf{w}_{1} \\star \\mathbf{x}\\right)\\right)\\right) $$ with $\\mathbf{w} \\in \\mathbb{R}^{d}, \\mathbf{w}_{\\ell} \\in \\mathbb{R}^{K}$ for $\\ell=1, \\ldots, L^{\\prime}$ and $\\star$ denoting the one-dimensional convolution operator with zero-padding, i.e., output of the convolution has the same dimensionality as the input. Let's assume $\\sigma$ is a tanh activation function. Thus, by flipping the signs of all of the weights leading in and out of a hidden neuron, the input-output mapping function represented by the network is unchanged. Besides, interchanging the values of all of the weights (i.e., by permuting the ordering of the hidden neurons within the layer) also leaves the network input-output mapping function unchanged. Suppose that, given the training data, SGD can find a solution with zero training loss, and the (absolute value) weights of such solution are non-zero and all unique. Choose the largest lower bound on the number of solutions (with zero training loss) achievable by $f_{\\mathrm{MLP}}$ with $L=1$ and $M$ hidden units on this dataset.\n\nOptions:\nA. $M! 2^M$\nB. $1$\nC. $2^M$\nD. $M !$\n", "answer": "Explanations: The largest lower bound on the number of solutions achievable by $f_{\\mathrm{MLP}}$ with $L=1$ and $M$ hidden units on this dataset is $M!$. This is because changing the signs of the weights leading in and out of a hidden neuron, as well as permuting the ordering of hidden neurons within the layer, results in different weight configurations that represent the same input-output mapping function. With unique non-zero weights in an MLP with $M$ hidden units, there are $M!$ possible weight permutations that do not alter the network's function, providing a lower bound on the number of solutions with zero training loss. This principle arises from the network's symmetry properties and the nature of weight permutations in a single hidden layer MLP. ///Answer: D"}
{"subject": "", "question": "Question: Does the following code snippet contain bugs? If so, which line(s)?\n\n\\begin{lstlisting}[language=C,style=c]\nvoid echo() {\n    char buf[100];\n    scanf(\"%s\", buf);\n    printf(buf);\n}\n\\end{lstlisting}?\n\nOptions:\nA. Line 3 has a bug / vuln\nB. Line 4 has a bug / vuln\nC. There is no bug / vuln in the code snippet\nD. Line 1 has a bug / vuln\nE. Line 2 has a bug / vuln\nF. Line 5 has a bug / vuln\n", "answer": "Explanations: Yes, the code snippet contains a bug. The bug is in the `printf` statement. \n\nThe `printf` function in C interprets the format string to format and print the arguments passed to it. In this case, the format string is coming directly from user input stored in the `buf` array. This is dangerous as it can lead to a format string vulnerability.\n\nA format string vulnerability occurs when an attacker can control the format string parameter of a function that performs formatting (like `printf`). An attacker can exploit this vulnerability to read or write arbitrary memory, which can lead to serious security issues.\n\nTo fix this bug, you should use `printf` with a format specifier that specifies how to format the input data. For example, you could use `%s` to print a string:\n\n```c\nprintf(\"%s\", buf);\n```\n\nBy using the correct format specifier, you ensure that the user input is treated as a string and not as a format string, thereby preventing format string vulnerabilities. ///Answer: F"}
{"subject": "", "question": "Question: Let $n$ be an integer. Tick the \\emph{true} assertion about the Miller-Rabin Primality Test.\n\nOptions:\nA. If the algorithms outputs $prime$, then $n$ is definitely a prime.\nB. If the algorithms outputs $composite$, then $n$ is definitely \\emph{not} a prime.\nC. The test can be used to factorize $n$ if it is composite.\nD. The test always outputs $prime$ if $n$ is a Carmichael number.\n", "answer": "Explanations: The true assertion about the Miller-Rabin Primality Test is: \"If the algorithm outputs 'composite', then $n$ is definitely not a prime number.\" The test is probabilistic, so a \"prime\" output indicates likelihood of primality, but a \"composite\" output guarantees non-primality. It is not used for factorization and may still output \"composite\" for Carmichael numbers. ///Answer: B"}
{"subject": "", "question": "Question: Consider a regression task. You are using your favorite learning algorithm with parameters w and add a regularization term of the form $\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$. Which of the following statements are correct for a typical scenario?\n\nOptions:\nA. The training error as a function of $\\lambda \\geq 0$ decreases.\nB. The training error as a function of $\\lambda \\geq 0$ increases.\nC. The test error as a function of $\\lambda \\geq 0$ increases.\nD. The test error as a function of $\\lambda \\geq 0$ decreases.\nE. The training error as a function of $\\lambda \\geq 0$ first decreases and then increases.\nF. The test error as a function of $\\lambda \\geq 0$ first decreases and then increases.\n", "answer": "Explanations: When adding a regularization term $\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$ to a regression task, increasing $\\lambda$ leads to a decrease in model complexity and helps prevent overfitting. Here's the analysis:\n\n1. Option 1: The training error as a function of $\\lambda \\geq 0$ decreases.\n- This is incorrect. As $\\lambda$ increases, the model's complexity decreases, potentially leading to higher training error.\n\n2. Option 2: The training error as a function of $\\lambda \\geq 0$ increases.\n- This is correct. Increasing $\\lambda$ can lead to higher training error due to reduced model complexity.\n\n3. Option 3: The test error as a function of $\\lambda \\geq 0$ increases.\n- This is incorrect. Regularization helps in generalization and can lead to lower test error.\n\n4. Option 4: The test error as a function of $\\lambda \\geq 0$ decreases.\n- This is correct. Regularization helps in generalization and can lead to lower test error.\n\n5. Option 5: The training error as a function of $\\lambda \\geq 0$ first decreases and then increases.\n- This is correct. Initially, the training error may decrease as model complexity reduces, but too much regularization can lead to underfitting and higher training error.\n\n6. Option 6: The test error as a function of $\\lambda \\geq 0$ first decreases and then increases.\n- This is incorrect. Test error typically decreases with increasing $\\lambda$ as regularization helps in generalization.\n\nTherefore, the correct options are:\n- The training error as a function of $\\lambda \\geq 0$ increases.\n- The test error as a function of $\\lambda \\geq 0$ decreases.\n- The training error as a function of $\\lambda \\geq 0$ first decreases and then increases.\n\nThe correct options are 2 and 4. ///Answer: B:"}
{"subject": "", "question": "Question: An ontology ?\n\nOptions:\nA. helps to separate layout issues from the structural representation of data\nB. provides a common syntactic framework to represent standardized domain models\nC. can be used as a mediation framework for integrating semantically heterogeneous databases\n", "answer": "Explanations: An ontology is a formal representation of knowledge within a domain, typically describing the concepts, relationships, and properties relevant to that domain. Let's analyze the given options:\n\n1: An ontology helps to separate layout issues from the structural representation of data:\nThis statement is not directly related to the primary purpose of an ontology. Ontologies are more about defining the semantics of a domain rather than dealing with layout issues.\n\n2: An ontology provides a common syntactic framework to represent standardized domain models:\nThis statement is partially correct. An ontology does provide a common framework, not just syntactic but also semantic, to represent standardized domain models. It focuses more on the semantics rather than just the syntax.\n\n3: An ontology can be used as a mediation framework for integrating semantically heterogeneous databases:\nThis statement is correct. Ontologies can indeed serve as a mediation framework for integrating databases with different schemas by providing a common understanding of the domain concepts.\n\nTherefore, the correct option is:\n\n3: An ontology can be used as a mediation framework for integrating semantically heterogeneous databases. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement. GSM anonymity is broken by ...\n\nOptions:\nA. replaying an A3 challenge.\nB. desynchronizing the mobile station and the network.\nC. using A5/1.\nD. sniffing the very first connection protocol.\n", "answer": "Explanations: To solve the problem, we need to identify the \\textbf{false} statement among the given options for breaking GSM anonymity.\n\nGiven options:\n1. Sniffing the very first connection protocol\n2. Desynchronizing the mobile station and the network\n3. Using A5/1\n4. Replaying an A3 challenge\n\nStep-by-step solution:\n\n1. Sniffing the very first connection protocol: This is a known method for breaking GSM anonymity. Attackers can intercept the initial communication between the mobile station and the network to gather information and compromise anonymity. This statement is \\textbf{true}.\n\n2. Desynchronizing the mobile station and the network: Desynchronizing the mobile station and the network can disrupt communication but may not necessarily break GSM anonymity. This statement could potentially be the \\textbf{false} statement we are looking for.\n\n3. Using A5/1: A5/1 is a weak encryption algorithm used in GSM networks. Exploiting the vulnerabilities in A5/1 can lead to breaking GSM anonymity. This statement is \\textbf{true}.\n\n4. Replaying an A3 challenge: Replaying an A3 challenge is a known attack vector in GSM networks where an attacker intercepts and replays authentication challenges to gain unauthorized access. This statement is also \\textbf{true}.\n\nTherefore, the \\textbf{false} statement among the options is likely to be \"Desynchronizing the mobile station and the network.\" This is because desynchronization may disrupt communication but may not directly result in breaking GSM anonymity, unlike the other options which are known methods for compromising anonymity.\n\nBy systematically analyzing each option and considering the implications of the techniques described, we can identify the false statement based on our understanding of GSM network security and anonymity. ///Answer: B"}
{"subject": "", "question": "Question: Under which condition is an element $x\\in \\mathbb{Z}_n$ invertible?\n\nOptions:\nA. $\\mathsf{gcd}(x,\\varphi (n)) = 1$.\nB. $\\mathsf{gcd}(x,n-1) = 1$.\nC. $\\mathsf{gcd}(x,n) = 1$.\nD. $\\mathsf{gcd}(x,n) \\ne 1$.\n", "answer": "Explanations: Another condition for an element $x \\in \\mathbb{Z}_n$ to be invertible is that $\\gcd(x, n) = 1$. This condition is based on the fact that for an element to have a multiplicative inverse modulo $n$, it must be coprime to $n$. If $\\gcd(x, n) \\neq 1$, then $x$ will not have an inverse in $\\mathbb{Z}_n$. ///Answer: C"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. Factoring?\n\nOptions:\nA. is a hard problem.\nB. can be solved in polynomial time on a standard computer using Shor's algorithm.\nC. is an easy problem if we have a fast algorithm for computing an order of an element.\nD. is an easy problem if we have a fast algorithm for computing $\\varphi(n)$.\n", "answer": "Explanations: The incorrect statement is: \"can be solved in polynomial time on a standard computer using Shor's algorithm.\"\n\nExplanation:\n- Factoring is a hard problem: Factoring large numbers into their prime factors is considered a hard problem, especially for large numbers with hundreds of digits.\n- Factoring can be solved in polynomial time on a standard computer using Shor's algorithm: Shor's algorithm is a quantum algorithm that can efficiently factor large numbers into their prime factors, but it requires a quantum computer to run effectively. It is not currently feasible to solve factoring in polynomial time on a standard computer using Shor's algorithm.\n- Factoring is an easy problem if we have a fast algorithm for computing an order of an element: This statement refers to the fact that factoring can be easier if we have efficient algorithms for certain related problems, such as computing the order of an element in a group.\n- Factoring is an easy problem if we have a fast algorithm for computing $\\\\varphi(n)$: This statement refers to the fact that factoring can be easier if we have efficient algorithms for computing Euler's totient function $\\\\varphi(n)$, which is related to the prime factorization of n. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{wrong} assertion concerning 3G.\n\nOptions:\nA. In 3G, the network is authenticated to the phone.\nB. The integrity of 3G messages is well protected.\nC. In 3G, there is a counter to protect against replay attacks.\nD. 3G uses f8 for encryption.\n", "answer": "Explanations: Reasoning:\n1. 3G (Third Generation) mobile telecommunications technology introduced significant advancements in security features compared to earlier generations.\n2. In 3G, mutual authentication is established between the mobile device (phone) and the network to ensure both parties are legitimate.\n3. The integrity of messages in 3G is typically protected using cryptographic techniques like message authentication codes (MACs) to detect any unauthorized modifications.\n4. To prevent replay attacks, 3G networks often employ a counter mechanism that ensures messages are used only once and are not replayed by an attacker.\n5. Encryption in 3G is performed using the Kasumi algorithm, which includes both the f8 and f9 functions for encryption and integrity protection.\n '3G uses f8 for encryption.' ///Answer: D"}
{"subject": "", "question": "Question: Which of the following is correct regarding prediction models?\n\nOptions:\nA. A high bias is a sign of overfitting.\nB. A high variance is a sign of underfitting.\nC. In low data regime, complex models tend to perform better.\nD. Simple models have higher bias than complex models.\n", "answer": "Explanations: To determine which statement regarding prediction models is correct, let's break down each option one by one:\n\n1. A high bias is a sign of overfitting: This statement is incorrect. High bias is a sign of underfitting, not overfitting. When a model has high bias, it means it is too simple to capture the underlying patterns in the data, leading to underfitting.\n\n2. A high variance is a sign of underfitting: This statement is also incorrect. High variance is a sign of overfitting, not underfitting. When a model has high variance, it means it is too complex and captures noise in the data, leading to overfitting.\n\n3. In a low data regime, complex models tend to perform better: This statement is generally incorrect. In a low data regime, complex models tend to overfit because they have more parameters to fit the limited data. Simple models are preferred when data is limited to avoid overfitting.\n\n4. Simple models have higher bias than complex models: This statement is generally correct. Simple models tend to have higher bias but lower variance compared to complex models. This trade-off is known as the bias-variance trade-off. Simple models may not capture all the complexities in the data, leading to higher bias but are less likely to overfit.\n\nTherefore, the correct statement regarding prediction models is: \"Simple models have higher bias than complex models.\" ///Answer: D"}
{"subject": "", "question": "Question: In linear cryptanalysis,\\dots?\n\nOptions:\nA. one needs to do a chosen plaintext attack.\nB. one studies how the differences in the input propagate in the cipher.\nC. one chooses the deviant property with the smallest bias in order to optimize the attack.\nD. one needs to have about $\\frac{1}{LP}$ pairs of plaintext-ciphertext in order to recover the correct key, where $LP$ is the linear probability of the cipher.\n", "answer": "Explanations: In linear cryptanalysis, one studies how the differences in the input propagate in the cipher to exploit linear approximations. By analyzing how certain input differences lead to specific output differences, one can potentially recover information about the encryption key.\n\nOne needs to have about $\\frac{1}{LP}$ pairs of plaintext-ciphertext in order to recover the correct key, where $LP$ is the linear probability of the cipher.\n\nOne needs to do a chosen plaintext attack in linear cryptanalysis to analyze the behavior of the cipher using known inputs and corresponding outputs.\n\nOne chooses the deviant property with the smallest bias in order to optimize the attack.\n\nIn linear cryptanalysis, one needs to have about $\\frac{1}{LP}$ pairs of plaintext-ciphertext in order to recover the correct key, where $LP$ is the linear probability of the cipher. ///Answer: B"}
{"subject": "", "question": "Question: Heidi is working on some linear regression problem to predict the price of goat milk. When training her model, she gets a loss of 0. Which of the statements below \\textbf{must then be true}?\n\nOptions:\nA. We must have $y^{(i)} = 0 \\ \\ \\forall \\ i \\in \\{1, ..., N\\}$\nB. The weights $\\mathbf{w}$ must all be 0 so that $\\hat{y}^{(i)} = \\mathbf{w}^T \\boldsymbol{x}^{(i)} = 0.$\nC. Our training set can be fit perfectly by a hyperplane (e.g., fit perfectly by a straight line if our data is 2-dimensional).\nD. Gradient descent is stuck at a local minima and fails to find the true global minimum.\n", "answer": "Explanations: When Heidi is working on a linear regression problem and gets a loss of 0 during training, the statement that \\textbf{must then be true} is: \"Our training set can be fit perfectly by a hyperplane (e.g., fit perfectly by a straight line if our data is 2-dimensional).\"\n\nIf the loss is 0 during training, it means that the model perfectly predicts the target variable (in this case, the price of goat milk) for all the data points in the training set. This implies that the data points are perfectly linearly separable and can be fit perfectly by a hyperplane. In the case of linear regression, this would mean that the data can be fit perfectly by a straight line (in the case of 2-dimensional data).\n\nThe other statements are not necessarily true when the loss is 0. For example, it is not required that all the target values $y^{(i)}$ are 0, nor that the weights $\\mathbf{w}$ must all be 0. Additionally, the fact that the loss is 0 does not mean that gradient descent is stuck at a local minimum; it could be that the loss function is well-structured such that the global minimum is easily reachable. ///Answer: C."}
{"subject": "", "question": "Question: You are working on a dataset with lots of outliers, and want to perform a regression task. Everything else being equal, and assuming that you do not do any pre-processing, which loss function will be less affected by these outliers?\n\nOptions:\nA. $\\mathcal{L}(y, \\hat{y})= (y - \\hat{y})^2$ (MSE)\nB. $\\mathcal{L}(y, \\hat{y})= |y - \\hat{y}|$ (MAE)\n", "answer": "Explanations: The correct answer to this question is '$\\mathcal{L}(y, \\hat{y})= |y - \\hat{y}|$ (MAE)'.\n\nHere is a step-by-step explanation of why Mean Absolute Error (MAE) is less affected by outliers compared to Mean Squared Error (MSE):\n\n1. Understanding of Outliers: Outliers are data points that significantly differ from the rest of the data in a dataset. They can have a strong influence on the results of regression analysis.\n\n2. Mean Absolute Error (MAE): The MAE is calculated as the average of the absolute differences between the predicted values and the actual values. The formula for MAE is:\n$\\mathcal{L}(y, \\hat{y})= \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n\n3. Mean Squared Error (MSE): The MSE is calculated as the average of the squared differences between the predicted values and the actual values. The formula for MSE is:\n$\\mathcal{L}(y, \\hat{y})= \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n\n4. Sensitivity to Outliers: MAE is less sensitive to outliers compared to MSE because the absolute difference in MAE does not amplify the effect of outliers as the squared difference in MSE does. In other words, outliers have a smaller impact on the MAE compared to MSE.\n\n5. Conclusion: In a dataset with lots of outliers, the MAE loss function will be less affected by these outliers compared to the MSE loss function. Therefore, the correct choice is '$\\mathcal{L}(y, \\hat{y})= |y - \\hat{y}|$ (MAE)'.\n\nBy understanding the properties of MAE and MSE in handling outliers, we can determine the most suitable loss function for regression tasks in the presence of outliers. ///Answer: B"}
{"subject": "", "question": "Question: Birthday attacks \\dots?\n\nOptions:\nA. are used to break Google Calendars.\nB. can be used to find collisions in hash functions.\nC. are equivalent to exhaustive search.\nD. imply that a majority of people is born in Spring.\n", "answer": "Explanations: To solve this problem, we first need to understand what a birthday attack is. A birthday attack is a type of cryptographic attack that exploits the mathematics behind the birthday paradox. The birthday paradox states that in a group of just 23 people, there is a greater than 50% chance that two people will share the same birthday. \n\nNow, let's look at the possible answers:\n\n(a) Birthday attacks are not specifically used to break Google Calendars. They are a type of cryptographic attack and can be used in various contexts, not just Google Calendars.\n\n(b) This answer is correct. Birthday attacks can be used to find collisions in hash functions. In a hash function, the output is typically a fixed length, regardless of the length of the input. A collision occurs when two different inputs produce the same output. Birthday attacks exploit the birthday paradox to find collisions more efficiently than brute force methods.\n\n(c) Birthday attacks are not equivalent to exhaustive search. While both involve trying many different inputs, a birthday attack specifically leverages the birthday paradox to find collisions more efficiently.\n\n(d) Birthday attacks do not imply that a majority of people are born in Spring. This answer is unrelated to the concept of birthday attacks.\n\nTherefore, the correct answer is (b) can be used to find collisions in hash functions. ///Answer: B"}
{"subject": "", "question": "Question: Select \\emph{incorrect} statement. The exhaustive search?\n\nOptions:\nA. can be used to find a secret key of AES.\nB. is a brute force attack.\nC. is not applicable against perfectly secure cipher.\nD. runs in time polynomial in the length of the key.\n", "answer": "Explanations: The incorrect statement in this question is: \"can be used to find a secret key of AES.\"\n\nExplanation:\n\n1. Exhaustive Search (also known as brute force attack): In this type of attack, all possible combinations of the key are tried until the correct key is found. This method involves systematically checking all possible keys until the correct one is discovered.\n\n2. AES (Advanced Encryption Standard): AES is a symmetric encryption algorithm widely used to secure data. It uses a key of varying lengths (128-bit, 192-bit, or 256-bit) to encrypt and decrypt data.\n\n3. The statement \"can be used to find a secret key of AES\" is incorrect because AES is designed to resist brute force attacks, including exhaustive search. The key length used in AES (128-bit, 192-bit, or 256-bit) makes it practically infeasible to break the encryption by trying all possible keys due to the vast number of combinations.\n\n4. Therefore, exhaustive search is not a practical method to find the secret key of AES due to the large key space and the time it would take to go through all possible keys.\n\nIn conclusion, the incorrect statement is \"can be used to find a secret key of AES\" because exhaustive search is not applicable against AES due to its strength and the large key space it offers, making it computationally infeasible to break through brute force methods. ///Answer: A"}
{"subject": "", "question": "Question: In User-Based Collaborative Filtering, which of the following is correct?\n\nOptions:\nA. Pearson Correlation Coefficient and Cosine Similarity have different value ranges, but return the same similarity ranking for the users\nB. Pearson Correlation Coefficient and Cosine Similarity have different value ranges and can return different similarity ranking for the users\nC. Pearson Correlation Coefficient and Cosine Similarity have the same value range, but can return different similarity ranking for the users\nD. Pearson Correlation Coefficient and Cosine Similarity have the same value range and return the same similarity ranking for the users.\n", "answer": "Explanations: Let's think step-by-step:\n\n1. **Pearson Correlation Coefficient and Cosine Similarity have different value ranges, but return the same similarity ranking for the users**: Incorrect.\n2. **Pearson Correlation Coefficient and Cosine Similarity have different value ranges and can return different similarity ranking for the users**: Partially correct.\n3. **Pearson Correlation Coefficient and Cosine Similarity have the same value range, but can return different similarity ranking for the users**: Incorrect.\n4. **Pearson Correlation Coefficient and Cosine Similarity have the same value range and return the same similarity ranking for the users**: Incorrect.\n\nTherefore, the correct statement is: \"Pearson Correlation Coefficient and Cosine Similarity have different value ranges and can return different similarity rankings for the users.\" ///Answer: B"}
{"subject": "", "question": "Question: Moore's Law ...\n\nOptions:\nA. is an empirical law.\nB. says that the cost of computers doubles every 18 months.\nC. will allow to break AES in 2015.\nD. is a main reason for discarding MD5 hash function.\n", "answer": "Explanations:  Moore's Law is a concept proposed by Gordon Moore, the co-founder of Intel, in 1965. It states that the number of transistors on a microchip will double approximately every two years, leading to an exponential increase in computing power. This law has been a driving force behind the rapid advancement of technology over the past several decades, as it has enabled the development of more powerful and efficient computer systems.\n\nOne implication of Moore's Law is that the cost of computers tends to decrease over time, as manufacturers are able to pack more computing power into smaller and more cost-effective packages. This has led to the widespread availability of affordable computing devices and has fueled the growth of industries such as mobile technology and cloud computing.\n\nIn the context of cybersecurity, Moore's Law has significant implications for the field of cryptography. For example, the increasing computational power of computers has made it possible to break encryption algorithms that were once considered secure. In 2015, researchers demonstrated that the AES encryption algorithm, which is widely used to secure sensitive data, could be compromised using a technique known as a brute force attack, thanks in part to the advancements predicted by Moore's Law.\n\nSimilarly, Moore's Law has also played a role in the deprecation of the MD5 hash function, which was once a popular choice for securing passwords and other sensitive data. As computing power has increased, vulnerabilities in the MD5 algorithm have been exposed, leading to a decline in its effectiveness for cryptographic purposes.\n\nOverall, Moore's Law serves as a reminder of the relentless pace of technological progress and the importance of staying ahead of the curve in fields such as cybersecurity. By understanding and adapting to the implications of this law, organizations can better protect their data and systems from emerging threats. ///Answer: B"}
{"subject": "", "question": "Question: Which statement about \textit{black-box} adversarial attacks is true:?\n\nOptions:\nA. They require access to the gradients of the model being attacked. \nB. They are highly specific and cannot be transferred from a model which is similar to the one being attacked.\nC. They cannot be implemented via gradient-free (e.g., grid search or random search) optimization methods.\nD. They can be implemented using gradient approximation via a finite difference formula.\n", "answer": "Explanations: Answer: D) They can be implemented using gradient approximation via a finite difference formula.\n\nExplanation: \nBlack-box adversarial attacks involve attacking a machine learning model without full knowledge of its internal mechanisms. In the context of deep learning models, these attacks can be implemented using gradient approximation via a finite difference formula. This method involves querying the model multiple times with slightly perturbed inputs to estimate the gradients, allowing for the generation of adversarial examples. This approach does not require access to the gradients of the model being attacked, making it suitable for black-box scenarios where gradient information is not available. ///Answer: D"}
{"subject": "", "question": "Question: Let $E$ be an elliptic curve. Solving which of the following problems would help you to break Elliptic Curve Diffie-Hellman (ECDH) over $E$?\n\nOptions:\nA. Let $P,R \\in E$. Given $P$ and $P+R$, compute $R$.\nB. Let $P,Q \\in E$. Given $P$ and $Q$, compute the product between $P$ and $Q$, i.e., $P \\times Q$.\nC. Let $P \\in E$ and $\\ell \\in \\mathbb{N}$. Given $P$ and $\\ell P$, compute $\\ell$.\nD. Find a point which is not on the curve $E$.\n", "answer": "Explanations: The correct choice that would help break Elliptic Curve Diffie-Hellman (ECDH) over $E$ is:\n\nC: Let $P \\in E$ and $\\ell \\in \\mathbb{N}$. Given $P$ and $\\ell P$, compute $\\ell$.\n\nThis choice involves solving the Discrete Logarithm Problem (DLP) on the elliptic curve $E$, which is a fundamental problem in cryptography. By being able to efficiently compute the scalar $\\ell$ from a point $P$ and $\\ell P$, an attacker could break the security of the ECDH protocol. ///Answer: C"}
{"subject": "", "question": "Question: The following function(s) have a unique minimizer.\n\nOptions:\nA. (a) $f(x)=x^{2}, x \\in[-3,2]$\nB. (b) $f(x)=\\log (x), x \\in(0,10]$\nC. (c) $f(x)=\\sin (x), x \\in[-10,10]$\nD. (d) $f(x)=e^{3 x}+x^{4}-3 x, x \\in[-10,10]$\n", "answer": "Explanations: Let's analyze each option to determine if the function has a unique minimizer within the specified interval:\n\n(a) $f(x) = x^{2}$ on the interval $x \\in [-3, 2]$:\n- This function is a simple quadratic function with a global minimum at $x = 0$. Since the interval $[-3, 2]$ includes the point where the function has a unique minimum at $x = 0$, this function has a unique minimizer within the given interval.\n\n(b) $f(x) = \\log(x)$ on the interval $x \\in (0, 10]$:\n- The function $f(x) = \\log(x)$ is not defined for $x \\leq 0$, so it is not defined at the lower boundary of the interval. Additionally, the logarithmic function is unbounded and approaches negative infinity as $x$ approaches 0. Therefore, it does not have a unique minimizer within the interval $(0, 10]$.\n\n(c) $f(x) = \\sin(x)$ on the interval $x \\in [-10, 10]$:\n- The function $f(x) = \\sin(x)$ is periodic with multiple local minima and maxima within the interval $[-10, 10]$. It does not have a unique minimizer within this interval as it achieves multiple minima (e.g., at $x = -\\frac{3\\pi}{2}$, $x = -\\frac{\\pi}{2}$, etc.).\n\n(d) $f(x) = e^{3x} + x^{4} - 3x$ on the interval $x \\in [-10, 10]$:\n- This function is a combination of an exponential term and a polynomial term. It has multiple local minima and maxima within the interval $[-10, 10]$. By analyzing its derivatives and plotting the function, we can see that it has multiple minimizers within the interval. For example, it has local minima at $x \\approx -0.65$ and $x \\approx 1.15$. Therefore, it does not have a unique minimizer within this interval.\n\nIn summary:\n- Option (a) has a unique minimizer within the interval $[-3, 2]$.\n- Option (b) does not have a unique minimizer within the interval $(0, 10]$.\n- Option (c) does not have a unique minimizer within the interval $[-10, 10]$.\n- Option (d) does not have a unique minimizer within the interval $[-10, 10] due to having multiple local minima. ///Answer: A"}
{"subject": "", "question": "Question: Select all statements that are true.A penalty will be applied for any wrong answers.\n\nOptions:\nA. The analyzer functionality of a parser determines the set of all possible associated syntactic structures for any syntactically correct sentence.\nB. The recognizer functionality of a parser decides if a given sequence of words is syntactically correct or not.\nC. For a sentence to be acceptable in general, it is sufficient to satisfy the positional and selectional constraints of a given language.\nD. Determining whether a sentence has a pragmatic meaning depends on the context that is available.\nE. Syntactic ambiguity has no effect on the algorithmic complexity of parsers.\n", "answer": "Explanations: A. True - The analyzer functionality of a parser does determine the set of all possible associated syntactic structures for any syntactically correct sentence.\nB. True - The recognizer functionality of a parser does decide if a given sequence of words is syntactically correct or not.\nC. False - For a sentence to be acceptable in general, it is not sufficient to just satisfy the positional and selectional constraints of a given language. Other factors such as semantics and pragmatics also play a role.\nD. True - Determining whether a sentence has a pragmatic meaning does depend on the context that is available.\nE. False - Syntactic ambiguity can affect the algorithmic complexity of parsers as they may need to explore multiple possible structures for a sentence. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following statements about code instrumentation is/are correct?\n\nOptions:\nA. We should instrument basic blocks when collecting edge coverage.\nB. The instrumentation code for coverage collection should not\n            change the original functionality.\nC. Binary rewriting-based coverage collection has lower runtime\n            overheads than compiler-based instrumentation.\nD. We can only do binary rewriting on position-independent code (PIC).\n", "answer": "Explanations: Let's evaluate each statement:\n\nOption 0: We should instrument basic blocks when collecting edge coverage.\n- This statement is partially correct. When collecting edge coverage, instrumenting basic blocks is essential to track the execution paths and determine which edges are covered. Therefore, this statement is correct.\n\nOption 1: The instrumentation code for coverage collection should not change the original functionality.\n- This statement is correct. When instrumenting code for coverage collection, it is crucial to ensure that the instrumentation does not alter the original behavior of the program. The purpose of instrumentation is to collect coverage data without affecting the program's functionality.\n\nOption 2: Binary rewriting-based coverage collection has lower runtime overheads than compiler-based instrumentation.\n- This statement is generally true. Binary rewriting can potentially have lower runtime overhead compared to compiler-based instrumentation because it involves modifying the compiled binary directly, optimizing the instrumentation process. However, the actual runtime overhead can vary based on the specific implementation and the level of instrumentation required.\n\nOption 3: We can only do binary rewriting on position-independent code (PIC).\n- This statement is not entirely accurate. While binary rewriting can be more challenging on position-independent code due to its address space layout randomization (ASLR) properties, it is still possible to perform binary rewriting on both position-independent and position-dependent code. Techniques like code relocation and dynamic memory allocation can be used to handle position-dependent code during binary rewriting.\n\nIn conclusion, statements 0, 1, and 2 are correct, while statement 3 is not entirely accurate. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion. The Vernam cipher provides \\dots?\n\nOptions:\nA. authenticity.\nB. integrity.\nC. confidentiality.\nD. none of the mentioned properties.\n", "answer": "Explanations: Answer: \n\nC) confidentiality.\n\nExplanation:\nThe Vernam cipher, also known as the one-time pad, is a symmetric key encryption algorithm that provides confidentiality by encrypting plaintext into ciphertext. The key used in the Vernam cipher is as long as the plaintext, and it is used only once, making it extremely secure when implemented correctly. However, it does not provide authenticity or integrity of the message. Authenticity ensures that the message comes from the expected sender and has not been tampered with, while integrity ensures that the message has not been altered during transmission. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement regarding Modes of Operation in Block ciphers.\n\nOptions:\nA. The CBC mode uses an IV value.\nB. The ECB mode is a secure way to use block ciphers using a single key.\nC. The CTR mode uses a counter which is new for every block.\nD. The OFB mode transforms a block cipher into a stream cipher.\n", "answer": "Explanations: A: The CBC mode uses an IV value. - True\nB: The ECB mode is a secure way to use block ciphers using a single key. - False\nC: The CTR mode uses a counter which is new for every block. - True\nD: The OFB mode transforms a block cipher into a stream cipher. - True\n\nTherefore, the false statement is B: The ECB mode is a secure way to use block ciphers using a single key. ///Answer: B:"}
{"subject": "", "question": "Question: Which one of the following encryption method is a simple substitution cipher?\n\nOptions:\nA. Vigen\\`ere cipher\nB. the Vernam cipher.\nC. the Caesar cipher.\nD. Enigma\n", "answer": "Explanations: A simple substitution cipher involves replacing each letter in the plaintext with a different letter in the ciphertext. The Caesar cipher is a basic example of a simple substitution cipher, where each letter is shifted by a fixed number of positions. Therefore, the correct answer is: \n\nAnswers 3) the Caesar cipher. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion when $x\\rightarrow+\\infty$.\n\nOptions:\nA. $x^2+5x+2 = O(x^3)$.\nB. $x^n = O(n^x)$ for any constant $n > 1$.\nC. $x\\log(x) = O(x^2)$.\nD. $1/x = O(1/x^2)$\n", "answer": "Explanations: The incorrect assertion when $x\\rightarrow +\\infty$ is: $x^n = O(n^x)$ for any constant $n > 1$.\n\nJustification: In Big O notation, $f(x) = O(g(x))$ means that $f(x)$ grows at most as fast as $g(x)$ for large $x$. \n\nFor the assertion $x^n = O(n^x)$, where $n$ is a constant greater than $1$, this is incorrect. When $x$ tends to infinity, $x^n$ grows much faster than $n^x$ for any constant $n>1$. In fact, $x^n$ grows polynomially while $n^x$ grows exponentially. \n\nTherefore, the correct assertion is that $x^n$ is not $O(n^x)$ for any constant $n>1$ as $x$ tends to infinity. ///Answer: B:"}
{"subject": "", "question": "Question: Consider:Non-terminals: S (top-level), NP (for \"noun phrase\"), VP (for \"verbal phrase\"), N (for \"Noun\"), V (for \"Verb\"), Det (for \"Determiner\").PoS tags: N, V, DetTerminals: I, yesterday, in, rain, went, home, the, cat, goOut of the following, select the ones which are possible valid \"syntactic rules\" as defined in a context-free grammar for processing (a tiny part of) English.A penalty will be applied for any incorrect answers.\n\nOptions:\nA. S\u00a0\u2192 NP VP\nB. NP \u2192 Det N\nC. V \u2192 VP N\u00a0\nD. NP \u2192 N\nE. VP \u2192 VP NP\nF. VP NP \u2192 V N\nG. VP \u2192 the cat\nH. Det \u2192 went\nI. Det N \u2192 NP\nJ. S \u2192 VP\n", "answer": "Explanations: 1. S \u2192 NP VP - This is a valid syntactic rule as it defines the structure of a sentence as consisting of a noun phrase followed by a verbal phrase.\n2. NP \u2192 Det N - This is a valid syntactic rule as it defines a noun phrase as consisting of a determiner followed by a noun.\n3. V \u2192 VP N - This is not a valid syntactic rule as it is defining a verb as consisting of a verbal phrase followed by a noun, which does not align with typical English syntax.\n4. NP \u2192 N - This is a valid syntactic rule as it defines a noun phrase as simply a noun.\n5. VP \u2192 VP NP - This is not a valid syntactic rule as it is defining a verbal phrase as consisting of a verbal phrase followed by a noun phrase, which does not align with typical English syntax.\n6. VP NP \u2192 V N - This is not a valid syntactic rule as it is defining a verbal phrase followed by a noun phrase as consisting of a verb followed by a noun, which does not align with typical English syntax.\n7. VP \u2192 the cat - This is not a valid syntactic rule as it is defining a verbal phrase as simply \"the cat,\" which does not align with typical English syntax.\n8. Det \u2192 went - This is not a valid syntactic rule as it is defining a determiner as \"went,\" which does not align with typical English syntax.\n9. Det N \u2192 NP - This is not a valid syntactic rule as it is defining a determiner followed by a noun as a noun phrase, which does not align with typical English syntax.\n10. S \u2192 VP - This is a valid syntactic rule as it defines a sentence as consisting of a verbal phrase. \n\nTherefore, the valid syntactic rules are:\n1. S \u2192 NP VP\n2. NP \u2192 Det N\n4. NP \u2192 N\n10. S \u2192 VP\n\nPenalty: -3 points for each incorrect answer. ///Answer: A"}
{"subject": "", "question": "Question: TransE is known to have difficulties in representing symmetric relationships r. Which of the following statements is correct for a symmetric relationship and provides insight for this problem?\n\nOptions:\nA. For all e1 , e2: \\( f(e_1, r, e_2) = f(e_2, r, e_1)  \\)\nB. For all e1 , e2: \\( f(e_1, r, e_2) = -f(e_2, r, e_1) \\)\nC. \\( \\Sigma_{ e_1, e_2} f(e_1, r, e_2) + f(e_2, r, e_1) \\)\u00a0is minimized if the embedding vector of r is large\nD. \\( \\Sigma_{ e_1, e_2} f(e_1, r, e_2) + f(e_2, r, e_1) \\) is minimized if the embedding vectors of e1and e2 are close to each other\n", "answer": "Explanations: The correct statement for a symmetric relationship that provides insight into the problem TransE faces is: 'For all e1 , e2: \\( f(e_1, r, e_2) = f(e_2, r, e_1) \\)'. This statement highlights the fact that in a symmetric relationship, the scoring function should return the same value regardless of the order of the entities. This property is important for accurately representing symmetric relationships in knowledge graphs, and the failure of TransE to satisfy this condition leads to difficulties in capturing such relationships. ///Answer: A"}
{"subject": "", "question": "Question: We report the final performance (e.g., accuracy) on the ...\n(One answer)?\n\nOptions:\nA.  training \nB.   validation \nC.  test \nD.  all the data together \n", "answer": "Explanations: The correct answer is C: test.\n\nWhen evaluating a machine learning model, the final performance should be reported on the test dataset. The test dataset is a separate dataset that has not been used during training or model selection. Reporting the final performance on the test dataset provides an unbiased estimate of how well the model generalizes to new, unseen data.\n\nReporting the performance on the training dataset (option A) would not provide a reliable measure of generalization because the model has already seen that data during training. Reporting on the validation dataset (option B) is used for tuning hyperparameters and model selection, but the final performance should be reported on the test dataset. Option D, reporting on all the data together, could lead to overfitting and does not provide a clear indication of how well the model generalizes to new data. ///Answer: C"}
{"subject": "", "question": "Question: Which protocol does \\emph{not} use RC4.\n\nOptions:\nA. WEP\nB. WPA\nC. WPA2\nD. TLS\n", "answer": "Explanations: WPA2 is the correct option as it does not use RC4. WEP, WPA, and TLS all have the capability to use RC4 encryption algorithms. WPA2, on the other hand, uses the more secure AES encryption algorithm, which is considered more robust and less vulnerable to attacks compared to RC4. This makes WPA2 a more secure option for protecting network communications. ///Answer: C"}
{"subject": "", "question": "Question: Assume that you get a confidence interval of size $\\delta$ for some problem given $N$ iid samples. Expressed as a function of $N$, how many iid samples do you need to get a confidence interval of $\\operatorname{size} \\delta / 3 ?$?\n\nOptions:\nA. $3 N$\nB. $N/3$\nC. $N^3$\nD. $9N$\nE. $\\sqrt{3 N}$\nF. $e^{3 N}$\n", "answer": "Explanations: Option A: $N/3$\n\nThis option could be considered incorrect because simply dividing the sample size by 3 would not lead to a confidence interval of size $\\delta / 3$. The size of a confidence interval is influenced by factors such as the sample variance and the confidence level, and a linear relationship between the sample size and the confidence interval size does not hold.\n\nOption B: $3N$\n\nThis option could be considered correct. The size of a confidence interval is inversely proportional to the square root of the sample size. Therefore, to reduce the size of the confidence interval by a factor of 3 (i.e., $\\delta / 3$), we would need to increase the sample size by a factor of 3.\n\nOption C: $N^2$\n\nThis option could be considered incorrect as there is no direct relationship between the sample size and the square of the sample size in determining the size of a confidence interval. Increasing the sample size by a factor of $N^2$ would not lead to a confidence interval of size $\\delta / 3$.\n\nOption D: $N^{1/3}$\n\nThis option could be considered incorrect as the relationship between the sample size and the size of a confidence interval is not typically a simple power function. The size of a confidence interval is determined by various factors, and a direct power relationship with the sample size is unlikely. ///Answer: A:"}
{"subject": "", "question": "Question: You are given a hash function $h$ based on the Merkle-Damg{\\aa}rd scheme. Which of the following attacks is the most difficult, \\emph{a priori}?\n\nOptions:\nA. first preimage attack\nB. collision search\nC. second preimage attack\nD. collision search on the compression function\n", "answer": "Explanations: C. second preimage attack\n\nJustification:\nIn the Merkle-Damg\u00e5rd construction, the second preimage attack is typically considered the most difficult, a priori, compared to other attacks such as the first preimage attack and collision search. \n\nFor a hash function based on the Merkle-Damg\u00e5rd construction, finding a second preimage involves finding a different input that hashes to the same output as a given input. This type of attack is generally more computationally challenging than finding a first preimage or generating a collision due to the nature of how the hash function processes the input blocks in a sequential manner.\n\nWhile collision search on the compression function is also a challenging task, the second preimage attack is often seen as the most difficult of the options listed when considering the security properties of a hash function in the Merkle-Damg\u00e5rd scheme. ///Answer: C"}
{"subject": "", "question": "Question: What is the value of \\(f(4)\\) where \\(f\\) is defined as \\(f(0) = f(1) = 1\\) and \\(f(n) = 2f(n - 1) + 3f(n - 2)\\) for integers \\(n \\geq 2\\)?\n\nOptions:\nA. 41\nB. 45\nC. 39\nD. 43\n", "answer": "Explanations: Let's break it down step by step:\n\nGiven the recursive definition \\(f(n) = 2f(n - 1) + 3f(n - 2)\\) for integers \\(n \\geq 2\\), with initial conditions \\(f(0) = f(1) = 1\\.\n\n1. Calculate \\(f(2)\\) using the recursive formula:\n   \\(f(2) = 2f(2 - 1) + 3f(2 - 2) = 2f(1) + 3f(0) = 2 \\times 1 + 3 \\times 1 = 2 + 3 = 5\\).\n\n2. Calculate \\(f(3)\\) using the recursive formula and the previously calculated values:\n   \\(f(3) = 2f(3 - 1) + 3f(3 - 2) = 2f(2) + 3f(1) = 2 \\times 5 + 3 \\times 1 = 10 + 3 = 13\\).\n\n3. Now, find \\(f(4)\\) directly by substituting the values of \\(f(3)\\) and \\(f(2)\\) into the formula:\n   \\(f(4) = 2f(4 - 1) + 3f(4 - 2) = 2f(3) + 3f(2) = 2 \\times 13 + 3 \\times 5 = 26 + 15 = 41\\).\n\nTherefore, the value of \\(f(4)\\) is 41, matching option 1) in the given choices. ///Answer: A"}
{"subject": "", "question": "Question: The substitution cipher with random substitution table is?\n\nOptions:\nA. computationally infeasible to break since number of possible keys is $26! \\approx 2^{88.4}$\nB. very easy to break since number of possible keys is only $26! \\approx 2^{88.4}$\nC. very easy to break even though number of possible keys is $26! \\approx 2^{88.4}$\nD. certified by NIST up to \"confidential\" level\n", "answer": "Explanations: Option 1: It is computationally infeasible to break since the number of possible keys is $26! \\approx 2^{88.4}$.\n\nExplanation:\nA substitution cipher with a random substitution table involves replacing each letter in the plaintext with a corresponding random letter in the ciphertext. In this case, there are 26 factorial ($26!$) possible keys since each letter can be substituted with any of the other 25 letters.\n\nThe number of possible keys in a substitution cipher with a random substitution table is $26! \\approx 2^{88.4}$, which means there are approximately $2^{88.4}$ possible keys. This number is extremely large, making it computationally infeasible to break the cipher through brute force or exhaustive search. It would require trying an astronomical number of key combinations, making it practically impossible to decrypt the message without knowing the specific key used for encryption.\n\nTherefore, due to the large number of possible keys in a substitution cipher with a random substitution table, it is considered computationally infeasible to break, ensuring a relatively high level of security for the encrypted message. ///Answer: A"}
{"subject": "", "question": "Question: Let $C$ be a perfect cipher with $\\ell$-bit blocks. Then, \\dots?\n\nOptions:\nA. for $x_1 \\neq x_2$, $\\Pr[C(x_1) = y_1, C(x_2)=y_2] = \\frac{1}{2^{2\\ell}}$.\nB. the size of the key space of $C$ should be at least $(2^{\\ell}!)$.\nC. given pairwise independent inputs to $C$, the corresponding outputs are independent and uniformly distributed.\nD. $C$ has an order $3$ decorrelation matrix which is equal to the order $3$ decorrelation matrix of a random function.\n", "answer": "Explanations: The correct option is:\n\ngiven pairwise independent inputs to $C$, the corresponding outputs are independent and uniformly distributed.\n\nExplanation:\nIn a perfect cipher, if the inputs are pairwise independent, the corresponding outputs are also independent and uniformly distributed. This property is essential for ensuring the security and unpredictability of the cipher. Pairwise independence ensures that no information about one input/output pair can leak information about another, leading to a high level of security in encryption schemes. The independence and uniform distribution of outputs make it harder for attackers to decipher the encrypted data, maintaining the confidentiality of the communication. ///Answer: C"}
{"subject": "", "question": "Question: Let $G$, be a valid generator matrix of dimensions $\\mathbb F^{k \times n}$. Recall that ``valid'' means that $k \\leq n$ and $\text{rank}(G) = k$. Let $D_1 \\in \\mathbb F^{k \times k}$ and $D_2 \\in \\mathbb F^{n \times n}$ be diagonal matrices with non-zero diagonal elements. True or false: $D_1 \\cdot G \\cdot D_2$ is also a valid generator matrix.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: 1. True\n\nExplanation (230 tokens):\nIn this scenario, we are dealing with a valid generator matrix $G$ of dimensions $k \\times n$, where the rank of $G$ is equal to $k$. When we multiply $G$ by diagonal matrices $D_1$ and $D_2$ on the left and right sides, respectively, the resulting matrix $D_1 \\cdot G \\cdot D_2$ will still have rank equal to $k$. This is because multiplying by diagonal matrices preserves the rank of the original matrix. Therefore, $D_1 \\cdot G \\cdot D_2$ remains a valid generator matrix, satisfying the conditions of having dimensions $k \\times n$ and rank equal to $k$. ///Answer: B"}
{"subject": "", "question": "Question: You need to debug your Stochastic Gradient Descent update for a classification of three bridge types.\n\tManually compute the model output for the feature vector $x=(1, 0, 0, 0, 0)$ and $W$ contains only zeros. The model is logistic regression, \\textit{i.e.}, $\\textrm{softmax}(Wx)$. \n\tRemember:\n\t\\begin{equation}\n\t    \\textrm{softmax}_i(s) = \\frac{e^{s_i}}{\\sum_k e^{s_k}}\n\t\\end{equation}\n\t(One answer!!!!!!)?\n\nOptions:\nA. $(0, 0, 0)$\nB. $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\nC. $(0, 0, 0, 0, 0)$\nD. $(\\frac{1}{5}, \\frac{1}{5}, \\frac{1}{5}, \\frac{1}{5}, \\frac{1}{5})$\n", "answer": "Explanations: Let's think step-by-step:\n\n1. **Given**:\n   - Feature vector $x = (1, 0, 0, 0, 0)$\n   - Weight matrix $W$ containing only zeros\n   - Model is logistic regression with softmax function: $\\textrm{softmax}(Wx)$\n\n2. **Compute the model output**:\n   - Since $W$ contains only zeros, the result of $Wx$ will be a vector of zeros.\n   - Therefore, the softmax function will normalize a vector of zeros, resulting in equal probabilities for all classes.\n\n3. **Apply the softmax function**:\n   - For a vector of zeros, the softmax function will yield equal probabilities for each class:\n     \\begin{equation}\n     \\textrm{softmax}(Wx) = \\left( \\frac{e^0}{e^0 + e^0 + e^0}, \\frac{e^0}{e^0 + e^0 + e^0}, \\frac{e^0}{e^0 + e^0 + e^0} \\right)\n     \\end{equation}\n     \\begin{equation}\n     = \\left( \\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3} \\right)\n     \\end{equation}\n\n4. **Final Answer**:\n   - The manually computed model output for the feature vector $x=(1, 0, 0, 0, 0)$ when $W$ contains only zeros is $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$. ///Answer: B"}
{"subject": "", "question": "Question: The CRT implies?\n\nOptions:\nA. $\\phi(mn)=\\phi(m)\\phi(n)$, for $gcd(m,n)=1$.\nB. $\\phi(mn)=\\phi(m)+\\phi(n)$, for $gcd(m,n)=1$.\nC. $\\phi(mn)=\\phi(m)^n$, for $m\\ne n$.\nD. $\\phi(mn)=\\phi(m)-n$, for $m > n$.\n", "answer": "Explanations: The correct implication for the Euler's totient function (phi function) is:\n\n$\\phi(mn)=\\phi(m)\\phi(n)$, for $gcd(m,n)=1$.\n\nThis property is a consequence of the multiplicative property of the Euler's totient function. When two numbers are coprime (i.e., their greatest common divisor is 1), the Euler's totient function of their product is equal to the product of their Euler's totient functions. ///Answer: A"}
{"subject": "", "question": "Question: If timeliness is ensured by a communication channel then the channel also provides\\ldots?\n\nOptions:\nA. liveliness.\nB. privacy.\nC. freshness.\nD. confidentiality.\n", "answer": "Explanations: First, let's break down the question to understand what is being asked. The question states that if timeliness is ensured by a communication channel, then the channel also provides something else. We need to determine what that additional feature might be based on the relationship between timeliness and other aspects of communication channels.\n\nWhen we talk about timeliness in communication, we are referring to how quickly a message is delivered from the sender to the receiver. To ensure timeliness, the channel needs to be efficient and effective in delivering messages promptly. Considering this, we need to think about what other characteristic might be related to timeliness in communication channels.\n\nNow, let's look at the options provided:\n- Liveliness: This term typically refers to being active, dynamic, or engaging. While liveliness can contribute to effective communication, it is not directly related to ensuring timeliness.\n- Privacy: Privacy in communication channels relates to keeping information confidential and away from unauthorized access. This aspect is important but not directly linked to ensuring timeliness.\n- Freshness: Freshness could be related to the up-to-dateness of information being delivered, which could be connected to ensuring timeliness.\n- Confidentiality: Confidentiality is about keeping information secret and known only to a select few. While it is crucial for certain types of communication, it is not directly tied to ensuring timeliness.\n\nBased on the options provided and our analysis, the most appropriate additional feature that a communication channel might provide if it ensures timeliness could be \"freshness.\" This is because timely delivery of messages often goes hand in hand with ensuring that the information is current and up-to-date. ///Answer: C"}
{"subject": "", "question": "Question: Nearest neighbor classifiers cannot be used for regression because they rely on majority voting, which is not suited for continuous labels.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: While traditional nearest neighbor classifiers are not typically used for regression tasks due to their reliance on majority voting, there are adaptations of the nearest neighbor algorithm that can handle continuous values. One such adaptation is the k-nearest neighbor regression algorithm. In k-nearest neighbor regression, instead of assigning a majority class label to a given input, the algorithm calculates the average or weighted average of the continuous values of the k nearest neighbors and uses this as the predicted value for the input. This allows the algorithm to make predictions for continuous output values in regression tasks. By using the k-nearest neighbor regression algorithm, it is possible to leverage the benefits of the nearest neighbor approach, such as its simplicity and ability to capture complex relationships in the data, while also being able to make predictions for continuous output values in regression tasks. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following arguments is correct? ?\n\nOptions:\nA. All students in this class understand math. Alice is a student in this class. Therefore, Alice doesn\u2019t understand math.\nB. Every physics major takes calculus. Mathilde is taking calculus. Therefore, Mathilde is a physics major.\nC. All cats like milk. My pet is not a cat. Therefore, my pet does not like milk.\nD. Everyone who eats vegetables every day is healthy. Linda is not healthy. Therefore, Linda does not eat vegetables every day.\n", "answer": "Explanations: The correct answer is:\n\nD: Everyone who eats vegetables every day is healthy. Linda is not healthy. Therefore, Linda does not eat vegetables every day.\n\nThis argument is based on the premise that everyone who eats vegetables every day is healthy. The conclusion that Linda does not eat vegetables every day logically follows from the fact that she is not healthy. If the premise is true, and Linda is not healthy, then it can be inferred that she does not eat vegetables every day. ///Answer: D"}
{"subject": "", "question": "Question: Let $n$ be a positive integer. The Fermat test most likely outputs ``prime'' \\dots?\n\nOptions:\nA. only when $n$ is prime.\nB. only when $n$ is non-prime.\nC. when $n$ is prime or when $n$ is not a Carmichael number.\nD. when $n$ is prime or when $n$ is a Carmichael number.\n", "answer": "Explanations: The correct answer is (c) when $n$ is prime or when $n$ is not a Carmichael number. \n\nThis is because the Fermat test is a probabilistic primality test that can determine with high probability whether a given number is composite. If the test outputs \"prime,\" then the number is either prime or a Carmichael number. Carmichael numbers are composite numbers that satisfy the conditions for primality in the Fermat test, making it important to consider them as well when interpreting the test results. ///Answer: C"}
{"subject": "", "question": "Question: You share an apartment with friends.\nKitchen, living room, balcony, and bath room are shared resources among all parties.\nWhich policy/policies violate(s) the principle of least privilege?\n\nOptions:\nA. Different bedrooms do not have a different key.\nB. There is no lock on the fridge.\nC. To access the kitchen you have to go through the living room.\nD. Nobody has access to the neighbor's basement.\n", "answer": "Explanations: A: The policy that violates the principle of least privilege is: - Different bedrooms do not have a different key.\n\nExplanation: The principle of least privilege states that each user should only have the minimum level of access necessary to perform their job functions. In this scenario, not having different keys for different bedrooms means that all roommates have access to each other's personal space, violating the principle of least privilege. This lack of privacy and security can lead to potential conflicts or breaches of privacy among the roommates. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. In Linear Cryptanalysis, the corresponding mask circuit of \\ldots?\n\nOptions:\nA. \\ldots a XOR gate ($X\\oplus Y = Z$) is $a\\cdot Z=(a\\cdot X)\\oplus (a\\cdot Y)$\nB. \\ldots a XOR to constant gate ($Y=X\\oplus K$) is $a\\cdot Y = (a\\cdot X)\\oplus (a\\cdot K)$\nC. \\ldots a linear circuit ($Y=M\\times X$) is $a\\cdot Y = (M\\times a)\\cdot X$\nD. \\ldots a duplicate gate ($X=Y=Z$) is $(a\\oplus b)\\cdot X=(a\\cdot Y)\\oplus (b\\cdot Z)$\n", "answer": "Explanations: To determine the \\textbf{false} assertion in Linear Cryptanalysis regarding the corresponding mask circuit of different gates, let's analyze each option one by one:\n\n1. For a XOR gate ($X\\oplus Y = Z$), the assertion is: $a\\cdot Z=(a\\cdot X)\\oplus (a\\cdot Y)$\n\nLet's check if this assertion holds:\nGiven $X\\oplus Y = Z$, we can multiply both sides by $a$ to get: $a\\cdot (X\\oplus Y) = a\\cdot Z$\n\nExpanding the left side using the distributive property of multiplication over XOR, we get: $a\\cdot X \\oplus a\\cdot Y = a\\cdot Z$\n\nTherefore, the assertion for a XOR gate is true: $a\\cdot Z=(a\\cdot X)\\oplus (a\\cdot Y)$\n\n2. For a XOR to constant gate ($Y=X\\oplus K$), the assertion is: $a\\cdot Y = (a\\cdot X)\\oplus (a\\cdot K)$\n\nLet's check if this assertion holds:\nGiven $Y=X\\oplus K$, we can multiply both sides by $a$ to get: $a\\cdot (X\\oplus K) = a\\cdot Y$\n\nExpanding the left side using the distributive property of multiplication over XOR, we get: $a\\cdot X \\oplus a\\cdot K = a\\cdot Y$\n\nTherefore, the assertion for a XOR to constant gate is true: $a\\cdot Y = (a\\cdot X)\\oplus (a\\cdot K)$\n\n3. For a linear circuit ($Y=M\\times X$), the assertion is: $a\\cdot Y = (M\\times a)\\cdot X$\n\nLet's check if this assertion holds:\nGiven $Y=M\\times X$, we can multiply both sides by $a$ to get: $a\\cdot (M\\times X) = a\\cdot Y$\n\nExpanding the left side using the distributive property of multiplication over addition, we get: $(a\\cdot M)\\times X = a\\cdot Y$\n\nTherefore, the assertion for a linear circuit is false: $a\\cdot Y \\neq (M\\times a)\\cdot X$\n\n4. For a duplicate gate ($X=Y=Z$), the assertion is: $(a\\oplus b)\\cdot X=(a\\cdot Y)\\oplus (b\\cdot Z)$\n\nLet's check if this assertion holds:\nGiven $X=Y=Z$, we have $X=Y=Z$, which means $X=Y$ and $Y=Z$. \n\nSubstitute $X=Y$ and $Y=Z$ into the assertion: $(a\\oplus b)\\cdot X=(a\\cdot Y)\\oplus (b\\cdot Z)$ becomes $(a\\oplus b)\\cdot X=(a\\cdot X)\\oplus (b\\cdot X)$\n\nTherefore, the assertion for a duplicate gate is false: $(a\\oplus b)\\cdot X \\neq (a\\cdot X)\\oplus (b\\cdot X)$\n\nTherefore, the \\textbf{false} assertion in Linear Cryptanalysis regarding the corresponding mask circuit of different gates is: \n\n- ... a linear circuit ($Y=M\\times X$) is $a\\cdot Y = (M\\times a)\\cdot X$ ///Answer: C"}
{"subject": "", "question": "Question: Once paging is enabled, load instruction / CR3 register / Page Table entry uses Virtual or Physical address?\n\nOptions:\nA. Physical / Physical / Physical\nB. Physical / Physical / Virtual\nC. Virtual / Physical / Physical\nD. Virtual / Virtual / Virtual\nE. Virtual / Virtual / Physical\n", "answer": "Explanations: When paging is enabled, the operating system uses a page table to map virtual addresses to physical addresses. Let's break down the components in the question:\n\n1) **Load instruction**: When loading instructions, the CPU uses virtual addresses, which are translated to physical addresses by the Memory Management Unit (MMU) using the page table.\n\n2) **CR3 register**: The Control Register 3 (CR3) contains the base address of the page directory, which is a physical address. It is used by the MMU during address translation.\n\n3) **Page Table entry**: Each entry in the page table contains the mapping between a virtual address and a physical address. The addresses stored in the page table are physical addresses.\n\nTherefore, the correct option is:\n\n3) Virtual / Physical / Physical ///Answer: C"}
{"subject": "", "question": "Question: Assume we enforce CFI for function returns. Which of the following statements are true?\n\nOptions:\nA. CFI on returns will make control-flow hijacking harder\nB. CFI on returns is too coarse-grained and may give the adversary sufficient valid targets for an exploit\nC. CFI on returns ensures that only the single valid target is allowed\nD. CFI on returns cannot support exceptions\n", "answer": "Explanations: Enforcing Control Flow Integrity (CFI) on returns plays a crucial role in enhancing the security of a program by making control-flow hijacking attacks more difficult for adversaries to carry out. By restricting the legitimate targets of control transfers when a function returns to its caller, CFI on returns effectively mitigates the risk of attackers manipulating the flow of execution to execute malicious code. Therefore, the correct option is: **CFI on returns will make control-flow hijacking harder**.\n\nThe other options are incorrect because:\n\n- **CFI on returns is too coarse-grained and may give the adversary sufficient valid targets for an exploit**: This statement is misleading, as CFI on returns actually reduces the number of valid targets available to attackers, making it harder for them to exploit vulnerabilities.\n  \n- **CFI on returns ensures that only the single valid target is allowed**: While CFI enforces restrictions on valid targets, it does not limit it to a single target. There can be multiple valid targets within the predetermined set based on the program's control flow structure.\n\n- **CFI on returns cannot support exceptions**: This statement is inaccurate, as CFI can be designed to support exception handling mechanisms in programs. CFI can be extended to protect both normal control flow and exception handling, thereby enhancing overall security measures. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{non-commutative} operation.\n\nOptions:\nA. $+$ (addition) over $\\mathbb{Z}$\nB. $\\oplus$ (exclusive-or)\nC. $-$ (subtraction) over $\\mathbb{Z}$\nD. $\\times$ (multiplication) over $\\mathbb{Z}$\n", "answer": "Explanations: To determine which operation is non-commutative, we need to check if the operation satisfies the commutative property. The commutative property states that changing the order of the operands does not change the result of the operation.\n\nLet's go through each operation:\n\n1. Addition ($+$) over $\\mathbb{Z}$: Addition is commutative because for any two integers $a$ and $b$, $a + b = b + a$. So, addition is commutative.\n\n2. Exclusive-OR ($\\oplus$): Exclusive-OR is commutative because for any two bits $a$ and $b$, $a \\oplus b = b \\oplus a$. So, Exclusive-OR is commutative.\n\n3. Subtraction ($-$) over $\\mathbb{Z}$: Subtraction is not commutative because for integers $a$ and $b$, $a - b \\neq b - a$ in general. Therefore, subtraction is non-commutative.\n\n4. Multiplication ($\\times$) over $\\mathbb{Z}$: Multiplication is commutative because for any two integers $a$ and $b$, $a \\times b = b \\times a$. So, multiplication is commutative.\n\nThus, the non-commutative operation in the given options is subtraction ($-$) over $\\mathbb{Z}$. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. The perfect cipher over $\\left\\{ 0,1 \\right\\}^\\ell$ is \\dots?\n\nOptions:\nA. uniformly distributed.\nB. $C^*\\;$.\nC. using a key space of size $2^\\ell!$.\nD. practical.\n", "answer": "Explanations: The incorrect assertion is: using a key space of size $2^\\ell!$.\n\nReasoning: \n- The perfect cipher over $\\left\\{ 0,1 \\right\\}^\\ell$ is indeed uniformly distributed, meaning that each possible ciphertext is equally likely to be produced.\n- $C^*$ refers to the set of all possible ciphertexts that can be produced by the cipher, which is a characteristic of a perfect cipher.\n- However, using a key space of size $2^\\ell!$ is incorrect. The key space for a perfect cipher should be of size $2^\\ell$, representing all possible keys that can be used for encryption and decryption. The factorial operation $2^\\ell!$ is not valid in this context.\n- The assertion that a perfect cipher is practical is also incorrect. A perfect cipher is a theoretical concept and not practical for real-world use due to various constraints and limitations. ///Answer: C:"}
{"subject": "", "question": "Question: Consider an Sbox $S:\\{0,1\\}^m \\rightarrow \\{0,1\\}^m$. We have that \\ldots?\n\nOptions:\nA. $\\mathsf{DP}^S(0,b)=1$ if and only if $S$ is a permutation.\nB. $\\sum_{b\\in \\{0,1\\}^m} \\mathsf{DP}^S(a,b)$ is even.\nC. $\\sum_{b\\in \\{0,1\\}^m \\backslash \\{0\\}} \\mathsf{DP}^S(0,b)= 0$\nD. $\\mathsf{DP}^S(0,b)=1$ if and only if $m$ is odd.\n", "answer": "Explanations: Answer: (C) $\\sum_{b\\in \\{0,1\\}^m \\backslash \\{0\\}} \\mathsf{DP}^S(0,b)= 0$\n\nExplanation:\nDifferential Probability ($\\mathsf{DP}^S$) is a measure used in cryptography to quantify the likelihood that a difference in the input to an Sbox will result in a specific difference in the output.\n\nLet's break down the statements:\n\nA) $\\mathsf{DP}^S(0,b)=1$ if and only if $S$ is a permutation.\nThis statement is incorrect. If $\\mathsf{DP}^S(0,b)=1$, it means that the probability of the output difference being $b$ when the input difference is $0$ is 1. This does not necessarily mean that $S$ is a permutation.\n\nB) $\\sum_{b\\in \\{0,1\\}^m} \\mathsf{DP}^S(a,b)$ is even.\nThis statement is not always true. The sum of differential probabilities over all possible output differences may be even or odd depending on the Sbox $S$ and the input difference $a$.\n\nC) $\\sum_{b\\in \\{0,1\\}^m \\backslash \\{0\\}} \\mathsf{DP}^S(0,b)= 0$\nThis statement is correct. When considering the sum of differential probabilities over all non-zero output differences for an input difference of $0$, it will always sum to $0$. This is because for every possible non-zero output difference $b$, there exists another output difference $-b$ that cancels it out in the sum.\n\nD) $\\mathsf{DP}^S(0,b)=1$ if and only if $m$ is odd.\nThis statement is incorrect. The value of $\\mathsf{DP}^S(0,b)$ does not depend on the parity of $m$. It is determined by the specific Sbox $S$ and the output difference $b$.\n\nTherefore, the correct answer is (C) $\\sum_{b\\in \\{0,1\\}^m \\backslash \\{0\\}} \\mathsf{DP}^S(0,b)= 0$. ///Answer: C"}
{"subject": "", "question": "Question: Suppose that an item in a leaf node N exists in every path. Which one is correct? ?\n\nOptions:\nA. N co-occurs with its prefix in every transaction.\nB. For every node P that is a parent of N in the fp tree, confidence(P->N) = 1\nC. N\u2019s minimum possible support is equal to the number of paths.\nD. The item N exists in every candidate set.\n", "answer": "Explanations: For every node P that is a parent of N in the FP tree, confidence(P->N) = 1.\n\nThis statement may not be correct because the presence of N in every path does not necessarily imply a perfect association with its parent node P. Confidence measures the strength of the relationship between two items in a frequent itemset, and while N may exist in every path, its relationship with its parent node P may not always be perfectly correlated with a confidence score of 1. ///Answer: B"}
{"subject": "", "question": "Question: What are possible morphological analyses of \"drinks\"?(Penalty for wrong ticks)?\n\nOptions:\nA. N+s\nB. drink+VERB+p\nC. drink+ADJ\nD. drink+N+p\nE. drink+VERB+3+Sing+Present+Indicative\nF. drinks+N+p\n", "answer": "Explanations: The word \"drinks\" can have multiple morphological analyses depending on the context in which it is used. Here are some possible analyses:\n\n1. drink+VERB+3+Sing+Present+Indicative: This analysis breaks down \"drinks\" into the base form \"drink\" with the morphological features of a verb in the third person singular, present indicative tense.\n\n2. drink+N+p: This analysis breaks down \"drinks\" into the base form \"drink\" with the morphological features of a noun in the plural form.\n\n3. drinks+N+p: This analysis breaks down \"drinks\" as a noun in the plural form.\n\n4. N+s: This analysis categorizes \"drinks\" as a noun with a plural form.\n\nIt's important to consider the context in which \"drinks\" is used to determine the most appropriate morphological analysis. ///Answer: E"}
{"subject": "", "question": "Question: Tick the assertion related to an open problem.\n\nOptions:\nA. $NP\\subseteq IP$.\nB. $P\\subseteq IP$.\nC. $PSPACE=IP$.\nD. $NP = \\text{co-}NP$.\n", "answer": "Explanations: To identify the assertion related to an open problem, we need to consider the status of each statement in the options provided in terms of being an open problem in computer science.\n\n1. $NP\\subseteq IP$: This statement is not related to any known open problem in computer science. The relationship between $NP$ and $IP$ is not a significant open question.\n\n2. $P\\subseteq IP$: Similarly, this statement does not relate to any open problem. The relationship between $P$ and $IP$ is not an unresolved issue.\n\n3. $PSPACE=IP$: This statement is not related to an open problem. The equality or inequality between $PSPACE$ and $IP$ is a well-understood area in complexity theory.\n\n4. $NP = \\text{co-}NP$: This statement is related to a major open problem in computer science known as the $NP$ vs. $co-NP$ problem. Resolving this question is one of the seven Millennium Prize Problems established by the Clay Mathematics Institute.\n\nTherefore, the assertion related to an open problem is Option 4: $NP = \\text{co-}NP$. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{\\emph{incorrect}} assertion. In an interactive proof system for a language $L$, having zero-knowledge implies that $\\ldots$?\n\nOptions:\nA. $\\exists$ ppt $\\mathcal{S}$ such that, for any $\\mathbf{x \\in L}$, $\\mathcal{S}(x,r)$ produces an output indistinguishable from the view of a \\textbf{honest verifier} interacting with a \\textbf{honest prover}.\nB. $\\exists$ ppt $\\mathcal{S}$ such that, for any $\\mathbf{x \\in L}$, $\\mathcal{S}(x,r)$ produces an output indistinguishable from the view of a \\textbf{malicious verifier} interacting with a \\textbf{honest prover}.\nC. $\\exists$ ppt $\\mathcal{S}$ such that, for any $\\mathbf{x \\not\\in L}$, $\\mathcal{S}(x,r)$ produces an output indistinguishable from the view of a \\textbf{honest verifier} interacting with a \\textbf{malicious prover}.\nD. the prover proves the membership of $x$ to $L$ without revealing any secret to the verifier.\n", "answer": "Explanations: To identify the incorrect assertion in the given multiple-choice question related to interactive proof systems and zero-knowledge proofs, let's evaluate each option:\n\n1. '$\\exists$ ppt $\\mathcal{S}$ such that, for any $x \\in L$, $\\mathcal{S}(x,r)$ produces an output indistinguishable from the view of an honest verifier interacting with a honest prover.'\n   - This option describes the property of zero-knowledge, where a simulator $\\mathcal{S}$ can simulate the interaction between an honest prover and an honest verifier without revealing any additional information. This is a correct statement about zero-knowledge proofs.\n\n2. '$\\exists$ ppt $\\mathcal{S}$ such that, for any $x \\in L$, $\\mathcal{S}(x,r)$ produces an output indistinguishable from the view of a malicious verifier interacting with a honest prover.'\n   - This option describes a scenario where the simulator can convince a malicious verifier that the interaction with the prover is legitimate. This is also a valid property of zero-knowledge proofs.\n\n3. '$\\exists$ ppt $\\mathcal{S}$ such that, for any $x \\notin L$, $\\mathcal{S}(x,r)$ produces an output indistinguishable from the view of an honest verifier interacting with a malicious prover.'\n   - This option contradicts the definition of zero-knowledge proofs. In zero-knowledge proofs, the simulator should only be able to simulate interactions for instances in the language $L$, not for instances outside of $L$. Therefore, this assertion is incorrect.\n\n4. 'The prover proves the membership of $x$ to $L$ without revealing any secret to the verifier.'\n   - This statement aligns with the concept of zero-knowledge proofs, where the prover can convince the verifier of the truth of a statement without revealing any additional information apart from the validity of the statement. This is a correct aspect of zero-knowledge proofs.\n\nTherefore, based on logical reasoning and understanding of zero-knowledge proofs, option 3 is the incorrect assertion in the given question as it deviates from the fundamental principles of zero-knowledge protocols. ///Answer: C"}
{"subject": "", "question": "Question: When computing PageRank iteratively, the computation ends when:?\n\nOptions:\nA. The norm of the difference of rank vectors of two subsequent iterations falls below a predefined threshold\nB. The difference among the eigenvalues of two subsequent iterations falls below a predefined threshold\nC. All nodes of the graph have been visited at least once\nD. The probability of visiting an unseen node falls below a predefined threshold\n", "answer": "Explanations: The correct answer is a) The norm of the difference of rank vectors of two subsequent iterations falls below a predefined threshold.\n\nWhen computing PageRank iteratively, the algorithm updates the rank vector for each node in the graph based on the ranks of its neighboring nodes. This process is repeated until the rank vectors converge to a stable solution. \n\nOne common way to determine when to stop the iterative computation is to monitor the norm of the difference between the rank vectors of two subsequent iterations. When this norm falls below a predefined threshold, it indicates that the ranks have converged and further iterations are unlikely to significantly change the results.\n\nOption b) The difference among the eigenvalues of two subsequent iterations falls below a predefined threshold is not a common stopping criterion for PageRank computation. While eigenvalues play a role in the PageRank algorithm, monitoring the difference among them is not typically used to determine convergence.\n\nOption c) All nodes of the graph have been visited at least once is not a suitable stopping criterion for PageRank computation because the goal is not necessarily to visit all nodes, but rather to converge on stable rank values for each node.\n\nOption d) The probability of visiting an unseen node falls below a predefined threshold is also not a common stopping criterion for PageRank computation. The focus is on determining the rank values for each node, rather than specifically tracking the probability of visiting unseen nodes.\n\nIn conclusion, the norm of the difference of rank vectors of two subsequent iterations falling below a predefined threshold is the most appropriate criterion for ending the iterative computation when computing PageRank. ///Answer: A"}
{"subject": "", "question": "Question: Assume in top-1 retrieval recommendation 1 is (2, 3, 1) and recommendation 2 is (2, 1, 3)\n?\n\nOptions:\nA. RMSE(rec 1) < RMSE(rec 2) and DCG(rec 1) > DCG(rec 2)\nB. RMSE(rec 1) = RMSE(rec 2) and DCG(rec 1) > DCG(rec 2)\nC. RMSE(rec 1) < RMSE(rec 2) and DCG(rec 1) = DCG(rec 2)\nD. RMSE(rec 1) = RMSE(rec 2) and DCG(rec 1) = DCG(rec 2)\n", "answer": "Explanations: Option: 'RMSE(rec 1) < RMSE(rec 2) and DCG(rec 1) > DCG(rec 2)'\n\nExplanation:\n- RMSE (Root Mean Square Error) measures the average squared difference between predicted and actual values. In the context of recommendation systems, it quantifies how well the predicted rankings match the actual rankings. A lower RMSE indicates better accuracy.\n- DCG (Discounted Cumulative Gain) measures the quality of a ranking. It considers both the relevance and the position of an item in the list. A higher DCG indicates a better ranking.\n\nGiven that recommendation 1 is (2, 3, 1) and recommendation 2 is (2, 1, 3):\n- RMSE(rec 1) < RMSE(rec 2): This implies that the predicted ranking of recommendation 1 (2, 3, 1) is closer to the actual ranking than recommendation 2 (2, 1, 3). Therefore, rec 1 has a lower RMSE than rec 2.\n- DCG(rec 1) > DCG(rec 2): This indicates that recommendation 1 has a higher quality ranking than recommendation 2 based on relevance and position.\n\nTherefore, based on the given recommendations, the correct option is 'RMSE(rec 1) < RMSE(rec 2) and DCG(rec 1) > DCG(rec 2)'. ///Answer: A"}
{"subject": "", "question": "Question: Consider the classes below and tick the correct answer. class A class B extends A class C[-T]?\n\nOptions:\nA. C[C[B]] <: C[C[A]]\nB. C[C[A]] <: C[C[B]]\nC. None of the other answers is correct.\n", "answer": "Explanations: Reasons:\n1. In Scala, the notation `C[-T]` indicates a contravariant type parameter `T` for the class `C`. Contravariance means that if `B` is a subclass of `A`, then `C[B]` is a subclass of `C[A]`.\n2. Given the class hierarchy provided in the question: `class A`, `class B extends A`, and the contravariant class `C[-T]`, we need to determine the relationships between `C[C[B]]` and `C[C[A]]`.\n\nThe correct answer is: None of the other answers is correct.\n\nExplanation:\n- `C[C[B]]` and `C[C[A]]` are not directly comparable in terms of subtyping relationships due to contravariance.\n- Contravariance in Scala reverses the subtyping relationship. Therefore, `C[B]` is a subtype of `C[A]` if `B` is a subtype of `A` in a covariant scenario. However, in a contravariant scenario, `C[B]` is a supertype of `C[A]` if `B` is a subtype of `A`.\n- In the provided question, we are dealing with contravariant type parameters. This means that `C[C[B]]` is a supertype of `C[C[A]]` because `B` is a subtype of `A`.\n- Therefore, none of the given answer options is correct as the correct relationship is that `C[C[B]]` is a supertype of `C[C[A]]` due to contravariance. ///Answer: C"}
{"subject": "", "question": "Question: What could Out of Vocabulary (OoV) forms consist of?\u00a0Select all that apply.A penalty will be applied for wrong answers.\n\nOptions:\nA. Words from the lexicon\nB. Words borrowed from other languages\nC. Words with spelling errors\nD. Neologisms\nE. Abbreviations\n", "answer": "Explanations: Out of Vocabulary (OoV) forms typically consist of the following:\n\n1. Words with spelling errors: These are words that are misspelled, leading to them being considered out of vocabulary as they do not match any valid word in the lexicon.\n\n2. Neologisms: These are newly coined words or expressions that have not yet been widely accepted into the lexicon. As a result, they may be considered out of vocabulary until they become more commonly recognized.\n\n3. Abbreviations: Abbreviated forms of words or phrases may not always be recognized by language models, especially if they are not commonly used or if the model has not been trained on them.\n\nWords from the lexicon, words borrowed from other languages, usually have counterparts or representations in the lexicon and are typically not considered OoV forms unless they are very specific or rare words that are not commonly included in the lexicon used by the language model. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following hold(s) true about update deployment in the secure\n    development lifecycle?\n\nOptions:\nA. Updates may bring new code that may be buggy, so additional\n            monitoring is required after deploying an update.\nB. One motivation for automatic updates is for manufacturers to\n            ensure that users have the latest code installed.\nC. You should always deploy third party updates automatically\n            and immediately in your project.\nD. Not allowing rolling back to previous versions is necessary\n            in the Secure Development Lifecycle.\n", "answer": "Explanations: The first statement \"Updates may bring new code that may be buggy, so additional monitoring is required after deploying an update\" holds true about update deployment in the secure development lifecycle. This statement emphasizes the importance of monitoring the system after updates to ensure that any unforeseen issues or bugs are identified and addressed promptly.\n\nThe second statement \"One motivation for automatic updates is for manufacturers to ensure that users have the latest code installed\" is also true. Automatic updates help manufacturers ensure that users have the most up-to-date software, which is crucial for patching security vulnerabilities and improving overall performance.\n\nThe third statement \"You should always deploy third party updates automatically and immediately in your project\" is not entirely accurate. While it is essential to stay current with third-party updates to address security vulnerabilities, immediately deploying them without proper testing and verification can introduce new risks. It is recommended to test third-party updates in a controlled environment before deploying them in production.\n\nThe fourth statement \"Not allowing rolling back to previous versions is necessary in the Secure Development Lifecycle\" is not true. Allowing the ability to roll back to previous versions can be crucial in case an update causes unexpected issues or disruptions. It provides a safety net to revert to a stable state while the root cause of the problem is investigated and resolved. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is/are true about stack canaries?\n\nOptions:\nA. The name \u201cstack canaries\u201d stems from birds being used to detect dangerous gas in mines.\nB. Stack canaries do not protect against format string vulnerabilities.\nC. Stack canaries on Linux are generated per thread.\nD. Stack canary instrumentation is, by default, inserted into every single function by the compiler.\nE. Stack canaries on Linux are generated per process.\n", "answer": "Explanations: 1. Understand what stack canaries are: Stack canaries are a security feature used in software to detect buffer overflows by placing a random value on the stack before the return address of a function. If this value is overwritten, it indicates a buffer overflow attack.\n\n2. Evaluate Option A: The statement that the name \"stack canaries\" stems from birds being used to detect dangerous gas in mines is false. This does not relate to the actual purpose or origin of stack canaries. Therefore, Option A is not true.\n\n3. Evaluate Option B: The statement that stack canaries do not protect against format string vulnerabilities is true. Stack canaries are designed to protect against buffer overflows, not format string vulnerabilities. Therefore, Option B is true.\n\n4. Evaluate Option C: The statement that stack canaries on Linux are generated per thread is false. Stack canaries are typically generated per process, not per thread. Therefore, Option C is not true.\n\n5. Evaluate Option D: The statement that stack canary instrumentation is, by default, inserted into every single function by the compiler is false. Stack canaries are not inserted into every function by default, as it can introduce performance overhead. Therefore, Option D is not true.\n\n6. Evaluate Option E: The statement that stack canaries on Linux are generated per process is true. Stack canaries are typically generated per process to provide protection against buffer overflows. Therefore, Option E is true.\n\nIn conclusion, the true statements about stack canaries are:\n\n- Stack canaries do not protect against format string vulnerabilities (Option B).\n- Stack canaries on Linux are generated per process (Option E). ///Answer: B"}
{"subject": "", "question": "Question: We want to return, from the two posting lists below, the top-2 documents matching a\nquery using Fagin\u2019s algorithm with the aggregation function taken as the sum of the tf-idf weights. How many entries (total of both lists) are accessed in the first phase of the\nalgorithm performing round-robin starting at List 1 (i.e., before performing the random\naccess)?\n\nOptions:\nA. 4\nB. 6\nC. 8\nD. 10\n", "answer": "Explanations: To determine the number of entries accessed in the first phase of Fagin's algorithm (round-robin phase) before performing random access, we need to consider the given posting lists and the top-2 documents matching a query using the sum of tf-idf weights as the aggregation function.\n\nGiven Posting Lists:\nList 1:\n- (doc1, 0.5), (doc2, 0.6), (doc3, 0.3), (doc4, 0.2)\nList 2:\n- (doc2, 0.4), (doc3, 0.7), (doc4, 0.5), (doc5, 0.1)\n\nFagin's Algorithm Process:\n1. We start by performing a round-robin traversal of the two posting lists until we find the top-2 documents based on the sum of tf-idf weights.\n2. We sum the tf-idf weights of the same document that appears in both lists.\n3. We keep track of the running total of tf-idf weights as we iterate through the lists.\n\nLet's perform the round-robin traversal:\n\n- At the beginning, we are at the start of both lists.\n- We compare the tf-idf weights of the first entries in both lists: (doc1, 0.5) and (doc2, 0.4).\n- We move to the next entry in List 2 since doc2 has a higher weight.\n- We compare the next entries: (doc1, 0.5) and (doc3, 0.7).\n- We move to the next entry in List 1 since doc3 has a higher weight.\n- We compare the next entries: (doc2, 0.6) and (doc3, 0.7).\n- We move to the next entry in List 1 since doc3 still has a higher weight.\n- We compare the next entries: (doc2, 0.6) and (doc4, 0.5).\n- We move to the next entry in List 2 since doc4 has a higher weight.\n- We compare the next entries: (doc2, 0.4) and (doc4, 0.5).\n- We move to the next entry in List 1 since doc4 has a higher weight.\n\nThe total number of entries accessed in the first phase of the algorithm performing round-robin starting at List 1 is 6.\n\nTherefore, the correct answer is:\n\n**B: 6** ///Answer: B"}
{"subject": "", "question": "Question: Does the following code compile? given Ordering[Int] with \\t def compare(x: Int, y: Int): Int = y - x given Ordering[Int] with \\t def compare(x: Int, y: Int): Int = x - y List(1, 2, 3).sorted?\n\nOptions:\nA. Yes\nB. No\n", "answer": "Explanations: No, the code will not compile.\n The reason is that when calling the `sorted` method on a `List[Int]`, it expects an implicit `Ordering[Int]` to be in scope to determine the sorting order of the elements. In this case, you are defining two conflicting implicit `Ordering[Int]` instances with the same priority. This ambiguity leads to a compilation error due to the conflicting definitions.\n To resolve this issue, you should only define one `Ordering[Int]` instance in the scope or explicitly provide the `Ordering` instance when calling the `sorted` method. ///Answer: B"}
{"subject": "", "question": "Question: Let $G_1, G_2, G_3$, be valid generator matrices of dimensions $\\mathbb F^{k_i \times n_i}$, all over the same field $\\mathbb F$. Recall that ``valid'' means that for all $i$, $k_i \\leq n_i$ and $\text{rank}(G_i) = k_i$. True or false:  Assuming $k_1 = k_2 + k_3$, the matrix $\begin{pmatrix} G_1 &\\vline &\begin{matrix} G_2 &\\vline &0\\ \\cline{1-3} 0 &\\vline &G_3 \\end{matrix} \\end{pmatrix}$ is also a valid generator matrix.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: Let's analyze the question step by step:\n\n1. The dimensions of the given matrices are:\n   - $G_1$ is $k_1 \\times n_1$\n   - $G_2$ is $k_2 \\times n_2$\n   - $G_3$ is $k_3 \\times n_3$\n\n2. Given that $k_1 = k_2 + k_3$, we can rewrite the dimensions of the matrices as follows:\n   - $G_1$ is $(k_2 + k_3) \\times n_1$\n   - $G_2$ is $k_2 \\times n_2$\n   - $G_3$ is $k_3 \\times n_3$\n\n3. Now, we need to construct the combined matrix:\n   $M = \\begin{pmatrix} G_1 & \\vline & G_2 & \\vline & 0 \\\\ 0 & \\vline & G_3 \\end{pmatrix}$\n\n4. The dimensions of $M$ will be $(k_2 + k_3) \\times (n_1 + n_2)$.\n\n5. We need to show that the rank of $M$ is equal to $k_1 = k_2 + k_3$ for it to be a valid generator matrix.\n\n6. By construction, it is clear that the rank of $M$ will be equal to $k_1 = k_2 + k_3$ (each block $G_i$ has full rank). \n\nTherefore, the statement is true. ///Answer: B"}
{"subject": "", "question": "Question: A major specificity of natural languages is that they are inherently implicit and ambiguous. How should this be taken into account in the NLP perspective?\n(penalty for wrong ticks)?\n\nOptions:\nA. by teaching humans to talk and write in a way that reduces implicitness and ambiguity\nB. by designing NLP algorithms and data structures able to efficiently cope with very ambiguous representations\nC. by interacting with human experts to formulate precise interpretation rules for linguistic entities\nD. by increasing the amount of a priori knowledge that NLP systems are able to exploit\n", "answer": "Explanations: To provide the most correct and accurate answer to this question, let's break down each option:\n\nA) Teaching humans to talk and write in a way that reduces implicitness and ambiguity: This option is not feasible as natural languages are inherently implicit and ambiguous due to the nature of human communication. It is not practical or realistic to expect humans to completely eliminate ambiguity and implicitness in their language use.\n\nB) Designing NLP algorithms and data structures able to efficiently cope with very ambiguous representations: This option is the most appropriate approach to addressing the inherent implicitness and ambiguity of natural languages in the NLP perspective. NLP algorithms can be designed to handle ambiguity through techniques such as probabilistic modeling, context analysis, and machine learning to accurately interpret and process natural language data.\n\nC) Interacting with human experts to formulate precise interpretation rules for linguistic entities: While human experts can provide valuable insights and guidance in developing NLP systems, relying solely on precise interpretation rules may not fully address the inherent ambiguity of natural languages. Interpretation rules can help in specific contexts but may not cover all possible linguistic nuances.\n\nD) Increasing the amount of a priori knowledge that NLP systems are able to exploit: While a priori knowledge can enhance the performance of NLP systems, simply increasing the amount of knowledge may not necessarily reduce implicitness and ambiguity in natural languages. NLP systems need to be able to adapt and learn from data to effectively handle the complexities of human language.\n\nTherefore, the most appropriate answer to the question is B) by designing NLP algorithms and data structures able to efficiently cope with very ambiguous representations. This approach acknowledges the challenges posed by the implicitness and ambiguity of natural languages and focuses on developing advanced algorithms to effectively process and interpret linguistic data in the NLP perspective. ///Answer: B"}
{"subject": "", "question": "Question: In an FP tree, the leaf nodes are the ones with:?\n\nOptions:\nA. Lowest confidence\nB. Lowest support\nC. Least in the alphabetical order\nD. None of the other options.\n", "answer": "Explanations: Answer: The correct answer is: Lowest support.\n\nReasoning: In an FP tree (Frequent Pattern tree), the leaf nodes represent frequent itemsets in the dataset. The support of an itemset is the number of transactions in which the itemset appears. Therefore, the leaf nodes of an FP tree will have the lowest support, as they represent the least frequent itemsets in the dataset. \n\nThe other options are not correct because the confidence level or alphabetical order of the itemsets does not determine their position as leaf nodes in an FP tree. ///Answer: B"}
{"subject": "", "question": "Question: In a first-preimage attack \\dots?\n\nOptions:\nA. you are given a hash function $h$ and you have to find two messages $m\\neq m'$ such that $h(m) = h(m')$.\nB. you are given a hash function $h$ and and a message $m$ and you have to have to find an $x$ such that $h(m)=x$.\nC. you are given a hash function $h$ and a message $m$ and you have to find $m'\\neq m$ such that $h(m) = h(m')$.\nD. you are given a hash function $h$ and a value $x$ and you have to have to find an $m$ such that $h(m)=x$.\n", "answer": "Explanations: In a first-preimage attack, the correct option is:\n\n- you are given a hash function $h$ and a value $x$ and you have to have to find an $m$ such that $h(m)=x$.\n\nExplanation: In a first-preimage attack, the goal is to find a message $m$ that hashes to a specific value $x$ under the given hash function $h$. This differs from a second-preimage attack where the attacker aims to find another message $m' \\neq m$ that hashes to the same value $x$. ///Answer: D"}
{"subject": "", "question": "Question: When using linear regression, which techniques improve your result? (One or multiple answers)?\n\nOptions:\nA. linear regression does not allow polynomial features\nB. polynomial combination of features\nC. adding new features that are non-linear combination of existing features\nD. because the linear nature needs to be preserved, non-linear combination of features are not allowed\n", "answer": "Explanations: The techniques that improve results when using linear regression include polynomial combination of features and adding new features that are non-linear combinations of existing features. By incorporating polynomial features, the model can capture non-linear relationships between the variables, improving the accuracy of the predictions. Additionally, introducing new features that are non-linear combinations of existing features can help the model better fit the data and make more accurate predictions. ///Answer: B"}
{"subject": "", "question": "Question: Let $X$ denote the random variable associated to the plaintexts and $Y$ the random variable associated to the corresponding ciphertexts. If a cryptosystem achieves perfect secrecy, then we have that \\dots?\n\nOptions:\nA. $\\Pr [X=x|Y=y] = \\Pr[Y=y]$.\nB. $\\Pr [X=x|Y=y] = \\Pr[X=x]$.\nC. $\\Pr [X=x|Y=y] = \\Pr[X=x,Y=y]$.\nD. $\\Pr [X=x] = \\Pr[Y=y]$.\n", "answer": "Explanations: Answer: $\\Pr [X=x|Y=y] = \\Pr[Y=y]$.\n\nExplanation: In a cryptosystem that achieves perfect secrecy, the probability of a specific plaintext $x$ given a specific ciphertext $y$ is equal to the probability of observing that specific ciphertext $y$. This means that given a ciphertext, the likelihood of a particular plaintext being the original message is independent of the ciphertext, which is a key characteristic of perfect secrecy. The formula $\\Pr [X=x|Y=y] = \\Pr[Y=y]$ represents this concept mathematically, showing that the conditional probability of a plaintext given a ciphertext is equal to the probability of observing that ciphertext. This relationship is crucial in understanding how perfect secrecy is achieved in cryptosystems and is a fundamental concept in cryptography. ///Answer: A:"}
{"subject": "", "question": "Question: Consider an RSA encryption where the $(p, q)$ are determined as $(53, 61)$. True or false: $(e,d) = (319, 23)$ are valid encoding/decoding exponents.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: To determine if $(e,d) = (319, 23)$ are valid encoding/decoding exponents, we need to check if they satisfy the conditions for RSA encryption.\n\nRecall that in RSA encryption, we choose two distinct prime numbers $p$ and $q$, calculate $n = p \\times q$, then compute Euler's totient function $\\phi(n) = (p-1)(q-1)$. \n\nNext, we choose an encryption exponent $e$ such that $1 < e < \\phi(n)$ and $\\text{gcd}(e, \\phi(n)) = 1$. The decryption exponent $d$ is then determined as the modular multiplicative inverse of $e$ modulo $\\phi(n)$, i.e., $d \\equiv e^{-1} \\pmod{\\phi(n)}$.\n\nGiven that $p = 53$ and $q = 61$, we first calculate $n = p \\times q = 53 \\times 61 = 3233$. Then, $\\phi(n) = (p-1)(q-1) = 52 \\times 60 = 3120$.\n\nNow, let's check if $(e,d) = (319, 23)$ are valid:\n1. Verify that $1 < e < \\phi(n)$ and $\\text{gcd}(e, \\phi(n)) = 1$:\n   - We have $319 < 3120$.\n   - To check $\\text{gcd}(319, 3120)$, we can use the Euclidean algorithm: \n     \\begin{align*}\n     3120 &= 9 \\times 319 + 129 \\\\\n     319 &= 2 \\times 129 + 61 \\\\\n     129 &= 2 \\times 61 + 7 \\\\\n     61 &= 8 \\times 7 + 5 \\\\\n     7 &= 1 \\times 5 + 2 \\\\\n     5 &= 2 \\times 2 + 1 \\\\\n     2 &= 2 \\times 1 + 0\n     \\end{align*}\n     The last non-zero remainder is $\\text{gcd}(319,3120) = 1$, so $\\text{gcd}(319,3120) = 1$.\n\n2. Calculate the decryption exponent $d$ using the modular multiplicative inverse:\n   To find the modular multiplicative inverse of $e=319$ modulo $\\phi(n)=3120$, we need to solve the congruence $319d \\equiv 1 \\pmod{3120}$.\n   By solving this congruence, we find that $d=23$.\n\nTherefore, $(e,d) = (319,23)$ are valid encoding/decoding exponents for the given RSA encryption with $(p,q) = (53,61)$.\n\nThe correct answer is:\n(b) True ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. Assume an arbitrary $f:\\{0,1\\}^p \\rightarrow \\{0,1\\}^q$, where $p$ and $q$ are integers.\n\nOptions:\nA. $\\mathsf{DP}^f(a,b)=\\displaystyle\\Pr_{X\\in_U\\{0,1\\}^p}[f(X\\oplus a)\\oplus f(X)\\oplus b=1]$, for all $a \\in \\{0,1\\}^p$, $b \\in \\{0,1\\}^q$.\nB. $\\Pr[f(x\\oplus a)\\oplus f(x)\\oplus b=0]=E(\\mathsf{DP}^f(a,b))$, for all $a, x \\in \\{0,1\\}^p$, $b \\in \\{0,1\\}^q$.\nC. $2^p\\mathsf{DP}^f(a,b)$ is odd, for all $a \\in \\{0,1\\}^p, b \\in \\{0,1\\}^q$.\nD. $\\displaystyle\\sum_{b\\in\\{0,1\\}^q} \\mathsf{DP}^f(a,b)=1$, for all $a \\in \\{0,1\\}^p$.\n", "answer": "Explanations: Let's go through each option one by one:\n\n1. $\\mathsf{DP}^f(a,b)=\\displaystyle\\Pr_{X\\in\\{0,1\\}^p}[f(X\\oplus a)\\oplus f(X)\\oplus b=1]$, for all $a \\in \\{0,1\\}^p$, $b \\in \\{0,1\\}^q$.\n\nThis is the definition of the Directed Probability (DP) function for the given function $f$. It represents the probability that the output of $f$ changes when a specific input bit is flipped. This assertion is true by definition.\n\n2. $\\Pr[f(x\\oplus a)\\oplus f(x)\\oplus b=0]=E(\\mathsf{DP}^f(a,b))$, for all $a, x \\in \\{0,1\\}^p$, $b \\in \\{0,1\\}^q$.\n\nThis assertion is also true. It states that the probability that the output of $f$ remains unchanged when a specific input bit is flipped is equal to the expected value of the DP function for that specific input bit.\n\n3. $2^p\\mathsf{DP}^f(a,b)$ is odd, for all $a \\in \\{0,1\\}^p, b \\in \\{0,1\\}^q$.\n\nThis assertion is false. $2^p\\mathsf{DP}^f(a,b)$ can be even or odd depending on the function $f$ and the specific values of $a$ and $b$. There is no general rule that it must be odd for all inputs.\n\n4. $\\displaystyle\\sum_{b\\in\\{0,1\\}^q} \\mathsf{DP}^f(a,b)=1$, for all $a \\in \\{0,1\\}^p$.\n\nThis assertion is false as well. The sum of all DP values for a fixed $a$ should not necessarily equal 1, as it depends on the function $f$ and the input $a$.\n\nTherefore, the true assertions are options 1 and 2. ///Answer: A"}
{"subject": "", "question": "Question: Let $S_{0},S_{1},S_{2},\\dots$ be an infinite sequence produced by a source $\\mathcal{S}$. All $S_{n}$ take values in $\\{0,1\\}$, and $S_{n+1}$ depends only on $S_n$, that is,   $p_{S_{n+1} | S_0, \\dots, S_n}(s_{n+1} | s_0, \\dots, s_n) = p_{S_{n+1} | S_n}(s_{n+1} | s_n)$.   The probability $p_{S_{n+1}|S_{n}}$ is schematically represented in the graph below:     \begin{center}    \tikzset{      state/.style = {        draw,        circle,        minimum size = 20pt,        font = \fontsize{12}{12}\\selectfont,      }    }    \begin{tikzpicture}[> = latex]      \node[state] (a) {$0$};      \node[state, right of = a] (b) {$1$};      \\path[->] (a) edge[bend left, above] node {$1/2$} (b);      \\path[->] (a) edge[loop left] node {$\frac{1}{2}$} ();      \\path[->] (b) edge[bend left, below] node {$1/4$} (a);      \\path[->] (b) edge[loop right] node {$\frac{3}{4}$} ();    \\end{tikzpicture}  \\end{center}  For instance, the edge from $0$ to $1$ means that $p_{S_{n+1}|S_{n}}(1|0) = \frac{1}{2}$. We also have that $p_{S_0}(0)=1$. True or false: $H^\\star(\\mathcal{S})$ is finite.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: Let's reconsider the problem with a more detailed explanation:\n\nIn the given source $\\mathcal{S}$, we have two states: $0$ and $1$. The transition probabilities are as follows:\n- From state $0$: $p_{S_{n+1} | S_n}(1 | 0) = \\frac{1}{2}$ and $p_{S_{n+1} | S_n}(0 | 0) = \\frac{1}{2}$\n- From state $1$: $p_{S_{n+1} | S_n}(0 | 1) = \\frac{1}{4}$ and $p_{S_{n+1} | S_n}(1 | 1) = \\frac{3}{4}$\n\nGiven that $p_{S_0}(0) = 1$, the source starts in state $0$ with certainty.\n\nTo determine if $H^\\star(\\mathcal{S})$ is finite, we need to analyze if the sequence reaches a point where the conditional probabilities stabilize. In this case, the sequence oscillates between states $0$ and $1$ indefinitely, without converging to a fixed distribution. This oscillation prevents the entropy rate from converging to a finite value.\n\nSince the sequence does not reach a point of convergence and the oscillation between states continues indefinitely, the entropy rate $H^\\star(\\mathcal{S})$ is infinite.\n\nTherefore, the correct answer is: False. The entropy rate $H^\\star(\\mathcal{S})$ is infinite. ///Answer: B"}
{"subject": "", "question": "Question: (Nearest Neighbor) The training error of the 1-nearest neighbor classifier is zero.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: A. True\n\nJustification: The statement is true because the 1-nearest neighbor classifier memorizes the training data during training, and therefore, the training error of the 1-nearest neighbor classifier is indeed zero since it classifies each training example correctly. ///Answer: A"}
{"subject": "", "question": "Question: Given the following data structure: enum IntSet: \\t case Empty \\t case NonEmpty(x: Int, l: IntSet, r: IntSet) And the following lemmas, holding for all x: Int, xs: List[Int], ys: List[Int], l: IntSet and r: IntSet: (SizeNil) nil.size === 0 (SizeCons) (x :: xs).size === xs.size + 1 (ConcatSize) (xs ++ ys).size === xs.size + ys.size (TreeSizeEmpty) Empty.treeSize === 0 (TreeSizeNonEmpty) NonEmpty(x, l, r).treeSize === l.treeSize + r.treeSize + 1 (ToListEmpty) Empty.toList === nil (ToListNonEmpty) NonEmpty(x, l, r).toList === l.toList ++ (x :: r.toList) Let us prove the following lemma for all s: IntSet: (ToListSize) s.toList.size === s.treeSize We prove it by induction on s. Base case: s is Empty. Therefore, we need to prove: Empty.toList.size === Empty.treeSize Starting from the left hand-side (Empty.toList.size), what exact sequence of lemmas should we apply to get the right hand-side (Empty.treeSize)?\n\nOptions:\nA. SizeNil, ToListEmpty, TreeSizeEmpty\nB. ToListEmpty, TreeSizeEmpty, SizeNil\nC. SizeNil, TreeSizeEmpty, ToListEmpty\nD. TreeSizeEmpty, SizeNil, TreeSizeEmpty\nE. ToListEmpty, SizeNil, TreeSizeEmpty\nF. TreeSizeEmpty, ToListEmpty, SizeNil\n", "answer": "Explanations: The correct option is: ToListEmpty, SizeNil, TreeSizeEmpty. \n\nWe first apply the ToListEmpty lemma to get nil.size, then apply the SizeNil lemma to get 0, and finally apply the TreeSizeEmpty lemma to get the right-hand side, which is 0. ///Answer: E"}
{"subject": "", "question": "Question: Which of the following statement(s) is/are correct?\n\nOptions:\nA. An information leak can be a preparation step of control-flow hijacking.\nB. An attacker-controlled format string can lead to arbitrary write.\nC. In format strings, \\%n prints a hex value\nD. When constructing a ROP payload, we use gadgets from all currently running processes\n", "answer": "Explanations: Among the given statements, the correct statement is:\n\nOption 0: An information leak can be a preparation step of control-flow hijacking.\n\nExplanation:\n1. An information leak can be a crucial step in many types of attacks, including control-flow hijacking. It involves obtaining sensitive data from the target system, such as memory addresses or configuration details, which can then be used to craft and execute further stages of the attack.\n2. Control-flow hijacking is a technique used by attackers to redirect the flow of a program's execution, typically to execute malicious code or gain unauthorized access.\n\nThe other statements are incorrect because:\n- Option 1: An attacker-controlled format string can lead to arbitrary write: This statement is generally incorrect. An attacker-controlled format string vulnerability can lead to information disclosure or arbitrary read, but not necessarily arbitrary write.\n- Option 2: In format strings, \\%n prints a hex value: This statement is incorrect. In format strings, the `%n` specifier is used to write the number of characters written so far into a variable.\n- Option 3: When constructing a ROP payload, we use gadgets from all currently running processes: This statement is incorrect. Return-Oriented Programming (ROP) involves constructing a payload using existing code snippets (gadgets) from the target process's memory space, not from all currently running processes. ///Answer: A"}
{"subject": "", "question": "Question: Let us assume that a kernel $K: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ is said to be valid if there exists $k \\in \\mathbb{N}$ and $\\Phi: \\mathcal{X} \\rightarrow \\mathbb{R}^{k}$ such that for all $\\left(x, x^{\\prime}\\right) \\in \\mathcal{X} \\times \\mathcal{X}, K\\left(x, x^{\\prime}\\right)=\\Phi(x)^{\\top} \\Phi\\left(x^{\\prime}\\right)$ Which one of the following kernels is not valid ?\n\nOptions:\nA. $\\mathcal{X}=\\mathbb{N}, K\\left(x, x^{\\prime}\\right)=2$\nB. $\\mathcal{X}=\\mathbb{R}^{d}, K\\left(x, x^{\\prime}\\right)=\\left(x^{\\top} x^{\\prime}\\right)^{2}$\nC. $\\mathcal{X}=\\mathbb{R}, K\\left(x, x^{\\prime}\\right)=\\cos \\left(x-x^{\\prime}\\right)$\nD. All of the proposed kernels are in fact valid.\nE. $\\mathcal{X}=\\mathbb{Q}, K\\left(x, x^{\\prime}\\right)=2^{x+x^{\\prime}}$\nF. $\\mathcal{X}=\\mathbb{R}^{d}, K\\left(x, x^{\\prime}\\right)=x^{\\top} A x^{\\prime}$, where $A$ is a $d \\times d$ symmetric positive semi-definite matrix.\n", "answer": "Explanations: To determine which of the given kernels is not valid, we need to check if there exists $k \\in \\mathbb{N}$ and a feature map $\\Phi: \\mathcal{X} \\rightarrow \\mathbb{R}^{k}$ that satisfies the condition for all pairs of points in $\\mathcal{X}$.\n\nLet's analyze each kernel one by one:\n\n1. $\\mathcal{X}=\\mathbb{N}, K(x, x')=2$\n\nFor any two points $x, x' \\in \\mathbb{N}$, $K(x, x') = 2$. This kernel is constant, and it does not depend on the inputs. To express this kernel as an inner product $\\Phi(x)^{\\top} \\Phi(x')$, we would need an infinite-dimensional feature space, which is not practical. So, this kernel is not valid.\n\n2. $\\mathcal{X}=\\mathbb{R}^{d}, K(x, x')=(x^\\top x')^2$\n\nFor this kernel, we can define the feature map $\\Phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d+1}$ as $\\Phi(x) = [x, \\sqrt{2}x_1, \\sqrt{2}x_2, \\ldots, \\sqrt{2}x_d]$. Then, $K(x, x') = \\Phi(x)^\\top \\Phi(x')$, showing that this kernel is valid.\n\n3. $\\mathcal{X}=\\mathbb{R}, K(x, x')=\\cos(x-x')$\n\nThe cosine kernel can be expressed as an inner product in an infinite-dimensional feature space using trigonometric functions, making it valid. So, this kernel is valid.\n\n4. $\\mathcal{X}=\\mathbb{Q}, K(x, x')=2^{x+x'}$\n\nFor rational numbers, the exponential function $2^{x+x'}$ may not be representable as an inner product in a finite-dimensional space, implying that this kernel is not valid.\n\n5. $\\mathcal{X}=\\mathbb{R}^{d}, K(x, x')=x^\\top A x'$\n\nFor this kernel, $K(x, x')$ can be expressed as an inner product in a feature space induced by the positive semi-definite matrix $A$. This kernel is valid.\n\nTherefore, the kernel that is not valid among the given choices is when $\\mathcal{X}=\\mathbb{N}, K(x, x')=2$. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is correct regarding schemas and ontologies?\n\nOptions:\nA.  An ontology is created from constructing mappings between schemas\nB. Ontologies can be used for reasoning about different schemas\nC. Ontologies always require a schema\nD. Semi-structured data cannot have a schema\n", "answer": "Explanations: The correct answer is: 'Ontologies can be used for reasoning about different schemas'.\n\nLet's break down each option to understand why this is the correct answer:\n\n1. 'An ontology is created from constructing mappings between schemas': This statement is incorrect. An ontology is a formal and explicit specification of a shared conceptualization. It defines the types, properties, and interrelationships of the entities within a domain. Ontologies are not created by constructing mappings between schemas.\n\n2. 'Ontologies can be used for reasoning about different schemas': This statement is correct. Ontologies provide a way to represent knowledge and to reason about concepts within a domain. They can be used to integrate information from different schemas by defining common concepts and relationships between them.\n\n3. 'Ontologies always require a schema': This statement is incorrect. While ontologies can be used to define the structure and semantics of a domain, they do not always require a schema. Ontologies are more focused on capturing the meaning and relationships between concepts rather than the structure itself.\n\n4. 'Semi-structured data cannot have a schema': This statement is incorrect. Semi-structured data can have a schema, although it may not be as rigid or formal as the schema for structured data. Schemas for semi-structured data can be defined using formats such as JSON or XML.\n\nTherefore, the correct answer is that ontologies can be used for reasoning about different schemas, as they provide a way to represent and reason about concepts in a domain, regardless of the underlying schema structures. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following congruence classes has a multiplicative inverse?\n\nOptions:\nA. $[3^{10} 2^5 14]_{19}$\nB. $[126]_{147}$\nC. $[60]_{15}$\n", "answer": "Explanations: To determine if a congruence class has a multiplicative inverse, we need to check if the number is relatively prime to the modulus.\n\n1. $[3^{10} 2^5 14]_{19}$\n\nFirst, calculate $3^{10} \\equiv 8 \\pmod{19}$ and $2^5 \\equiv 3 \\pmod{19}$. Then, $[3^{10} 2^5 14]_{19} \\equiv [8 \\cdot 3 \\cdot 14]_{19} \\equiv [336]_{19} \\equiv [17]_{19}$.\n\nNow, we check if 17 is relatively prime to 19. Since gcd(17,19) = 1, 17 has a multiplicative inverse.\n\n2. $[126]_{147}$\n\nCheck if 126 is relatively prime to 147. Since gcd(126,147) = 21 \u2260 1, 126 does not have a multiplicative inverse.\n\n3. $[60]_{15}$\n\nCheck if 60 is relatively prime to 15. Since gcd(60,15) = 15 \u2260 1, 60 does not have a multiplicative inverse.\n\nTherefore, the congruence class $[3^{10} 2^5 14]_{19}$ has a multiplicative inverse, while the other two do not. ///Answer: A"}
{"subject": "", "question": "Question: Does AddressSanitizer prevent \\textbf{all} use-after-free bugs?\n\nOptions:\nA. No, because quarantining free\u2019d memory chunks forever prevents\n            legit memory reuse and could potentially lead to out-of-memory\n            situations.\nB. No, because UAF detection is not part of ASan's feature set.\nC. Yes, because free\u2019d memory chunks are poisoned.\nD. Yes, because free\u2019d memory is unmapped and accesses therefore\n            cause segmentation faults.\n", "answer": "Explanations: D) Yes, because free'd memory is unmapped and accesses therefore cause segmentation faults.\n\nAddressSanitizer (ASan) is a memory error detector that can detect various memory issues, including use-after-free bugs. When memory is freed, ASan marks that memory as unaddressable, meaning that any attempt to access that memory will result in a segmentation fault. This helps to identify and prevent use-after-free bugs by catching them at runtime and providing valuable information for debugging. While ASan is a powerful tool for detecting memory errors, it may not catch all possible issues, but it is effective in detecting many common memory-related bugs. ///Answer: D"}
{"subject": "", "question": "Question: Let $\\mathcal C_1$ be a linear code over $\\mathbb F_3^n$, and let $\\mathcal C_2$ be a linear code over $\\mathbb F_2^n$. True or false: $\\mathcal C_1 \\cap \\mathcal C_2$ is necessarily a linear code over $\\mathbb F_3^n$.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: False. \n\nExplanation:\nWhen we take the intersection of two linear codes, the resulting code may not be a linear code in general. The intersection of two linear codes is not necessarily a linear code unless one of the codes is contained in the other.\n\nIn this case, $\\mathcal C_1 \\cap \\mathcal C_2$ may not be a linear code over $\\mathbb F_3^n$ because the intersection of a linear code over $\\mathbb F_3^n$ and a linear code over $\\mathbb F_2^n$ does not guarantee the closure of the resulting code under addition and scalar multiplication in $\\mathbb F_3^n$.\n\nTherefore, the statement that $\\mathcal C_1 \\cap \\mathcal C_2$ is necessarily a linear code over $\\mathbb F_3^n$ is false. ///Answer: A:"}
{"subject": "", "question": "Question: The following member of the exponential family represents a scalar Gaussian: $p(y)=\\exp \\left\\{(2,-1)\\left(y, y^{2}\\right)^{\\top}-\\right.$ $\\left.1-\\frac{1}{2} \\ln (\\pi)\\right\\}$. What are the mean $\\mu$ and the variance $\\sigma^{2}$ ?\n\nOptions:\nA. (a) $\\mu=-1, \\sigma^{2}=0$.\nB. (b) $\\mu=0, \\sigma^{2}=0$.\nC. (c) $\\mu=1, \\sigma^{2}=0$.\nD. (d) $\\mu=-1, \\sigma^{2}=\\frac{1}{2}$\nE. (e) $\\mu=0, \\sigma^{2}=\\frac{1}{2}$.\nF. (f) $\\mu=1, \\sigma^{2}=\\frac{1}{2}$.\nG. (g) $\\mu=-1, \\sigma^{2}=1$.\nH. (h) $\\mu=0, \\sigma^{2}=1$.\nI. (i) $\\mu=1, \\sigma^{2}=1$\n", "answer": "Explanations: The correct option is (h) $\\mu=0, \\sigma^{2}=1$.\n\nTo find the mean $\\mu$ and variance $\\sigma^{2}$ of the scalar Gaussian distribution represented by the given member of the exponential family, we need to first identify the natural parameters. In this case, the natural parameters are $(2, -1)$.\n\nThe mean $\\mu$ and variance $\\sigma^{2}$ of a scalar Gaussian distribution in exponential family form are given by:\n$\\mu = -\\frac{\\partial}{\\partial \\eta_{1}} \\ln h(y|\\eta)$ at $\\eta_{1} = 0$\n$\\sigma^{2} = -\\frac{\\partial^{2}}{\\partial \\eta_{2}^{2}} \\ln h(y|\\eta)$ at $\\eta_{2} = 0$\n\nFrom the given exponential family representation, we have $\\eta = (2, -1)$ and $h(y | \\eta) = \\exp \\{(2, -1)(y, y^{2})^{\\top} - 1 - \\frac{1}{2} \\ln(\\pi)\\}$.\n\nBy differentiating $h(y | \\eta)$ with respect to $\\eta_{1}$ and $\\eta_{2}$, we find:\n$\\frac{\\partial}{\\partial \\eta_{1}} \\ln h(y | \\eta) = y$ and $\\frac{\\partial^{2}}{\\partial \\eta_{2}^{2}} \\ln h(y | \\eta) = 2y^{2}$\n\nEvaluating these expressions at $\\eta_{1} = 0$ and $\\eta_{2} = 0$ gives us the mean and variance:\n$\\mu = -\\frac{\\partial}{\\partial \\eta_{1}} \\ln h(y | \\eta) \\bigg|_{\\eta_{1} = 0} = 0$\n$\\sigma^{2} = -\\frac{\\partial^{2}}{\\partial \\eta_{2}^{2}} \\ln h(y | \\eta) \\bigg|_{\\eta_{2} = 0} = 1$\n\nTherefore, the mean $\\mu$ is 0 and the variance $\\sigma^{2}$ is 1, which corresponds to option (h). ///Answer: H"}
{"subject": "", "question": "Question: The Davis-Meyer scheme is used to \\ldots?\n\nOptions:\nA. build a signature using a hash function.\nB. build a compression function using an encryption scheme.\nC. build a public-key cryptosystem using a block cipher.\nD. build a block cipher using a stream cipher.\n", "answer": "Explanations: Option 0: build a signature using a hash function.\n\nThe Davis-Meyer scheme is used to build a signature using a hash function. This scheme involves taking a message and hashing it using a cryptographic hash function. The output of the hash function is then combined with a secret key using a specific operation, such as XOR or bitwise addition. This combined value can then be used as a signature for the message, providing authentication and integrity protection. The Davis-Meyer scheme is a common technique used in digital signature algorithms. ///Answer: A"}
{"subject": "", "question": "Question: If rule {A,B} -> {C} has confidence c1 and rule {A} -> {C} has confidence c2, then ?\n\nOptions:\nA. c2 >= c1\nB. c1 > c2 and c2 > c1 are both possible\nC. c1 >= c2\n", "answer": "Explanations: To compare the confidences c1 and c2 for the given rules, we need to consider the relationship between the antecedents of the rules. If rule {A,B} -> {C} implies that whenever items A and B are present together, item C is also present, while rule {A} -> {C} implies that whenever item A is present, item C is also present.\n\nDepending on the specific dataset and support values, the confidences c1 and c2 can vary. In general, if rule {A,B} -> {C} has a higher confidence than rule {A} -> {C}, it suggests that the presence of item B further strengthens the association between A and C, leading to a higher confidence for the rule {A,B} -> {C} (c1 > c2). Therefore, the answer to the question is 'c1 > c2'.\n\nIf the confidences c1 and c2 are the same, it could indicate that item B does not significantly impact the association between items A and C, resulting in equal confidences for both rules ({A,B} -> {C} and {A} -> {C}).\n\nHowever, without specific values for c1 and c2 or additional information about the dataset and the support values, a definitive answer cannot be provided. ///Answer: B"}
{"subject": "", "question": "Question: Consider a regression model where data $(x,y)$ is generated by input $x$ uniformly randomly sampled from $[0,1]$ and $y(x) = x^2 + \\epsilon$,\n\t        where $\\epsilon$ is random noise with mean 0 and variance 1.\n            Two models are carried out for regression:\n            model A is a trained quadratic function $g(x; \\wv) = w_2 x^2 + w_1 x + w_0$ where $\\wv = (w_0, w_1, w_2)^\top\\in\\mathbb R^3$,\n            and model B is a constant function $h(x) = 1/2$.\n            Then compared to model B, model A has ?\n\nOptions:\nA. higher bias, higher variance.\nB. higher bias, lower variance.\nC. lower bias, higher variance.\nD. lower bias, lower variance.\n", "answer": "Explanations: In the context of machine learning models, bias and variance are two types of errors that can affect the performance of a model.\n\n- Bias: Bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n\n- Variance: Variance is the error introduced by using a model that is too complex for the underlying data. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting).\n\nGiven the options provided:\n\n1. Higher bias, higher variance: This option suggests that the model has both high bias and high variance. This would mean the model is both underfitting and overfitting, which is contradictory. Thus, this option is not correct.\n\n2. Higher bias, lower variance: This option suggests that the model has high bias and low variance. This would indicate that the model is underfitting but not overfitting. This is a common scenario in simple models or when there is not enough complexity to capture the underlying patterns in the data. This option is plausible.\n\n3. Lower bias, higher variance: This option suggests that the model has low bias and high variance. This would indicate that the model is not underfitting but is overfitting the data, capturing noise rather than signal. This option is plausible.\n\n4. Lower bias, lower variance: This option suggests that the model has both low bias and low variance. This would indicate that the model is neither underfitting nor overfitting, striking a good balance between bias and variance. This is often the goal in machine learning models. This option is also plausible.\n\nTherefore, the most likely correct answers are options 2 (higher bias, lower variance) and 3 (lower bias, higher variance), depending on the specific behavior of the model. The best choice would depend on the context and performance requirements of the model. ///Answer: C"}
{"subject": "", "question": "Question: Dan has been working with decision trees. His friend Eve recommends using random forests instead. What is most likely to happen?\n\nOptions:\nA. Accuracy will increase, interpretability will increase\nB. Accuracy will increase, interpretability will decrease\nC. Accuracy will decrease, interpretability will increase\nD. Accuracy will decrease, interpretability will decrease\n", "answer": "Explanations: The correct answer is: Accuracy will increase, interpretability will decrease\n\nExplanation: Random forests typically have higher accuracy compared to decision trees due to the ensemble learning technique they use. However, random forests are more complex and harder to interpret compared to decision trees. So, while accuracy is likely to increase, interpretability is likely to decrease when switching from decision trees to random forests. ///Answer: B"}
{"subject": "", "question": "Question: Let $K = (K_1, K_2,..., K_n)$, where each $K_i$ is independently chosen from $\\{0, 1\\}$ with uniform probability. Let $K'=(K'_1, K'_2, ..., K'_n)$ such that, for each $i$, $K'_i \\in \\{0, 1\\}$ and $K'_{i} = \\sum_{j = 1}^i K_j \\  \text{mod} \\ 2.$ True or false: Using $K$ as the key one can achieve perfect secrecy if the message is $n$ bits.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: True. \n\nUsing $K$ as the key, perfect secrecy can be achieved because $K'$ is essentially a one-time pad. Each $K'_i$ is calculated as the sum of all previous $K_j$ values modulo 2, ensuring that each $K'_i$ is independent of the other $K'_j$ values. This means that each bit of the message will be XORed with an independent and uniformly distributed random bit from $K$, resulting in perfect secrecy. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. Assume that $p$ is prime.\n\nOptions:\nA. $QR(p)$ is of order $\\frac{p-1}{4}$\nB. $\\mathbf{Z}_{p}^*$ has only one generator.\nC. $\\mathbf{Z}_{p}^*$ has $\\varphi(\\varphi(p))$ generators.\nD. All elements of $\\mathbf{Z}_{p}$ are invertible.\n", "answer": "Explanations: Reasoning:\n1. When analyzing properties related to prime numbers, especially in modular arithmetic, certain characteristics and theorems come into play.\n2. We will evaluate each option based on the properties of prime numbers and the multiplicative group of integers modulo $p$.\n 'All elements of $\\\\mathbf{Z}_{p}$ are invertible.' ///Answer: D"}
{"subject": "", "question": "Question: Which of the following is TRUE for Recommender Systems (RS)?\n\nOptions:\nA. The complexity of the Content-based RS depends on the number of users\nB. Item-based RS need not only the ratings but also the item features\nC. Matrix Factorization is typically robust to the cold-start problem.\nD. Matrix Factorization can predict a score for any user-item combination in the dataset.\n", "answer": "Explanations: The correct statements are:\n\n1. 'Item-based RS need not only the ratings but also the item features'\n2. 'Matrix Factorization can predict a score for any user-item combination in the dataset.'\n\nNow, let's explain why these statements are true:\n\n1. Item-based RS need not only the ratings but also the item features: In item-based RS, recommendations are made based on the similarity between items. In order to calculate this similarity, not only the ratings of items by users are considered, but also the features of the items. These features can include attributes like genre, category, keywords, etc. By considering item features in addition to ratings, the system can provide more accurate and relevant recommendations to users.\n\n2. Matrix Factorization can predict a score for any user-item combination in the dataset: Matrix Factorization is a popular technique used in collaborative filtering-based RS. It works by factorizing the user-item interaction matrix into two lower-dimensional matrices, which represent latent factors for users and items. By using these latent factors, Matrix Factorization can predict the missing values (unrated items) in the matrix, allowing it to predict a score for any user-item combination in the dataset. This enables the system to make personalized recommendations even for items that a user has not interacted with yet.\n\nI hope this explanation helps clarify why these statements are true for Recommender Systems. If you have any further questions or need more clarification, feel free to ask! ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textit{incorrect} assertion. Consider a device that is running a software implementation of the PKCS\\#1v1.5 RSA cryptosystem.\n\nOptions:\nA. Analysing the power consumption of the device during decryption may be used to help recover the secret key.\nB. Inducing computational errors in the device during encryption may help recover the secret key.\nC. Issues related to message formatting may be used to help recover the secret key.\nD. Measuring the timing of the decryption computation may be used to help recover the secret key.\n", "answer": "Explanations: a) Incorrect. Analysing the power consumption of the device during decryption is a form of side-channel attack known as a power analysis attack. This type of attack can potentially reveal information about the secret key by observing variations in power consumption, but it is not specific to RSA decryption. \n\nb) Incorrect. Inducing computational errors during encryption may not necessarily help recover the secret key. While errors in the encryption process could potentially reveal information about the key, it is not a guaranteed or direct method for recovering the key.\n\nc) Incorrect. Issues related to message formatting, such as padding schemes, can be vulnerabilities in RSA implementations, but they are not directly used to recover the secret key. These issues may lead to attacks such as padding oracle attacks, but they do not directly recover the secret key.\n\nd) Correct. Measuring the timing of the decryption computation can be used in a timing attack to help recover the secret key. By measuring the time taken for decryption operations, an attacker can potentially infer information about the key and decrypt messages. Timing attacks are a common form of side-channel attack in RSA implementations. ///Answer: D"}
{"subject": "", "question": "Question: Let $X$ be a random variable distributed over the alphabet $\\mathcal{X}=\\{0,1,\\ldots,n\\}$. Assume also that there exist $x_1,x_2\\in\\mathcal{X}$ such that $p_X(x_1)\neq p_X(x_2)$ (\textit{i.e.}, $X$ is not uniformly distributed over $\\mathcal{X}$). Let $Y=2^X$ and $Z=\\lfloor X/2 \rfloor$.  True or false: $H(Y)=H(X)$?\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False\n\nExplanation:\nTo determine if $H(Y)=H(X)$, we need to calculate the entropy of $Y$ and compare it to the entropy of $X$.\n\nEntropy is defined as $H(X) = -\\sum_{x\\in\\mathcal{X}} p_X(x) \\log p_X(x)$.\n\nFor $Y=2^X$, the probability mass function of $Y$ can be calculated as $p_Y(y) = P(Y=y) = P(2^X=y) = P(X=\\log_2 y)$.\n\nSince $Y=2^X$, $X=\\log_2 Y$, so $p_Y(y) = p_X(\\log_2 y)$.\n\nTherefore, $H(Y) = -\\sum_{y\\in\\mathcal{Y}} p_Y(y) \\log p_Y(y) = -\\sum_{y\\in\\mathcal{Y}} p_X(\\log_2 y) \\log p_X(\\log_2 y)$.\n\nFor $Z=\\lfloor X/2 \\rfloor$, the entropy of $Z$ can be calculated in a similar manner.\n\nSince the transformation from $X$ to $Y$ and $Z$ involves non-linear functions ($2^X$ and $\\lfloor X/2 \\rfloor$), the entropies of $Y$ and $Z$ will not be equal to the entropy of $X$. Therefore, the statement that $H(Y)=H(X)$ is false. ///Answer: B"}
{"subject": "", "question": "Question: Let $0\\leq\\alpha\\leq1$ be an unknown constant. Let $X$ be a random variable taking values in $\\mathcal{X}=\\{0,1,2\\}$ with probability $p_X(0) = p_X(1) = \\alpha$ and $p_X(2) = 1-2\\alpha$. Let $Y$ be a random variable defined as follows \begin{equation*}Y = \begin{cases}1, &\text{if }X=2\\0, &\text{if } X\neq 2\\end{cases}.\\end{equation*} You also know that $H(X|Y) = \frac{1}{2}$. Choose the correct value of $\\alpha$.\n\nOptions:\nA. $\frac{1}{4}$\nB. $\frac{1}{2}$\nC. $1$\nD. $\frac{1}{8}$\n", "answer": "Explanations: Given the random variables $X$ and $Y$ as described, with $X$ taking values in $\\mathcal{X}=\\{0,1,2\\}$ and $Y$ defined based on $X$ as provided, and considering that $H(X|Y) = \\frac{1}{2}$, we need to determine the correct value of $\\alpha$.\n\nThe conditional entropy $H(X|Y)$ is defined as:\n\n\\begin{equation}\nH(X|Y) = \\sum_{y\\in\\mathcal{Y}} p_Y(y) H(X|Y=y)\n\\end{equation}\n\nGiven the definition of $Y$, $Y$ can only take values $0$ and $1$, so we have:\n\n\\begin{equation}\nH(X|Y=0) = H(X|Y=1) = H(X|Y=0) = H(X|Y=1) = 0\n\\end{equation}\n\nSince $Y=1$ when $X\\neq 2$, $Y=0$ when $X=2$, and $\\alpha$ is the probability of $X=0$ or $X=1$, we have:\n\n\\begin{equation}\np_Y(1) = \\alpha\n\\end{equation}\n\n\\begin{equation}\np_Y(0) = 1 - p_Y(1) = 1 - \\alpha\n\\end{equation}\n\nNow, we can compute $H(X|Y)$:\n\n\\begin{equation}\nH(X|Y) = p_Y(0) \\cdot H(X|Y=0) + p_Y(1) \\cdot H(X|Y=1) = 0 + \\alpha \\cdot 0 = 0\n\\end{equation}\n\nHowever, it is given that $H(X|Y) = \\frac{1}{2}$. Since $H(X|Y)$ should be non-negative, the correct value of $\\alpha$ would be such that the resulting conditional entropy is non-negative. \n\nThe correct value of $\\alpha$ that ensures $H(X|Y) = \\frac{1}{2}$ is:\n\n\\begin{equation}\n\\alpha = \\frac{1}{2}\n\\end{equation}\n\nTherefore, the correct value of $\\alpha$ is $\\frac{1}{2}$, making the correct option \"$\\frac{1}{2}$\". ///Answer: B"}
{"subject": "", "question": "Question: If A is an uncountable set and B is an uncountable set, A \u2212 B cannot be :?\n\nOptions:\nA. countably infinite\nB. uncountable\nC. the null set\nD. none of the other options\n", "answer": "Explanations: When dealing with sets, it's important to consider the properties of uncountable sets. An uncountable set is a set that is too large to be put into one-to-one correspondence with the natural numbers. \n\nOption A: Finite\nThis option is incorrect because the difference between two uncountable sets can still result in an uncountable set. Even if one of the sets is finite, the subtraction of two uncountable sets can yield an uncountable set. Therefore, A - B can indeed be uncountable even if one of the sets is finite.\n\nOption B: Countable\nThis option is incorrect because the difference between two uncountable sets can still be uncountable. The key characteristic of uncountable sets is that they cannot be put into one-to-one correspondence with the natural numbers. Therefore, the difference between two uncountable sets can also be uncountable.\n\nOption C: Infinite\nThis option is correct. The difference between two uncountable sets can indeed be infinite. In fact, it is possible for the difference between two uncountable sets to be infinitely large, as uncountable sets have an infinite number of elements.\n\nOption D: Countably Infinite\nThis option is incorrect because the difference between two uncountable sets cannot be countably infinite. Countably infinite sets have a cardinality that can be put into one-to-one correspondence with the natural numbers, which is smaller than the cardinality of uncountable sets. Therefore, the difference between two uncountable sets will also be uncountable. ///Answer: C"}
{"subject": "", "question": "Question: Consider an array $A[1,\\ldots, n]$ consisting of the $n$ distinct numbers $1,2, \\ldots, n$. We are further guaranteed that $A$ is almost sorted in the following sense: $A[i] \\neq i$ for at most $\\sqrt{n}$ values of $i$.  What are tight asymptotic worst-case running times for Insertion Sort and Merge Sort on such instances?\n\nOptions:\nA. It is $\\Theta(n + \\sqrt{n}\\log n)$ for Merge Sort and $\\Theta(n)$ for Insertion Sort.\nB. It is $\\Theta(n \\log n)$ for Merge Sort and $\\Theta(n^2)$ for Insertion Sort.\nC. It is $\\Theta(n + \\sqrt{n}\\log n)$ for Merge Sort and $\\Theta(n^{3/2})$ for Insertion Sort.\nD. It is $\\Theta(n + \\sqrt{n}\\log n)$ for Merge Sort and $\\Theta(n^2)$ for Insertion Sort.\nE. It is $\\Theta(n \\log n)$ for Merge Sort and $\\Theta(n)$ for Insertion Sort.\nF. It is $\\Theta(n \\log n)$  for Merge Sort and $\\Theta(n^{3/2})$ for Insertion Sort.\n", "answer": "Explanations: The correct option is 4. It is $\\Theta(n + \\sqrt{n}\\log n)$ for Merge Sort and $\\Theta(n^2)$ for Insertion Sort.\n\nFor Merge Sort, the worst-case running time is $\\Theta(n\\log n)$ regardless of the input distribution, so in this case, it remains $\\Theta(n\\log n)$.\n\nFor Insertion Sort, the worst-case running time is $\\Theta(n^2)$. In this almost sorted array scenario, at most $\\sqrt{n}$ elements are out of place, so the worst-case running time is still $\\Theta(n^2)$ as it would need to potentially swap each of these elements to their correct positions. ///Answer: D"}
{"subject": "", "question": "Question: We are given a data set $S=\\left\\{\\left(\\boldsymbol{x}_{n}, y_{n}\\right)\\right\\}$ for a binary classification task where $\\boldsymbol{x}_{n}$ in $\\mathbb{R}^{D}$. We want to use a nearestneighbor classifier. In which of the following situations do we have a reasonable chance of success with this approach? [Ignore the issue of complexity.]?\n\nOptions:\nA. $n \\rightarrow \\infty, D$ is fixed\nB. $ n \\rightarrow \\infty, D \\ll \\ln (n)$\nC. $ n=D^2, D \\rightarrow \\infty$\nD. $ n$ is fixed, $D \\rightarrow \\infty$\n", "answer": "Explanations: Answer: \n\nThe option where we have a reasonable chance of success with the nearest neighbor classifier approach is: '$n \\rightarrow \\infty, D \\ll \\ln(n)$'\n\nExplanation:\n\nIn a binary classification task, the nearest-neighbor classifier relies on the concept of proximity between data points in the feature space. To determine which class a new data point belongs to, the classifier looks at the class labels of the data points closest to it.\n\nLet's break down each of the given options to understand why '$n \\rightarrow \\infty, D \\ll \\ln(n)$' is the most suitable scenario for success:\n\n1. $n \\rightarrow \\infty, D$ is fixed:\n   - In this scenario, we have a large number of data points $n$ but a fixed number of features $D$. With a fixed number of features, the nearest neighbor approach might not perform well as it relies heavily on the density of data points in the feature space. In high-dimensional spaces, the data becomes sparse, making it difficult to find close neighbors.\n\n2. $n \\rightarrow \\infty, D \\ll \\ln(n)$:\n   - This scenario is more favorable for the nearest neighbor classifier. When the number of data points $n$ increases towards infinity and the number of dimensions $D$ is much smaller than the natural logarithm of $n$, the data becomes more densely populated in the feature space. This density facilitates the identification of nearest neighbors, leading to better classification results.\n\n3. $n = D^2, D \\rightarrow \\infty$:\n   - In this case, as the number of features $D$ grows to infinity with a fixed number of data points, the curse of dimensionality becomes a significant issue. The nearest neighbor classifier struggles in high-dimensional spaces due to the increased distance between points, making it harder to find true nearest neighbors.\n\n4. $n$ is fixed, $D \\rightarrow \\infty$:\n   - When the number of data points $n$ is fixed and the number of features $D$ increases significantly, the nearest neighbor approach becomes less effective. As the feature space expands, the notion of proximity becomes less meaningful, and distinguishing between classes based on nearest neighbors becomes challenging.\n\nTherefore, the scenario where $n \\rightarrow \\infty$ and $D \\ll \\ln(n)$ provides a reasonable chance of success for the nearest neighbor classifier due to the dense distribution of data points in a lower-dimensional space, facilitating accurate classification based on proximity. ///Answer: B"}
{"subject": "", "question": "Question: Suppose we run JOS and set a breakpoint at syscall (in lib/syscall.c). What are the Current Privilege Level (CPL) before invoking the syscall function and after executing the int 0x30 instruction?\n\nOptions:\nA. 0 3\nB. 0 0\nC. 3 0\nD. 3 3\n", "answer": "Explanations: Before invoking the syscall function, the Current Privilege Level (CPL) is typically set to 0, as the syscall instruction is typically used by user-level programs to request services from the kernel, which runs in the most privileged mode (ring 0) on x86 systems. \n\nWhen the int 0x30 instruction is executed, it triggers a software interrupt, causing the processor to switch to the kernel mode (ring 0) to handle the system call. In this case, the CPL changes to 0 after executing the int 0x30 instruction.\n\nTherefore, the correct answer is:\nOption 2: 0 0 ///Answer: B"}
{"subject": "", "question": "Question: What is the support of the itemset {beer, chocolate} and the confidence of the rule\u00a0{chocolate} \u2192 {beer} in the dataset below?TID: Items BoughtT1: milk, butter, beerT2: milk, diaper, beer, eggsT3: butter, diaper, beer, chocolate\nT4: milk, butter, diaper, beerT5: milk, butter, diaper, chocolate\n\n\n\n?\n\nOptions:\nA. 0.2/0.25\nB. 0.4/0.5\nC. 0.2/0.5\nD. 0.4/0.25\n\n\n\n\n", "answer": "Explanations: To calculate the support of the itemset {beer, chocolate}, we need to count the number of transactions that contain both beer and chocolate, and then divide by the total number of transactions.\n\nTransactions containing {beer, chocolate}: T3, T5\n\nSupport({beer, chocolate}) = (Number of transactions containing {beer, chocolate}) / (Total number of transactions)\nSupport({beer, chocolate}) = 2 / 5 = 0.4\n\nTo calculate the confidence of the rule {chocolate} \u2192 {beer}, we need to find the number of transactions containing chocolate and beer and then divide by the number of transactions containing chocolate.\n\nTransactions containing {chocolate}: T3, T5\nTransactions containing {chocolate, beer}: T3\n\nConfidence({chocolate} \u2192 {beer}) = (Number of transactions containing {chocolate, beer}) / (Number of transactions containing {chocolate})\nConfidence({chocolate} \u2192 {beer}) = 1 / 2 = 0.5\n\nTherefore, the correct option is:\nSupport of {beer, chocolate}: 0.4\nConfidence of the rule {chocolate} \u2192 {beer}: 0.5\n\nSo, the correct answer is Option 1: 0.4/0.5 ///Answer: B"}
{"subject": "", "question": "Question: Select the \\textbf{most accurate} answer. For a hash function to be secure (\\textbf{but still efficient}) against collision attacks in 2015, the output length should be\\dots?\n\nOptions:\nA. around 80 bits.\nB. around 160 bits.\nC. around 512 bits.\nD. around 1024 bits.\n", "answer": "Explanations: To determine the correct output length for a hash function to be secure against collision attacks in 2015, we need to consider the concept of collision resistance and the security requirements at that time.\n\n1. Around 80 bits: This output length is considered too short to be secure against collision attacks in 2015. A hash function with an output length of 80 bits would not provide enough security against modern computing power and cryptographic attacks.\n\n2. Around 160 bits: This option is the correct answer. In 2015, it was generally recommended to use a hash function with an output length of around 160 bits to achieve a good balance between security and efficiency. A 160-bit hash output provides a high level of collision resistance and is considered secure for most applications at that time.\n\n3. Around 512 bits: A hash function with an output length of 512 bits would be excessive in terms of security for most practical purposes in 2015. While it would offer a very high level of security, it would also be less efficient in terms of computational resources and storage requirements.\n\n4. Around 1024 bits: Similarly to the previous option, a hash function with an output length of 1024 bits would be overkill for most applications in 2015. It would provide an extremely high level of security but at the cost of efficiency and performance.\n\nTherefore, the most accurate answer is around 160 bits, as it provides a good balance between security and efficiency against collision attacks in 2015. ///Answer: B"}
{"subject": "", "question": "Question: Using a block cipher, we can build \\ldots?\n\nOptions:\nA. only hash functions.\nB. only MACs.\nC. only hash functions and MACs.\nD. hash functions, MACs, and stream ciphers.\n", "answer": "Explanations: In the context of building a secure communication system using a block cipher, it is important to understand the roles of each cryptographic primitive. Let's discuss each option and its rationale:\n\n1. **Only Hash Functions**: Hash functions are primarily used for data integrity verification. They are not designed for encryption or authentication purposes. While they are essential for ensuring data integrity, using only hash functions would not provide confidentiality or authentication in a communication system. Therefore, using only hash functions would not be sufficient for building a secure communication system with a block cipher.\n\n2. **Only MACs (Message Authentication Codes)**: MACs are used for ensuring message authentication and integrity. While MACs can provide authentication and integrity protection, they do not provide confidentiality. Therefore, using only MACs would not be sufficient for achieving confidentiality in a communication system that requires encryption, which is typically provided by block ciphers.\n\n3. **Only Hash Functions and MACs**: Combining hash functions and MACs can provide a more comprehensive security solution. Hash functions can be used for data integrity checks, while MACs can provide message authentication. However, this combination still lacks confidentiality provided by encryption mechanisms like block ciphers. Therefore, although this combination enhances security in terms of integrity and authentication, it is not sufficient for ensuring confidentiality.\n\n4. **Hash Functions, MACs, and Stream Ciphers**: This option provides a more complete solution for building a secure communication system. Hash functions can be used for data integrity verification, MACs for message authentication, and stream ciphers for confidentiality (encryption). By combining these three cryptographic primitives, you can achieve a system that ensures data integrity, authenticity, and confidentiality.\n\nTherefore, the most suitable choice for building a secure communication system using a block cipher would be to incorporate hash functions, MACs, and stream ciphers. This combination offers a balanced approach to address the key security requirements of data integrity, authentication, and confidentiality. ///Answer: D"}
{"subject": "", "question": "Question: Let $N=3\\cdot5\\cdot13$. Then a quadratic residue in $\\mathbb{Z}_N^*$ has?\n\nOptions:\nA. $2$ square roots.\nB. $3$ square roots.\nC. $6$ square roots.\nD. $8$ square roots.\n", "answer": "Explanations: Since $N=3\\cdot5\\cdot13$, we have $\\phi(N) = (3-1)(5-1)(13-1) = 2\\cdot4\\cdot12 = 96$. \n\nBy the Chinese Remainder Theorem, $\\mathbb{Z}_N^* \\cong \\mathbb{Z}_3^* \\times \\mathbb{Z}_5^* \\times \\mathbb{Z}_{13}^*$, and we have $\\mathbb{Z}_3^* \\cong \\mathbb{Z}_2$, $\\mathbb{Z}_5^* \\cong \\mathbb{Z}_4$, and $\\mathbb{Z}_{13}^* \\cong \\mathbb{Z}_{12}$. \n\nTherefore, $\\mathbb{Z}_N^* \\cong \\mathbb{Z}_2 \\times \\mathbb{Z}_4 \\times \\mathbb{Z}_{12} \\cong \\mathbb{Z}_{96}$. \n\nThus, the number of quadratic residues in $\\mathbb{Z}_N^*$ is $\\frac{96}{2} = 48$. \n\nHence, the correct answer is $\\boxed{48}$ square roots. ///Answer: C:"}
{"subject": "", "question": "Question: Which one of these is \\emph{not} a MAC construction?\n\nOptions:\nA. HMAC\nB. OMAC\nC. MD5\nD. ISO/IEC 9797\n", "answer": "Explanations: A MAC construction (Message Authentication Code construction) is a method for generating a MAC (Message Authentication Code) from a message and a secret key. The MAC is used to verify the integrity and authenticity of the message. \n\nA: HMAC (Hash-based Message Authentication Code) is a widely-used MAC construction that combines a cryptographic hash function with a secret key.\nB: OMAC (One-Key CBC-MAC) is a MAC construction based on the Cipher Block Chaining (CBC) mode of operation.\nC: MD5 (Message Digest Algorithm 5) is a cryptographic hash function, not a MAC construction.\nD: ISO/IEC 9797 is a standard that specifies various methods for generating MACs.\n\nTherefore, the correct answer is: \nC: MD5 ///Answer: C"}
{"subject": "", "question": "Question: What is a correct pruning strategy for decision tree induction?\n\nOptions:\nA. Apply Maximum Description Length principle\nB. Stop partitioning a node when either positive or negative samples dominate the samples of the other class\nC. Choose the model that maximizes L(M) + L(M|D)\nD. Remove attributes with lowest information gain\n", "answer": "Explanations: Let's reassess the provided options based on the explanation provided:\n\n1. **Apply Maximum Description Length principle**:\n   - The Maximum Description Length (MDL) principle is a valid pruning strategy for decision trees.\n   - It aims to balance the model complexity with the fit to the data to prevent overfitting.\n   - By selecting the simplest model that best explains the data, the MDL principle helps in post-pruning to achieve better generalization.\n\n2. **Stop partitioning a node when either positive or negative samples dominate the samples of the other class**:\n   - This strategy can lead to premature stopping and potentially suboptimal tree structures.\n   - It may not always result in the best pruning decisions as it is a simplistic rule that does not consider the overall complexity of the tree.\n\n3. **Choose the model that maximizes L(M) + L(M|D)**:\n   - While maximizing the likelihood of the model and the likelihood given the data is a valid model selection criterion, it is not a direct pruning strategy for decision trees.\n   - It focuses more on model selection rather than simplifying the tree structure.\n\n4. **Remove attributes with lowest information gain**:\n   - Pruning by removing attributes with the lowest information gain is a common strategy to simplify decision trees and reduce overfitting.\n   - Features with low information gain contribute less to the decision-making process, so removing them can help improve the model's generalization performance.\n\nBased on the explanation provided, the correct pruning strategy for decision tree induction is indeed: **Apply Maximum Description Length principle**. This principle helps simplify the tree post-construction by balancing model complexity and accuracy, thereby reducing overfitting while maintaining predictive power. Thank you for pointing out this clarification. ///Answer: A"}
{"subject": "", "question": "Question: Consider a 3-gram language model.\u00a0Select all possible ways we can compute the maximum likelihood of the word sequence:\"time flies like an arrow\"You will get a penalty for wrong ticks.\n\nOptions:\nA. P(time flies like).P(an|flies like).P(arrow|like an)\nB. P(time flies like).P(like|time flies).P(an|flies like).P(arrow|like an)\nC. P(time).P(flies like|time).P(an|flies like).P(arrow|like an)\nD. P(time flies).P(like|flies).P(an|like).P(arrow|an)\nE. P(time flies).P(flies|time).P(like|flies).P(an|like).P(arrow|an)\nF. P(time flies).P(like|time flies).P(an|flies like).P(arrow|like an)\n", "answer": "Explanations: To compute the maximum likelihood of a word sequence in a 3-gram language model, we need to calculate the product of the conditional probabilities of each word given the previous two words.\n\nLet's break down each possible answer:\n\n1. P(time flies like).P(an|flies like).P(arrow|like an)\n   - This answer is incorrect because it does not consider the correct conditional probabilities for each word in the sequence.\n\n2. P(time flies like).P(like|time flies).P(an|flies like).P(arrow|like an)\n   - This answer is correct. It considers the correct conditional probabilities for each word in the sequence.\n\n3. P(time).P(flies like|time).P(an|flies like).P(arrow|like an)\n   - This answer is incorrect because it does not follow the 3-gram model by considering the probabilities of each word given the previous two words.\n\n4. P(time flies).P(like|flies).P(an|like).P(arrow|an)\n   - This answer is incorrect because it does not consider the correct conditional probabilities for each word in the sequence.\n\n5. P(time flies).P(flies|time).P(like|flies).P(an|like).P(arrow|an)\n   - This answer is incorrect because it does not follow the 3-gram model by considering the probabilities of each word given the previous two words.\n\n6. P(time flies).P(like|time flies).P(an|flies like).P(arrow|like an)\n   - This answer is incorrect because it misses the last word \"arrow\" in the sequence when calculating the maximum likelihood.\n\nTherefore, the correct answer is:\n2. P(time flies like).P(like|time flies).P(an|flies like).P(arrow|like an) ///Answer: B"}
{"subject": "", "question": "Question: Whenever I want to use Z-Score standardization (also known as normalization), I should use the mean and standard deviation of the training set to normalize my training, validation, and test set.\n\nOptions:\nA. TRUE\nB. FALSE\n", "answer": "Explanations: The correct statement is:\n\n1. TRUE\n\nWhen using Z-Score standardization (normalization), it is important to calculate the mean and standard deviation of the training set and use these values to normalize not only the training set but also the validation and test sets. This ensures consistency in the normalization process and helps prevent data leakage or bias in the model evaluation. ///Answer: A"}
{"subject": "", "question": "Question: A generator matrix $G$ of binary $(6,3)$ linear code maps the information vectors $m_1 = (1,0,1)$ and $m_2=(1,1,1)$ into the codewords $c_1 = (1,1,0,0,0,1)$ and $c_2=(1,0,0,0,1,0)$ respectively. Which of the following is true?\n\nOptions:\nA. The second row of $G$ is $(0,1,0,0,1,1)$.\nB. $G$ is in systematic form.\nC. $d_{\\min} = 3$.\n", "answer": "Explanations: Sure, let's break down the terms in the question:\n\n1. **Generator matrix ($G$)**: In Coding Theory, a generator matrix is a matrix that defines a linear code. It maps the information bits (message) to the codewords in a systematic way.\n\n2. **Binary $(6,3)$ linear code**: This refers to a binary linear code of length 6 and dimension 3. The length is the number of bits in a codeword, and the dimension is the number of information bits in a message.\n\n3. **Information vectors ($m_1$, $m_2$)**: These are the input vectors representing the information bits that are encoded into codewords.\n\n4. **Codewords ($c_1$, $c_2$)**: These are the output vectors obtained after encoding the information vectors using the generator matrix.\n\n5. **Options**:\n    - **Option 1**: Checking if the second row of the generator matrix is $(0,1,0,0,1,1)$.\n    - **Option 2**: Checking if the generator matrix is in systematic form, which means the matrix has an identity submatrix.\n    - **Option 3**: Checking if the minimum Hamming distance ($d_{\\min}$) of the code is 3, which is the smallest number of bit positions at which any two codewords differ.\n\nTo determine which of the options is true, we need to analyze the given information about the generator matrix and the codewords derived from it. We should check if the properties described in each option align with the information provided in the question.\n\nLet's start by constructing the generator matrix $G$ based on the given mappings of information vectors to codewords:\n\nGiven that:\n- $m_1 = (1,0,1)$ maps to $c_1 = (1,1,0,0,0,1)$\n- $m_2 = (1,1,1)$ maps to $c_2 = (1,0,0,0,1,0)$\n\nWe can construct a $3 \\times 6$ generator matrix $G$ as follows:\n$$ G = \\begin{bmatrix} 1 & 0 & 1 & | & 1 & 1 & 0 & 0 & 0 & 1 \\\\ 1 & 1 & 1 & | & 1 & 0 & 0 & 0 & 1 & 0 \\\\ \\end{bmatrix} $$\n\nNow, let's evaluate each option:\n1. **The second row of $G$ is $(0,1,0,0,1,1)$**: We can verify if this row matches the second row of the constructed $G$ matrix.\n2. **$G$ is in systematic form**: Check if $G$ has an identity submatrix.\n3. **$d_{\\min} = 3$**: Calculate the minimum Hamming distance between all possible pairs of codewords.\n\nAfter verifying each option, we can determine which of them is true based on the properties of the generator matrix and the given codewords. ///Answer: C"}
{"subject": "", "question": "Question: We apply a Gaussian Mixture Model made of $K$ isotropic Gaussians (invariant to rotation around its center) to $N$ vectors of dimension $D$. What is the number of \\emph{free} parameters of this model?\n        ?\n\nOptions:\nA. $KD + 2K - 1 - 1$\nB. $KD + 2K - 1 + N - 1$\nC. $KD + KD^2 - 1$\nD. $2KD - 1$\nE. $2KD + N - 1$\nF. $NKD + NKD^2$\nG. $NKD + NKD^2 + N$\nH. $2NKD$\nI. $2NKD + N$\nJ. $KD + K - 1$\nK. $KD + K + N$\nL. $NKD$\nM. $NKD + N$\n", "answer": "Explanations: The number of free parameters for a Gaussian Mixture Model made of $K$ isotropic Gaussians in $D$ dimensions can be calculated as follows:\n\nEach Gaussian component has:\n- $D$ means (1 for each dimension),\n- 1 variance (isotropic, so all dimensions share the same variance),\n- and 1 mixing coefficient.\n\nTherefore, each Gaussian component contributes $D + 1 + 1 = D + 2$ parameters.\n\nSince we have $K$ Gaussian components in the mixture, the total number of parameters for the Gaussian components is $K \\times (D + 2) = KD + 2K$.\n\nAdditionally, we have $N$ data points, and for each data point, we need to estimate the probability of it belonging to each of the $K$ Gaussian components. This results in $N \\times K$ additional parameters.\n\nTherefore, the total number of free parameters for the Gaussian Mixture Model is $KD + 2K + NK = NKD + 2K + N$. \n\nSo, the correct option is $NKD + 2K + N$ ///Answer: E:"}
{"subject": "", "question": "Question: Given the following teleporting matrix (\u0395) for nodes A, B and C:\n[0 1\u20442 0]\n[0 0 0]\n[0 1\u20442 1]\nand making no assumptions about the link matrix (R), which of the following is correct:?\n\nOptions:\nA. A random walker can never reach node A\nB. A random walker can never leave node A\nC. A random walker can always leave node C\nD. A random walker can always leave node B\n", "answer": "Explanations: To determine the correct statement based on the teleporting matrix E provided, we need to analyze the probabilities of a random walker transitioning between nodes A, B, and C.\n\n1. \"A random walker can never reach node A\":\nLooking at the teleporting matrix E, we see that there is a direct transition from node B to node A with a probability of 1/2. Therefore, a random walker starting at node B can reach node A with a certain probability. Hence, this statement is not correct.\n\n2. \"A random walker can never leave node A\":\nSince there are no outgoing transitions from node A in the teleporting matrix E, a random walker starting at node A will indeed never leave node A. Therefore, this statement is correct.\n\n3. \"A random walker can always leave node C\":\nFrom node C in the teleporting matrix E, there is a 50% probability of transitioning to node B and a 50% probability of staying at node C. Therefore, a random walker starting at node C can indeed leave node C with a certain probability. Hence, this statement is not correct.\n\n4. \"A random walker can always leave node B\":\nIn the teleporting matrix E, there are no outgoing transitions from node B, as it is a dead-end node. This means that a random walker starting at node B cannot leave node B. Therefore, this statement is not correct.\n\nTherefore, the correct statement based on the provided teleporting matrix E is: \"A random walker can never leave node A.\" ///Answer: A:"}
{"subject": "", "question": "Question: What is the order of $2^{124}$ in $(\\mathbb{Z}_{2^{128}},+)$?\n\nOptions:\nA. 8.\nB. $\\varphi(2^{128})$.\nC. 124.\nD. 16.\n", "answer": "Explanations: The order of an element $g$ in a group is the smallest positive integer $n$ such that $g^n = e$, where $e$ is the identity element of the group.\\n\\nIn this case, we are looking for the order of $2^{124}$ in the group $(\\\\mathbb{Z}_{2^{128}},+)$. Since the group operation is addition modulo $2^{128}$, we are essentially looking for the smallest positive integer $n$ such that $n \\\\cdot 2^{124} \\\\equiv 0 \\\\pmod{2^{128}}$.\\n\\nIt is easy to see that the order of $2^{124}$ in this group is $2^{128} / \\\\text{gcd}(2^{124},2^{128}) = 2^{128} / 2^{124} = 2^4 = 16$.\\n\\nTherefore, the correct answer is d) 16. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion. Using double encryption leads to\\dots?\n\nOptions:\nA. doubling the complexity of the Encryption algorithm.\nB. doubling the complexity of the Decryption algorithm.\nC. doubling the complexity of all the attacks against the cipher.\nD. doubling the length of the key.\n", "answer": "Explanations: The false assertion is: 3. doubling the complexity of all the attacks against the cipher.\n\nExplanation:\n1. doubling the complexity of the Encryption algorithm: This statement is true. Using double encryption involves applying two layers of encryption, which can indeed increase the complexity of the encryption algorithm as it needs to perform the encryption process twice.\n2. doubling the complexity of the Decryption algorithm: This statement is true. Similarly, the decryption algorithm would need to be able to reverse the process of the double encryption, which can increase its complexity compared to a single encryption.\n3. doubling the complexity of all the attacks against the cipher: This statement is false. While double encryption can make it harder for attackers to decrypt the data, it does not necessarily double the complexity of all attacks against the cipher. Some attacks may still be effective even with double encryption in place.\n4. doubling the length of the key: This statement is true. Double encryption usually involves using two different keys, thus doubling the length of the key used in the encryption process.\n\nTherefore, the false assertion is statement 3. ///Answer: C"}
{"subject": "", "question": "Question: Is it true that $K$-means can be equivalently written as the following matrix factorization problem? Here $\\boldsymbol{X}$ denotes the $N \\times D$ data matrix. The $\\boldsymbol{\\mu}_{k}$ denote columns of $\\boldsymbol{M}$, rows of $\\boldsymbol{Z}$, and $L(\\boldsymbol{z}, \\boldsymbol{\\mu})=\\left\\|\\boldsymbol{X}^{\\top}-\\boldsymbol{M} \\boldsymbol{Z}^{\\top}\\right\\|_{\\text {Frob }}^{2}$. $$ \\begin{aligned} & \\min _{\\boldsymbol{z}, \\boldsymbol{\\mu}} L(\\boldsymbol{z}, \\boldsymbol{\\mu}) \\\\ & \\text { s.t. } \\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{D}, \\\\ & z_{n k} \\in\\{0,1\\}, \\sum_{k=1}^{K} z_{n k}=1 . \\end{aligned} $$?\n\nOptions:\nA. yes\nB. no\n", "answer": "Explanations: Yes, it is true that $K$-means can be equivalently written as the matrix factorization problem mentioned. The formulation given is essentially the same as the $K$-means clustering objective, where we are trying to minimize the Frobenius norm of the difference between the data matrix $\\boldsymbol{X}$ and the matrix reconstructed using the cluster centers $\\boldsymbol{M}$ and the assignment matrix $\\boldsymbol{Z}$.\n\nGiven this understanding, the correct option is:\n\n**True** ///Answer: A"}
{"subject": "", "question": "Question: Consider the loss function $L: \\R^d \to \\R$, $L(\\wv) = \frac{\beta}{2}\\|\\wv\\|^2$, where $\beta > 0$ is a constant. We run gradient descent on $L$ with a stepsize $\\gamma > 0$ starting from some $\\wv_0 \neq 0$. Which of the statements below is true? ?\n\nOptions:\nA. Gradient descent converges to the global minimum for any stepsize $\\gamma > 0$.\nB. Gradient descent with stepsize $\\gamma = \frac{2}{\beta}$ produces iterates that diverge to infinity ($\\|\\wv_t\\| \to \\infty$ as $t\to \\infty$).\nC. Gradient descent converges in two steps for $\\gamma = \frac{1}{\beta}$ (i.e., $\\wv_2$ is the \textbf{first} iterate attaining the global minimum of $L$).\nD. Gradient descent converges to the global minimum for any stepsize in the interval $\\gamma \\in \big( 0, \frac{2}{\beta}\big)$.\n", "answer": "Explanations: The correct statement is: Gradient descent converges to the global minimum for any stepsize in the interval $\\gamma \\in \\left(0, \\frac{2}{\\eta}\\right)$.\n\nExplanation: \nIn gradient descent, the update rule for the weight vector is given by:\n\\[\\wv_{t+1} = \\wv_t - \\gamma \\nabla L(\\wv_t)\\]\n\nFor the given loss function $L(\\wv) = \\frac{\\eta}{2}\\|\\wv\\|^2$, the gradient is $\\nabla L(\\wv) = \\eta \\wv$. \n\nSubstituting the gradient in the update rule, we get:\n\\[\\wv_{t+1} = \\wv_t - \\gamma \\eta \\wv_t = (1 - \\gamma \\eta) \\wv_t\\]\n\nFor convergence, we require that $|1 - \\gamma \\eta| < 1$, which gives us $0 < \\gamma < \\frac{2}{\\eta}$.\n\nTherefore, gradient descent converges to the global minimum for any stepsize in the interval $\\gamma \\in \\left(0, \\frac{2}{\\eta}\\right)$. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. The advantage of a distinguisher... \\ldots?\n\nOptions:\nA. \\ldots never depends on the number of samples tested.\nB. \\ldots can be expressed using the statistical distance between two functions.\nC. \\ldots can be expressed using type I and type II errors.\nD. \\ldots can be expressed in function of pre-assigned costs per error type.\n", "answer": "Explanations: The \\textbf{false} assertion is \\textbf{1) ... never depends on the number of samples tested.}\n\nIn reality, the advantage of a distinguisher can depend on the number of samples tested. This is because having more samples can potentially increase the confidence in the distinguisher's output, leading to a more reliable assessment of the system being tested. ///Answer: A"}
{"subject": "", "question": "Question: Given two distributions $P_0$ and $P_1$ over a discrete set $Z$, the maximal advantage of a distinguisher using a single sample is\\dots?\n\nOptions:\nA. $\\frac{1}{2}\\sum_{x\\in Z} | P_0(x) - P_1(x)|$.\nB. $1 - \\prod_{x\\in Z}(P_0(x)-P_1(x))^2$.\nC. $1$ if $P_0 = P_1$ and $0$ otherwise.\nD. always $1$ for computationally unbounded adversaries.\n", "answer": "Explanations: To determine the maximal advantage of a distinguisher using a single sample between two distributions $P_0$ and $P_1$ over a discrete set $Z, we need to consider how well the distinguisher can differentiate between the two distributions.\n\nOption A calculates the total variation distance between $P_0$ and $P_1$, which is a measure of how different the two distributions are. This can be a useful metric for distinguishing between distributions.\n\nOption B involves taking the product of the squared differences between each probability mass function value of $P_0$ and $P_1$ and subtracting that from 1. This does not directly correspond to a standard measure of distinguishability between distributions.\n\nOption C states that the maximal advantage is 1 if $P_0 = P_1$ and 0 otherwise. This indicates that there is maximal advantage when the two distributions are identical, but does not provide a quantitative measure for distinguishing between them.\n\nOption D suggests that the maximal advantage is always 1 for computationally unbounded adversaries, regardless of the actual distributions $P_0$ and $P_1. This does not accurately reflect the distinguishability of the distributions based on a single sample.\n\nTherefore, the correct answer for the maximal advantage of a distinguisher using a single sample between two distributions $P_0$ and $P_1$ over a discrete set $Z is option A: $\\frac{1}{2}\\sum_{x\\in Z} | P_0(x) - P_1(x)|$. This formula calculates the total variation distance between the two distributions, providing a measure of how well they can be distinguished. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following provides forward secrecy.\n\nOptions:\nA. Transferring keys in plain.\nB. Ephemeral Diffie-Hellman.\nC. Semi-static Diffie-Hellman.\nD. Static Diffie-Hellman.\n", "answer": "Explanations: Forward secrecy is a property of key agreement protocols that ensures that the compromise of a long-term key does not compromise past session keys. This means that even if an attacker obtains the long-term secret keys, they would not be able to decrypt past communication sessions.\n\nNow, let's analyze each option:\n\n1) Transferring keys in plain: Transferring keys in plain text does not provide forward secrecy because if the keys are intercepted, all past and future communications encrypted with those keys can be decrypted. Therefore, this option does not provide forward secrecy.\n\n2) Ephemeral Diffie-Hellman: Ephemeral Diffie-Hellman (also known as DHE or ECDHE) is a key exchange protocol that provides forward secrecy. In this protocol, a new set of keys is generated for each session, ensuring that compromising one session does not affect the security of other sessions.\n\n3) Semi-static Diffie-Hellman: Semi-static Diffie-Hellman refers to a setup where one party's key remains static while the other party's key changes. This does not provide forward secrecy because if the static key is compromised, all past and future communications involving that key are at risk.\n\n4) Static Diffie-Hellman: Static Diffie-Hellman refers to a setup where the same key pair is used for multiple sessions. This also does not provide forward secrecy because compromising the static key would allow an attacker to decrypt all past and future communications using that key.\n\nTherefore, the option that provides forward secrecy is \\textbf{Ephemeral Diffie-Hellman}. ///Answer: B"}
{"subject": "", "question": "Question: The encryption in GSM is done by ...\n\nOptions:\nA. A3.\nB. A8.\nC. E0.\nD. A5.\n", "answer": "Explanations: The encryption in GSM is done by A5.\\n\\nExplanation:\\n1. A3: A3 is not used for encryption in GSM. It is used for authentication purposes.\\n2. A8: A8 is not used for encryption in GSM. It is used for key generation.\\n3. E0: E0 is a stream cipher algorithm used in the encryption of 3G networks, not in GSM.\\n4. A5: A5 is the algorithm used for encryption in GSM networks. It is a symmetric stream cipher that encrypts the voice and data transmitted over the air interface in GSM networks to ensure privacy and security.\\n\\nIn conclusion, the correct answer is A5, as it is the encryption algorithm specifically designed for use in GSM networks. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following statements are correct?\n\nOptions:\nA. One iteration of standard SGD for SVM costs roughly $\\Theta(D)$, where $D$ is the dimension.\nB. Unions of convex sets are convex.\nC. Hinge loss (as in SVMs) is typically preferred over L2 loss (least squares loss) in classification tasks.\nD. In PCA, the first principal direction is the eigenvector of the data matrix $\\boldsymbol{X}$ with largest associated eigenvalue.\nE. MSE (mean squared error) is typically more sensitive to outliers than MAE (mean absolute error).\nF. One iteration of standard SGD for logistic regression costs roughly $\\Theta(N D)$, where $N$ is the number of samples and $D$ is the dimension.\n", "answer": "Explanations: Let's go through each statement one by one:\n\n1. \"One iteration of standard SGD for SVM costs roughly $\\\\Theta(D)$, where $D$ is the dimension.\"\n    - This statement is correct. In standard Stochastic Gradient Descent (SGD) for Support Vector Machines (SVM), the cost of one iteration is proportional to the dimension $D$ of the data.\n\n2. \"Unions of convex sets are convex.\"\n    - This statement is correct. The union of convex sets is indeed convex. This property is fundamental in convex optimization.\n\n3. \"Hinge loss (as in SVMs) is typically preferred over L2 loss (least squares loss) in classification tasks.\"\n    - This statement is correct. Hinge loss is often preferred over L2 loss in classification tasks, especially in scenarios where we want to focus on maximizing the margin between classes.\n\n4. \"In PCA, the first principal direction is the eigenvector of the data matrix $\\\\boldsymbol{X}$ with the largest associated eigenvalue.\"\n    - This statement is correct. In Principal Component Analysis (PCA), the first principal direction is indeed the eigenvector corresponding to the largest eigenvalue of the data covariance matrix.\n\n5. \"MSE (mean squared error) is typically more sensitive to outliers than MAE (mean absolute error).\"\n    - This statement is correct. Mean Squared Error (MSE) gives higher weights to large errors due to squaring, making it more sensitive to outliers compared to Mean Absolute Error (MAE).\n\n6. \"One iteration of standard SGD for logistic regression costs roughly $\\\\Theta(N D)$, where $N$ is the number of samples and $D$ is the dimension.\"\n    - This statement is correct. In standard Stochastic Gradient Descent (SGD) for logistic regression, the cost of one iteration is proportional to both the number of samples $N$ and the dimension $D$ of the data.\n\nTherefore, all of the statements provided are correct. Let me know if you have any more questions! ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion. Let $X$ be the plaintext, $Y$ be the ciphertext and $K$ be the key.\n\nOptions:\nA. The scheme is perfectly secure iff $X$ is independent of $Y$.\nB. The scheme is perfectly secure iff for all $x,y$ $ \\Pr[Y=y] \\neq 0 \\Rightarrow \\Pr [X=x | Y =y ] = \\Pr[X=x]\\;.$\nC. It is possible to achieve perfect secrecy with $K$ smaller than $X$.\nD. Perfect secrecy is too expensive for most of the applications.\n", "answer": "Explanations: Option A: $X$ and $Y$ are the same length.\n- This assertion is false. In encryption processes, the length of the ciphertext is not always the same as the length of the plaintext. Encryption algorithms can change the length of the data during encryption, such as through padding or block cipher modes. Therefore, $X$ and $Y$ being the same length is not necessarily true in encryption scenarios.\n\nOption B: $X$ and $K$ are the same length.\n- This assertion is false. The length of the plaintext $X$ and the key $K$ are typically independent of each other in encryption processes. The key length is determined by the specific encryption algorithm and security requirements, while the plaintext length is determined by the data being encrypted. These lengths do not need to be the same for encryption to occur.\n\nOption C: $Y$ and $K$ are the same length.\n- This assertion is false. The length of the ciphertext $Y$ and the key $K$ are also typically independent of each other in encryption processes. The key length is chosen based on the encryption algorithm and security considerations, while the ciphertext length is determined by the encryption process. There is no requirement for $Y$ and $K$ to be the same length in encryption.\n\nOption D: $X$, $Y$, and $K$ are all the same length.\n- This assertion is false. In encryption, the lengths of the plaintext, ciphertext, and key are not necessarily the same. Each component in the encryption process has its own unique characteristics and length requirements. The lengths of $X$, $Y$, and $K$ are determined by the encryption algorithm and the specific encryption process, and they may not be equal in length. ///Answer: D"}
{"subject": "", "question": "Question: Let $\\Ym = \\xm\\Wm + \bv$, where $\\xm,\\Wm,\\Ym \\in \\R^{k \times k}$ and $\bv \\in R^{1 \times k}$, represent a linear layer of width $k$ operating on a batch of $k$ inputs where the addition is broadcasted as in Numpy or PyTorch.\n\tThe network is trained with respect to a loss function $L(\\Ym)$ that only depends on $\\Wm$ and $\bv$ through $\\Ym$.\n\tGiven $\\delta_{\\Ym}=\frac{\\partial L}{\\partial \\Ym}$, how can we compute $\\delta_{\\Wm}=\frac{\\partial L}{\\partial \\Wm}$ and $\\delta_{\bv}=\frac{\\partial L}{\\partial \bv}$?\n\tLet $\\mathbf{1}_{1,k} = [1, 1, \\ldots, 1]$ with shape $1 \times k$. ?\n\nOptions:\nA. $\\delta_{\\Wm}=\\xm \\delta_{\\Ym},\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$ \nB. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm,\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$\nC. $\\delta_{\\Wm}=\\xm^\top \\delta_{\\Ym},\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$\nD. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm^\top,\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$\nE. $\\delta_{\\Wm}=\\xm \\delta_{\\Ym},\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^\top$\nF. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm,\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^\top$\nG. $\\delta_{\\Wm}=\\xm^\top \\delta_{\\Ym},\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^\top$\nH. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm^\top,\\quad \\delta_{\bv}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^\top$\n", "answer": "Explanations: Let's carefully analyze each option:\n\nA. $\\delta_{\\Wm}=\\xm \\delta_{\\Ym},\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$  \nThis option is **True**. The derivative of the loss function with respect to $\\Wm$ can be obtained by multiplying $\\xm$ with $\\delta_{\\Ym}$, and the derivative with respect to $\\mathbf{v}$ can be obtained by broadcasting $\\delta_{\\Ym}$ with $\\mathbf{1}_{1,k}$.\n\nB. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm,\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$  \nThis option is **False**. The correct order of multiplication for computing $\\delta_{\\Wm}$ should be $\\xm$ on the left side of $\\delta_{\\Ym}$.\n\nC. $\\delta_{\\Wm}=\\xm^{T} \\delta_{\\Ym},\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$  \nThis option is **False**. The transpose operation on $\\xm$ is not necessary for calculating the derivative with respect to $\\Wm$.\n\nD. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm^{T},\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}$  \nThis option is **False**. Similar to option B, the correct order of multiplication for computing $\\delta_{\\Wm}$ should be $\\xm$ on the left side of $\\delta_{\\Ym}$, without the transpose operation.\n\nE. $\\delta_{\\Wm}=\\xm \\delta_{\\Ym},\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^{T}$  \nThis option is **False**. The transpose operation on $\\delta_{\\Ym}$ in the calculation of $\\delta_{\\mathbf{v}}$ is incorrect.\n\nF. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm,\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^{T}$  \nThis option is **False**. The transpose operation on $\\delta_{\\Ym}$ in the calculation of $\\delta_{\\mathbf{v}}$ is incorrect.\n\nG. $\\delta_{\\Wm}=\\xm^{T} \\delta_{\\Ym},\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^{T}$  \nThis option is **False**. The transpose operations on both $\\xm$ and $\\delta_{\\Ym}$ are not necessary for computing the derivatives.\n\nH. $\\delta_{\\Wm}=\\delta_{\\Ym} \\xm^{T},\\quad \\delta_{\\mathbf{v}}=\\mathbf{1}_{1,k} \\delta_{\\Ym}^{T}$  \nThis option is **False**. The transpose operations on both $\\xm$ and $\\delta_{\\Ym}$ are incorrect.\n\nTherefore, the correct options are **A** and the other options are false. ///Answer: A"}
{"subject": "", "question": "Question: Modularity of a social network always:?\n\nOptions:\nA. Increases with the number of communities\nB. Increases when an edge is added between two members of the same community\nC. Decreases when new nodes are added to the social network that form their own communities\nD. Decreases if an edge is removed\n", "answer": "Explanations: The correct option is: 'Increases when an edge is added between two members of the same community'.\n\nExplanation:\nTo understand why the modularity of a social network increases when an edge is added between two members of the same community, we need to first define what modularity in a social network means. Modularity is a measure of the structure of a network, indicating how well the network can be divided into communities or modules. \n\nIn social network analysis, modularity is used to evaluate the strength of division of a network into communities. A higher modularity value indicates that the network has dense connections between nodes within the same community but sparse connections between nodes in different communities.\n\nWhen an edge is added between two members of the same community, it means that there is an additional connection established within an existing community. This new edge reinforces the cohesion of the community by increasing the number of connections between its members. As a result, the modularity of the social network increases because the community structure becomes more pronounced and distinct.\n\nIn contrast, if an edge is added between members of different communities, it may weaken the existing community structure by introducing connections that bridge across communities. This could potentially decrease the modularity of the network, as the distinction between communities becomes less clear.\n\nTherefore, the statement that modularity of a social network increases when an edge is added between two members of the same community is reflective of how network structure and connectivity influence the strength of community divisions. ///Answer: B"}
{"subject": "", "question": "Question: In JOS, suppose a value is passed between two Envs. What is the minimum number of executed system calls?\n\nOptions:\nA. 1\nB. 2\nC. 3\nD. 4\n", "answer": "Explanations: In the JOS operating system, if a value is passed between two Envs (environments), the minimum number of executed system calls would be 2. \n\nThe first system call would be used to send the value from one Env to the kernel, and the second system call would be used to receive the value in the other Env from the kernel. \n\nTherefore, the minimum number of executed system calls in this scenario would be 2. ///Answer: B"}
{"subject": "", "question": "Question: How does matrix factorization address the issue of missing ratings?\n?\n\nOptions:\nA. It uses regularization of the rating matrix\nB. It performs gradient descent only for existing ratings\nC. It sets missing ratings to zero\nD. It maps ratings into a lower-dimensional space\n", "answer": "Explanations: The answer is 4) It maps ratings into a lower-dimensional space.\n\nMatrix factorization is a collaborative filtering technique used in recommendation systems to address the issue of missing ratings. By mapping the ratings into a lower-dimensional space, matrix factorization can effectively capture latent features or patterns in the data, allowing it to predict missing ratings based on the relationships between users and items. This approach leverages the idea that even though certain ratings may be missing, they can still be inferred from the existing ratings and the underlying structure of the data. Regularization (option 1) is used to prevent overfitting but does not directly address missing ratings. Performing gradient descent only for existing ratings (option 2) is not accurate, as the goal is to predict missing ratings as well. Setting missing ratings to zero (option 3) is a naive approach and may not reflect the true underlying patterns in the data. ///Answer: D"}
{"subject": "", "question": "Question: The differential probability of a function $f:\\{0,1\\}^p\\rightarrow \\{0,1\\}^q$ is, given $a\\in\\{0,1\\}^p$ and $b\\in\\{0,1\\}^q$, defined by \\dots?\n\nOptions:\nA. $\\mathrm{DP}^f(a,b)=\\Pr_{X\\in _U \\{0,1\\}^p} [f(X\\oplus a)=f(X\\oplus b)] $.\nB. $\\mathsf{DP}^f(a,b)=\\Pr_{X\\in _U \\{0,1\\}^p} [f(X)\\oplus a=f(X)\\oplus b] $.\nC. $\\mathsf{DP}^f(a,b)=\\Pr_{X\\in _U \\{0,1\\}^p} [f(X\\oplus b)=f(X)\\oplus a] $.\nD. $\\mathsf{DP}^f(a,b)=\\Pr_{X\\in _U \\{0,1\\}^p} [f(X\\oplus a)=f(X)\\oplus b] $.\n", "answer": "Explanations: The correct answer is:\n\nA: $\\mathrm{DP}^f(a,b)=\\Pr_{X\\in _U \\{0,1\\}^p} [f(X\\oplus a)=f(X\\oplus b)] $.\n\nThe reason for this is that the differential probability of a function measures the probability that the function output changes when the input is changed by a specific difference. In this case, $a\\in\\{0,1\\}^p$ and $b\\in\\{0,1\\}^q$ represent the differences in the inputs. The definition in option A captures this by XORing the input with $a$ and $b$ separately and checking the probability that the function output remains the same. Options B, C, and D do not correctly capture the definition of the differential probability. ///Answer: A:"}
{"subject": "", "question": "Question: What is/are the goal/s of compartmentalization?\n\nOptions:\nA. Isolate faults to individual (ideally small) components.\nB. Allow easier abstraction of functionalities across components.\nC. Make faults more severe as the surrounding code is smaller.\nD. Better performance (i.e., lower overhead) since a compartment\n        can fail without affecting others.\n", "answer": "Explanations: The goals of compartmentalization include:\n\n1. Isolating faults to individual (ideally small) components: By compartmentalizing code, any faults or errors that occur are contained within specific components, making it easier to identify and fix issues without impacting the entire system.\n\n2. Allowing easier abstraction of functionalities across components: Compartmentalization enables the separation of different functionalities into distinct components, making it easier to understand and manage the system's overall structure and logic.\n\n3. Making faults more severe as the surrounding code is smaller: When faults occur within a compartmentalized system, the impact is limited to a smaller portion of the codebase, making it easier to troubleshoot and minimize the scope of potential issues.\n\n4. Achieving better performance with lower overhead: Compartmentalization can lead to improved performance as each compartment operates independently, allowing for more efficient resource allocation and utilization. Additionally, if a compartment fails, it can do so without affecting the functionality of other components, contributing to overall system stability and reliability. ///Answer: A:"}
{"subject": "", "question": "Question: A monad M is a parametric type M[T] with two operations, flatMap and unit: extension [T, U](m: M[T]) def flatMap(f: T => M[U]): M[U] def unit[T](x: T): M[T] To qualify as a monad, a type has to satisfy the three following laws for all m: M[T], x: T, f: T => M[U] and g: U => M[V]: (Associativity) m.flatMap(f).flatMap(g) === m.flatMap(f(_).flatMap(g)) (Left unit) unit(x).flatMap(f) === f(x) (Right unit) m.flatMap(unit) === m Is List with its usual flatMap method and unit(x) = List(x) a monad?\n\nOptions:\nA. Yes\nB. No\n", "answer": "Explanations: Yes, List with its usual flatMap method and unit(x) = List(x) satisfies the three monad laws. The associativity law is upheld as List.flatMap(f).flatMap(g) will always be equivalent to List.flatMap(f(_).flatMap(g)), ensuring the order of operations does not affect the result. The left unit law holds true as unit(x).flatMap(f) will always be equal to f(x), showcasing the identity element's behavior. Finally, the right unit law is maintained as m.flatMap(unit) will always be equal to m, showcasing the preservation of values in the monadic context. ///Answer: A"}
{"subject": "", "question": "Question: The Vernam cipher \\ldots?\n\nOptions:\nA. is always secure.\nB. is secure only if we always use the same key.\nC. is secure only if we always use a different key.\nD. is always insecure.\n", "answer": "Explanations: The Vernam cipher is a type of encryption technique that is theoretically unbreakable if used correctly. It involves XORing the plaintext with a random key of the same length to produce the ciphertext. \n\nIf we always use the same key, the encryption becomes vulnerable to frequency analysis attacks and other cryptographic attacks. Therefore, the statement \"is secure only if we always use a different key\" is incorrect.\n\nIf we always use a different key for each message, the Vernam cipher remains secure and unbreakable. Therefore, the correct option is \"is secure only if we always use a different key.\" ///Answer: C"}
{"subject": "", "question": "Question: Which of the following attacks makes sense for a MAC:?\n\nOptions:\nA. Chosen message attack.\nB. Chosen ciphertext attack.\nC. Ciphertext only attack.\nD. Decryption attack.\n", "answer": "Explanations: A: Chosen message attack.\n\nIn a MAC (Message Authentication Code), the goal is to verify the integrity and authenticity of a message. A chosen message attack involves an attacker being able to choose arbitrary messages and obtain their corresponding MACs. This type of attack can potentially allow the attacker to learn information about the MAC algorithm or even forge MACs for messages they did not create.\n\nTherefore, a chosen message attack makes sense for a MAC and can be a security concern if the MAC algorithm is vulnerable to this type of attack. ///Answer: A"}
{"subject": "", "question": "Question: \nYour aim is to evaluate a movie review analysis system, the purpose of which is to determine whether a review is globally positive or negative.\nFor each movie review, such a system outputs one of the following classes: positive and negative.\nTo perform your evaluation, you collect a large set of reviews and have it annotated by two human annotators. This corpus contains 95% of negative reviews (this 95% ratio is for this first question only and may change in the next\n            questions).\n\nWhat metrics do you think are appropriate to evaluate the system on this corpus?\n\nYou will get a penalty for wrong ticks.\n?\n\nOptions:\nA. Cohen's kappa\nB. accuracy\nC. precision\nD. recall\nE. standard deviation\nF. F1-score\n", "answer": "Explanations: The appropriate metrics to evaluate the movie review analysis system on the corpus with 95% negative reviews are:\n1. Cohen's kappa: to measure inter-rater agreement between system classifications and human annotations.\n2. Accuracy: to assess overall correctness of system classifications.\n3. Precision: to evaluate the proportion of true positive classifications.\n4. Recall: to measure the system's ability to capture all actual positive instances.\n5. F1-score: to provide a balanced evaluation of precision and recall. ///Answer: A"}
{"subject": "", "question": "Question: (Infinite Data) Assume that your training data $\\mathcal{S}=\\left\\{\\left(\\mathbf{x}_{n}, y_{n}\\right)\\right\\}$ is iid and comes from a fixed distribution $\\mathcal{D}$ that is unknown but is known to have bounded support. Assume that your family of models contains a finite number of elements and that you choose the best such element according to the training data. You then evaluate the risk for this chosen model. Call this the training risk. As $|\\mathcal{S}|$ tends to infinity, this training risk converges to the true (according to the distribution $\\mathcal{D}$ ) risk of the best model in this family.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The correct answer is True.\n\nWhen the training data $\\mathcal{S}$ is iid and comes from a fixed distribution $\\mathcal{D}$ with bounded support, and the family of models contains a finite number of elements, the training risk of the chosen best model according to the training data will converge to the true risk of the best model in this family as the size of the training data, denoted by $|\\mathcal{S}|$, tends to infinity.\n\nThis is due to the law of large numbers in statistics, which states that as the sample size becomes larger, the sample mean converges to the population mean. In this context, as the size of the training data grows towards infinity, the training risk of the best model will converge to the true risk of the best model, reflecting the performance of the model on the underlying distribution $\\mathcal{D$. \n\nTherefore, the statement is true. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement.\n\nOptions:\nA. Cryptographic primitives used in Bluetooth are provably secure.\nB. In WEP, authentication is done with the pre-shared keys.\nC. The security of Bluetooth 2.0 pairing is based on PIN.\nD. Due to memory limitations, dummy devices can share the same key with everyone.\n", "answer": "Explanations: The false statement is: Cryptographic primitives used in Bluetooth are provably secure.\n\nExplanation: While cryptographic primitives used in Bluetooth may be secure, they are not provably secure. This means that there is no mathematical proof that they are secure under all conditions. Instead, their security is based on extensive testing and analysis, as well as the assumption that they are secure based on current knowledge in the field of cryptography. ///Answer: A"}
{"subject": "", "question": "Question: Let $n=pq$ be a RSA modulus and let $(e,d)$ be a RSA public/private key. Tick the \\emph{correct} assertion.\n\nOptions:\nA. Finding a multiple of $\\lambda(n)$ is equivalent to decrypt a ciphertext.\nB. $ed$ is a multiple of $\\phi(n)$.\nC. The two roots of the equation $X^2 - (n-\\phi(n)+1)X+n$ in $\\mathbb{Z}$ are $p$ and $q$.\nD. $e$ is the inverse of $d$ mod $n$.\n", "answer": "Explanations: ##Answer:\n$ed$ is a multiple of $\\phi(n)$.\n\n##Reasons:\n1. In the RSA cryptosystem, the public and private keys are typically chosen such that they satisfy the relationship $ed \\equiv 1 \\pmod{\\phi(n)}$, where $\\phi(n) = (p-1)(q-1)$ is Euler's totient function for the RSA modulus $n = pq$.\n2. The correct assertion is that $ed$ is a multiple of $\\phi(n)$, not just equivalent to 1. This relationship ensures that the public exponent $e$ and the private exponent $d$ are inverses modulo $\\phi(n)$, providing the basis for RSA encryption and decryption.\n3. The other options are incorrect:\n   - Finding a multiple of $\\lambda(n)$ is not equivalent to decrypting a ciphertext. The Carmichael function $\\lambda(n)$ is related to the least common multiple of the orders of the prime factors of $n$, but it is not directly used in RSA decryption.\n   - The statement about the roots of the equation $X^2 - (n-\\phi(n)+1)X+n$ in $\\mathbb{Z}$ being $p$ and $q$ is not accurate in the context of RSA keys and modulus.\n   - $e$ being the inverse of $d$ mod $n$ is not necessarily true; the inverse relationship should be mod $\\phi(n)$ to ensure the correctness of RSA encryption and decryption.\n4. Therefore, the correct assertion in the context of RSA keys and modulus is that $ed$ is a multiple of $\\phi(n)$ to maintain the required relationship between the public and private exponents for proper RSA operation. ///Answer: B:"}
{"subject": "", "question": "Question: Fundamentally, why clustering is considered an unsupervised machine learning technique?\n\nOptions:\nA. Number of clusters are not known.\nB. The class labels are not known.\nC. The features are not known.\nD. The clusters can be different with different initial parameters.\n", "answer": "Explanations: The correct option is: 2. The class labels are not known.\n\nIn clustering, the algorithm groups similar data points together based on certain characteristics or features, without being provided with any class labels or target variable. This is why clustering is considered an unsupervised machine learning technique. ///Answer: B"}
{"subject": "", "question": "Question: Consider the Vernam Cipher. Let P=0010 1011 and K=1110 0110 and what is C=P $\\oplus$ K?\n\nOptions:\nA. 0011 0010\nB. 1100 0111\nC. 1100 1101\nD. 1101 0111\n", "answer": "Explanations: To find the ciphertext (C), we perform the XOR operation (\u2295) between the plaintext (P) and the key (K) bit by bit.\n\nGiven:\nP = 0010 1011\nK = 1110 0110\n\nPerforming XOR operation:\nC = P \u2295 K\nC = 0010 1011 \u2295 1110 0110\nC = 1100 1101\n\nTherefore, the answer is 1100 1101. ///Answer: C"}
{"subject": "", "question": "Question: Let $H$ be a hash function based on the Merkle-Damg{\\aa}rd construction. The Merkle-Damg{\\aa}rd theorem says that \\dots?\n\nOptions:\nA. \\dots $H$ is collision-resistant when the compression function is collision-resistant.\nB. \\dots the compression function is collision-resistant when $H$ is collision-resistant.\nC. \\dots $H$ is collision-resistant.\nD. \\dots $H$ is not collision-resistant.\n", "answer": "Explanations: The Merkle-Damg\u00e5rd construction is a method used to build a cryptographic hash function from a compression function. In this construction, the message to be hashed is processed in blocks using the compression function, and the final output is obtained by applying a finalization function to the output of the last compression function.\n\nThe Merkle-Damg\u00e5rd theorem states that if the compression function is collision-resistant, then the resulting hash function built using the Merkle-Damg\u00e5rd construction is collision-resistant as well. This means that if it is computationally infeasible to find two different inputs that produce the same output for the compression function, then it should also be computationally infeasible to find two different inputs that produce the same hash value for the hash function.\n\nBased on the Merkle-Damg\u00e5rd theorem, the correct statement among the given options is:\n\n- $H$ is collision-resistant when the compression function is collision-resistant.\n\nThis statement aligns with the principle that the security properties of the hash function are derived from the security properties of the underlying compression function in the Merkle-Damg\u00e5rd construction. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. In an interactive proof,?\n\nOptions:\nA. \\ldots the completeness property does not depend on the random coins of the prover.\nB. \\ldots the completeness property does not depend on the random coins of the verifier.\nC. \\ldots the soundness property refers to the case where the verifier is corrupted.\nD. \\ldots the complexity of the verifier is polynomial in the size of the input.\n", "answer": "Explanations: To determine the true assertion, let's go through each option:\n\n1) The completeness property in an interactive proof does not depend on the random coins of the prover. This statement is false. The completeness property ensures that an honest prover can convince the verifier with high probability, and this may indeed depend on the prover's random coins.\n\n2) The completeness property in an interactive proof does not depend on the random coins of the verifier. This statement is true. The completeness property guarantees that if the prover's statement is true, the verifier will accept with high probability regardless of the verifier's random coins.\n\n3) The soundness property in an interactive proof refers to the case where the verifier is corrupted. This statement is not accurate. The soundness property ensures that no cheating prover can convince the verifier with high probability.\n\n4) In an interactive proof, the complexity of the verifier is polynomial in the size of the input. This statement is generally true. Interactive proofs are designed such that the verifier's computation complexity is polynomial in the size of the input to allow for efficient verification.\n\nTherefore, the correct assertion is: the completeness property does not depend on the random coins of the verifier. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion.\n\nOptions:\nA. The ElGamal encryption is non-deterministic.\nB. An ElGamal ciphertext is longer than an ElGamal plaintext.\nC. The security of ElGamal is based on the factorization problem.\nD. ElGamal uses public and private keys.\n", "answer": "Explanations: The false assertion is: The security of ElGamal is based on the factorization problem.\n\nExplanation:\n\n1. The ElGamal encryption is non-deterministic: This statement is true. In the ElGamal encryption scheme, the randomness introduced during encryption makes it non-deterministic, resulting in different ciphertexts for the same plaintext.\n\n2. An ElGamal ciphertext is longer than an ElGamal plaintext: This statement is true. In the ElGamal encryption scheme, the ciphertext consists of two elements (a pair of group elements), making it longer than the plaintext message.\n\n3. The security of ElGamal is based on the factorization problem: This statement is false. The security of the ElGamal encryption scheme is based on the computational difficulty of the discrete logarithm problem, particularly in cyclic groups like the multiplicative group of integers modulo a prime.\n\n4. ElGamal uses public and private keys: This statement is true. The ElGamal encryption scheme is a public-key cryptosystem that uses a pair of keys - a public key for encryption and a private key for decryption.\n\nTherefore, the false assertion among the options is: The security of ElGamal is based on the factorization problem. ///Answer: C"}
{"subject": "", "question": "Question: Maintaining the order of document identifiers for vocabulary construction when partitioning the document collection is important?\n\nOptions:\nA. in the index merging approach for single node machines\nB. in the map-reduce approach for parallel clusters\nC. in both\nD. in neither of the two\n", "answer": "Explanations: The correct answer is \"in both\" - maintaining the order of document identifiers is important in both the index merging approach for single node machines and the map-reduce approach for parallel clusters.\n\nExplanation:\n1. In the index merging approach for single node machines: In this approach, the entire document collection is processed on a single machine to create the inverted index. Maintaining the order of document identifiers is crucial in this process to ensure that the vocabulary construction is accurate and consistent. Any disruption in the order could lead to errors in the index, impacting document retrieval.\n\n2. In the map-reduce approach for parallel clusters: In this approach, the document collection is partitioned into smaller chunks and processed in parallel on multiple nodes. Each node processes a subset of documents, and the final output needs to be merged in a way that preserves the order of document identifiers. Maintaining the order is important in this approach as well to ensure the accuracy of the inverted index and proper document retrieval.\n\nTherefore, maintaining the order of document identifiers is important in both the index merging approach for single node machines and the map-reduce approach for parallel clusters. ///Answer: C"}
{"subject": "", "question": "Question: Consider an RSA encryption where the public key is published as $(m, e) = (35, 11)$. Which one of the following numbers is a valid decoding exponent?\n\nOptions:\nA. $11$\nB. $7$\nC. $5$\nD. $17$\n", "answer": "Explanations: In RSA encryption, the public key is typically represented as $(m, e)$ where $m$ is the modulus and $e$ is the encryption exponent. To decode a message encrypted with RSA, we need the private key, which consists of a decoding exponent $d$.\n\nTo find the decoding exponent $d$, we need to calculate it using the given public key $(m, e) = (35, 11)$. The decoding exponent $d$ is the multiplicative inverse of $e$ modulo $\\phi(m)$, where $\\phi(m)$ is Euler's totient function.\n\nFirst, we need to calculate $\\phi(m)$, which is the number of positive integers less than $m$ that are coprime to $m$. In this case, $m = 35$, and since $35 = 5 \\times 7$, we have $\\phi(35) = (5-1)(7-1) = 24$.\n\nNext, we need to find the multiplicative inverse of $e = 11$ modulo $\\phi(m) = 24$. This means finding a number $d$ such that $11d \\equiv 1 \\pmod{24}$. \n\nBy trying out the given options:\n1. $11 \\times 11 = 121 \\equiv 1 \\pmod{24}$, so $d = 11$ is a valid decoding exponent.\n\nTherefore, the correct answer is option 1: $11$. ///Answer: A"}
{"subject": "", "question": "Question: Consider a twelve-round Feistel scheme $\\Psi(F^{k_1},F^{k_2},\\dots,F^{k_{12}})$. Tick the \\emph{minimal necessary} condition for $\\Psi$ to be a permutation:?\n\nOptions:\nA. $F^{k_i}$ are functions\nB. $F^{k_i}$ are injective functions\nC. $F^{k_i}$ are surjective functions\nD. $F^{k_i}$ are bijective functions\n", "answer": "Explanations: In a Feistel scheme, the round function $F$ is applied iteratively in a structure that includes both permutation and substitution operations. For the Feistel scheme to be a permutation, it is crucial that the round functions $F^{k_i}$ are bijective functions.\n\nA bijective function is one that is both injective (one-to-one) and surjective (onto). In the context of a Feistel scheme, this means that each round function must map distinct inputs to distinct outputs (injective) and cover all possible outputs for a given input (surjective).\n\nTherefore, the ticked condition for the Feistel scheme $\\Psi$ to be a permutation is that the round functions $F^{k_i}$ are bijective functions. ///Answer: D"}
{"subject": "", "question": "Question: What is the worst case complexity of listing files in a directory? The file system implements directories as hash-tables.\n\nOptions:\nA. $O(1)$\nB. $O(number of direntries in the directory)$\nC. $O(size of the file system)$\nD. $O(number of direntries in the file system)$\nE. $O(log(number of direntries in the directory))$\n", "answer": "Explanations: To determine the worst case complexity of listing files in a directory when the file system implements directories as hash-tables, we need to consider the operations involved in listing files in a directory. \n\nWhen using a hash-table to implement directories, the time complexity of listing files in a directory depends on the number of entries in the directory and how collisions are handled in the hash-table.\n\nIn the worst case scenario, all entries in the directory hash to the same bucket, resulting in a linked list of entries. In this case, we would need to iterate through all entries in the linked list to list all files in the directory.\n\nTherefore, the worst case complexity of listing files in a directory implemented as a hash-table would be $O(\\text{number of direntries in the directory})$, which is option b).\n\nIn conclusion, the correct answer is:\n\nb) $O(\\text{number of direntries in the directory})$ ///Answer: B"}
{"subject": "", "question": "Question: Let $f$ be any hash function with output domain size $N$.\n\nOptions:\nA. One can find a collision in $f$ in $\\mathcal{O}(\\sqrt{N})$ using almost no memory.\nB. One can find a preimage in $f$ in $\\mathcal{O}(\\sqrt{N})$ using $\\sqrt{N}$ memory.\nC. One can find a second preimage in $f$ in $\\mathcal{O}(\\sqrt{N})$ using $\\sqrt{N}$ memory.\nD. The best collision attack against $f$ runs in time $\\mathcal{O}(N)$.\n", "answer": "Explanations: To analyze the options:\n\n1. **Finding a collision in $f$ in $\\mathcal{O}(\\sqrt{N})$ using almost no memory**:\nTo find a collision in $f$, one can use the birthday attack, which requires about $\\sqrt{N}$ evaluations of the hash function on average to find a collision with high probability. Therefore, this statement is correct.\n\n2. **Finding a preimage in $f$ in $\\mathcal{O}(\\sqrt{N})$ using $\\sqrt{N}$ memory**:\nFinding a preimage in a hash function is generally considered computationally infeasible. With a good hash function, finding a preimage would require an exhaustive search through the entire input space, which is of size $N$. Therefore, this statement is incorrect.\n\n3. **Finding a second preimage in $f$ in $\\mathcal{O}(\\sqrt{N})$ using $\\sqrt{N}$ memory**:\nFinding a second preimage in a hash function is also considered computationally infeasible. It would require searching through the entire input space, similar to finding a preimage. Therefore, this statement is incorrect.\n\n4. **The best collision attack against $f$ runs in time $\\mathcal{O}(N)**:\nThe best collision attack against $f$ is the birthday attack, which runs in $\\mathcal{O}(\\sqrt{N})$ time complexity. Therefore, this statement is incorrect.\n\nHence, the correct option is: One can find a collision in $f$ in $\\mathcal{O}(\\sqrt{N})$ using almost no memory. ///Answer: A"}
{"subject": "", "question": "Question: The number of non-zero entries in a column of a term-document matrix indicates:?\n\nOptions:\nA. how many terms of the vocabulary a document contains\nB. how often a term of the vocabulary occurs in a document\nC. how relevant a term is for a document\nD. none of the other responses is correct\n", "answer": "Explanations: Answer: B. how often a term of the vocabulary occurs in a document\n\nExplanation: The number of non-zero entries in a column of a term-document matrix represents how often a term of the vocabulary occurs in a document. Each row in the term-document matrix corresponds to a term in the vocabulary, and each column corresponds to a document. A non-zero entry indicates that the term represented by that row appears in the document represented by that column. Therefore, the more non-zero entries there are in a column, the more frequently the terms in the vocabulary appear in that particular document. This aligns with option B, making it the most accurate answer. ///Answer: B"}
{"subject": "", "question": "Question: Let $n$ be an RSA modulus. Tick the \\emph{false} assertion.\n\nOptions:\nA. The knowledge of $\\lambda(n)$ allows to factorize $n$.\nB. The knowledge of $\\lambda(n)$ allows to recover the RSA secret exponent.\nC. The knowledge of $\\lambda(n)$ allows to decrypt any ciphertext encrypted with the public exponent.\nD. The knowledge of $\\lambda(n)$ allows to factorize $\\lambda(n)$.\n", "answer": "Explanations: To determine the false assertion, let's analyze each option:\n\n1. The knowledge of $\\lambda(n)$ allows to factorize $n$: This statement is true. Knowing $\\lambda(n)$ can help factorize $n$ since $\\lambda(n)$ is related to the factors of $n$.\n\n2. The knowledge of $\\lambda(n)$ allows to recover the RSA secret exponent: This statement is true. By knowing $\\lambda(n)$ and the public exponent $e$, it is possible to calculate the private exponent $d$.\n\n3. The knowledge of $\\lambda(n)$ allows to decrypt any ciphertext encrypted with the public exponent: This statement is true. With the knowledge of $\\lambda(n)$, the private exponent $d$ can be calculated, which allows for decryption of ciphertext encrypted with the public exponent.\n\n4. The knowledge of $\\lambda(n)$ allows to factorize $\\lambda(n)$: This statement is false. The knowledge of $\\lambda(n)$ does not provide a direct way to factorize $\\lambda(n)$ since $\\lambda(n)$ is a function of the factors of $n$ and not a separate number to be factorized.\n\nTherefore, the false assertion is: \"The knowledge of $\\lambda(n)$ allows to factorize $\\lambda(n)$.\" ///Answer: D"}
{"subject": "", "question": "Question: Consider a movie recommendation system which minimizes the following objective\n        \frac{1}{2} \\sum_{(d,n)\\in\\Omega} [x_{dn} - (\\mathbf{W} \\mathbf{Z}^\top)_{dn}]^2 + \frac{\\lambda_w}{2} \norm{\\mathbf{W}}_\text{Frob}^2 + \frac{\\lambda_z}{2} \norm{\\mathbf{Z}}_\text{Frob}^2\n        where $\\mathbf{W}\\in \\R^{D\times K}$ and $\\mathbf{Z}\\in \\R^{N\times K}$.\n\t    Suppose movies are divided into genre A and genre B (i.e., $\\mathbf{W}_A\\in \\R^{D_A\times K}, \\mathbf{W}_B\\in \\R^{D_B\times K}, \\mathbf{W}=[\\mathbf{W}_A; \\mathbf{W}_B]$, with $D_A\\!+\\!D_B=D$) and users are divided into group 1 and group 2  (i.e., $\\mathbf{Z}_1\\in \\R^{N_1\times K}, \\mathbf{Z}_2\\in \\R^{N_2\times K}, \\mathbf{Z}=[\\mathbf{Z}_1; \\mathbf{Z}_2]$, with $N_1\\!+\\!N_2=N$). In addition, group 1 users only rate genre A movies while group 2 users only rate genre B movies. Then instead of training a large recommendation system with $(\\mathbf{W}, \\mathbf{Z})$, one may train two smaller recommendation systems with parameters $(\\mathbf{W_A}, \\mathbf{Z_1})$ and $(\\mathbf{W_B}, \\mathbf{Z_2})$ separately. If SGD is used to solve the minimization problems and all conditions remain the same (e.g., hyperparameters, sampling order, initialization, etc), then which of the following statements is true about the two training methods?\n\nOptions:\nA. Feature vectors obtained in both cases remain the same. \nB. Feature vectors obtained in both cases are different.\nC. Feature vectors obtained in both cases can be either same or different, depending on the sparsity of rating matrix.\nD. Feature vectors obtained in both cases can be either same or different, depending on if ratings in two groups and genres are evenly distributed.\n", "answer": "Explanations: In this scenario, we have two separate recommendation systems trained independently with parameters $(\\mathbf{W_A}, \\mathbf{Z_1})$ for group 1 users and genre A movies, and $(\\mathbf{W_B}, \\mathbf{Z_2})$ for group 2 users and genre B movies. We will analyze the implications of training these smaller systems on the feature vectors obtained.\n\nThe given objective function is a regularized matrix factorization problem where the feature vectors $\\mathbf{W}$ and $\\mathbf{Z}$ are learned to minimize the reconstruction error and the Frobenius norm regularization terms.\n\nNow, let's consider the implications of training the two smaller recommendation systems:\n\n- Feature vectors obtained in both cases remain the same:\n    - This statement is not correct because the two systems are trained independently on different subsets of data (group 1 users vs. group 2 users, genre A movies vs. genre B movies), so the learned feature vectors are not expected to be the same.\n\n- Feature vectors obtained in both cases are different:\n    - This statement is correct as the two systems are trained separately on different subsets of data and there is no shared information between the groups and genres, leading to different learned feature vectors.\n\n- Feature vectors obtained in both cases can be either the same or different, depending on the sparsity of the rating matrix:\n    - This statement is less likely to be correct as the sparsity of the rating matrix is typically related to the amount of data available and may not directly impact the difference in learned feature vectors between the two systems.\n\n- Feature vectors obtained in both cases can be either the same or different, depending on if ratings in two groups and genres are evenly distributed:\n    - This statement is also less likely to be correct as the distribution of ratings in the two groups and genres would affect the learned features within each group, but it does not necessarily guarantee that the feature vectors will be the same or different between the two systems.\n\nTherefore, the most appropriate and likely correct statement is that the feature vectors obtained in both cases are different, as the systems are trained independently on separate data subsets and are not expected to share the same learned representations. ///Answer: B."}
{"subject": "", "question": "Question: Let $p$ and $q$ be two distinct prime numbers and let $x \\in \\mathbf{Z}_{pq}^*$. Which of the following assertion is always true in $\\mathbf{Z}_{pq}^*$?\n\nOptions:\nA. $x^{p} = 1$\nB. $x^{q} = 1$\nC. $x^{pq} = 1$\nD. $x^{(p-1)(q-1)} = 1$\n", "answer": "Explanations: The correct assertion that is always true in $\\mathbf{Z}_{pq}^*$ is '$x^{(p-1)(q-1)} = 1$'.\n\nTo explain the process, we can use Euler's theorem, which states that for any integer $a$ coprime to $n$, where $n$ is a positive integer, we have $a^{\\phi(n)} \\equiv 1 \\pmod{n}$. Here, $\\phi(n)$ represents Euler's totient function, which gives the number of positive integers less than $n$ that are coprime to $n$.\n\nIn this case, since $p$ and $q$ are distinct prime numbers, $\\phi(pq) = (p-1)(q-1)$. Therefore, by Euler's theorem, we have $x^{(p-1)(q-1)} \\equiv 1 \\pmod{pq}$ for any $x \\in \\mathbf{Z}_{pq}^*$.\n\nThis means that $x^{(p-1)(q-1)} = 1$ is always true in $\\mathbf{Z}_{pq}^*$. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. In an interactive proof,?\n\nOptions:\nA. \\ldots the completeness property does depend on the random coins of the prover.\nB. \\ldots the complexity of a dishonest prover is kept within polynomial bounds.\nC. \\ldots the soundness property places a threshold on the acceptance of illegitimate proofs.\nD. \\ldots the complexity of the verifier is polynomial in the size of the input.\n", "answer": "Explanations: {\n    \"explaination\": \"The false assertion is: the completeness property does depend on the random coins of the prover. In an interactive proof system, the completeness property does not depend on the random coins of the prover, but rather on the protocol itself and the interaction between the prover and verifier.\",\n    \"answer\": \"the completeness property does depend on the random coins of the prover.\"\n} ///Answer: A"}
{"subject": "", "question": "Question: Generating public parameters for the ElGamal cryptosystem of about $s$ bits requires a complexity in the order of magnitude?\n\nOptions:\nA. $s^2$\nB. $s^3$\nC. $s^4$\nD. $s^5$\n", "answer": "Explanations: When generating public parameters for the ElGamal cryptosystem, we need to perform certain operations such as generating large prime numbers, finding primitive roots modulo a prime, and computing other necessary values. \n\nTo determine the complexity, let's break down the process:\n1. Generating two large prime numbers of about $s$ bits each: This can be done using algorithms like the Sieve of Eratosthenes or the Miller-Rabin primality test, which have a complexity of around $O(s^2)$.\n\n2. Finding a primitive root modulo a prime: This involves checking the primitive roots for the generated prime numbers, which has a complexity of around $O(s)$.\n\n3. Computing other necessary values like the public key: This may involve additional calculations that can be done in polynomial time with respect to $s$.\n\nPutting these steps together, the overall complexity for generating public parameters for the ElGamal cryptosystem is in the order of magnitude of $s^2$.\n\nTherefore, the correct answer is '$s^2$'. ///Answer: A"}
{"subject": "", "question": "Question: Let $H$ be a hash function. Collision resistance means that \\dots?\n\nOptions:\nA. given $y$, it is hard to find $x$ such that $H(x)=y$\nB. given $x$, it is hard to find $y$ such that $H(x)=y$\nC. it is hard to find $x_1$ and $x_2\\neq x_1$ such that $H(x_1)=H(x_2)$\nD. given $x_1$, it is hard to find $x_2\\neq x_1$ such that $H(x_1)=H(x_2)$\n", "answer": "Explanations: The correct option is: \n\n- Given $x_1$, it is hard to find $x_2\\neq x_1$ such that $H(x_1)=H(x_2)$\n\nExplanation:\n\n1. Collision resistance in a hash function means that it is difficult to find two different inputs that produce the same output hash value. This property is crucial for ensuring the security and integrity of cryptographic systems.\n\n2. The option \"Given $x_1$, it is hard to find $x_2\\neq x_1$ such that $H(x_1)=H(x_2)\" precisely captures this definition. It states that if you have one input $x_1$, it should be computationally difficult to find another input $x_2$ that is not equal to $x_1$ but produces the same hash value $H(x_1)=H(x_2)$.\n\n3. This property prevents attackers from creating fake data that produces the same hash value as legitimate data, which could lead to security vulnerabilities such as forged documents, fake digital signatures, or unauthorized access to systems.\n\n4. By ensuring collision resistance, a hash function can maintain the integrity and authenticity of data in various applications like digital signatures, password storage, and data verification.\n\n5. Therefore, the option \"Given $x_1$, it is hard to find $x_2\\neq x_1$ such that $H(x_1)=H(x_2)\" is the correct choice for defining collision resistance in a hash function. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. In order to ensure message sequentiality \\ldots?\n\nOptions:\nA. the participants can acknowledge the receipt of every message.\nB. we can use TLS.\nC. the messages that are sent contain an authenticated sequence number.\nD. we make sure the participants finish with the same last message.\n", "answer": "Explanations: The incorrect assertion is \"the participants can acknowledge the receipt of every message.\" \n\nMy process for determining this is to carefully read each assertion and identify any potential issues or inaccuracies. In this case, the other assertions all relate to methods for ensuring message sequentiality, such as using TLS, including authenticated sequence numbers in messages, and making sure participants finish with the same last message. The assertion about participants acknowledging the receipt of every message does not directly relate to ensuring message sequentiality and may not be necessary for this purpose. Therefore, I concluded that this assertion is the incorrect one. ///Answer: A"}
{"subject": "", "question": "Question: One-time pad ...\n\nOptions:\nA. never uses a key $K$ which is picked from a uniform distribution.\nB. pads the message at least once before encryption.\nC. allows an efficient key management.\nD. uses an invertible group operation such as ``$\\oplus$\" for encryption.\n", "answer": "Explanations: Option: never uses a key $K$ which is picked from a uniform distribution.\n\nExplanation:\nThe one-time pad is a type of encryption method that provides perfect secrecy if executed correctly. Here's how you can understand and solve this question on your own:\n\n1. **Understanding the One-Time Pad**:\n   - The one-time pad is a symmetric key encryption algorithm that uses a randomly generated key known as the one-time pad.\n   - This key is as long as the message being encrypted and is used only once (hence the name \"one-time pad\").\n   - Each bit or character of the message is encrypted using modular addition with the corresponding key bit or character.\n\n2. **Analyzing the Options**:\n   - The given option states that the one-time pad never uses a key $K$ picked from a uniform distribution. This is a correct statement.\n   - In the one-time pad, the key $K$ is indeed generated randomly and should ideally be picked from a true random source to ensure perfect secrecy.\n   - If the key $K$ were not random or picked from a uniform distribution, it could lead to patterns in the encryption process, compromising the security of the encryption scheme.\n\n3. **Eliminating Incorrect Options**:\n   - Let's briefly analyze the other options to ensure the correctness of our choice:\n     - The one-time pad does not pad the message before encryption; it encrypts the message as is.\n     - Efficient key management is not a defining feature of the one-time pad, as managing one-time keys securely can be challenging.\n     - The one-time pad uses modular addition (XOR operation) for encryption, not an invertible group operation.\n\n4. **Conclusion**:\n   - By understanding the principles of the one-time pad and analyzing the options provided, you can conclude that the option \"never uses a key $K$ which is picked from a uniform distribution\" aligns with the characteristics of the one-time pad encryption method.\n\nIn this way, you can approach similar questions by breaking down the concepts, reasoning through the options, and eliminating incorrect choices based on your knowledge and understanding of the topic. ///Answer: A"}
{"subject": "", "question": "Question: KEM/DEM refers to\\dots?\n\nOptions:\nA. an encryption scheme.\nB. a digital signature scheme.\nC. a commitment scheme.\nD. a hash function.\n", "answer": "Explanations: In the context of cryptography, KEM/DEM refers to a hybrid encryption scheme that combines a Key Encapsulation Mechanism (KEM) with a Data Encapsulation Mechanism (DEM). \n\n1. A Key Encapsulation Mechanism (KEM) is used to generate and encapsulate a symmetric encryption key. This key is securely shared with the intended recipient using their public key. This ensures that only the recipient, who possesses the corresponding private key, can decrypt the symmetric key.\n\n2. A Data Encapsulation Mechanism (DEM) is then used with the symmetric encryption key to encrypt the actual data. This symmetric key is typically much faster for encrypting and decrypting large amounts of data compared to asymmetric keys.\n\nTherefore, KEM/DEM is an encryption scheme because it combines the secure key exchange provided by the KEM with the efficient data encryption provided by the DEM. \n\nSo, the correct answer is Option 1: an encryption scheme. ///Answer: A"}
{"subject": "", "question": "Question: Consider a binary classification problem with classifier $f(\\mathbf{x})$ given by $$ f(\\mathbf{x})= \\begin{cases}1, & g(\\mathbf{x}) \\geq 0 \\\\ -1, & g(\\mathbf{x})<0\\end{cases} $$ and $\\mathbf{x} \\in \\mathbb{R}^{6}$. Consider a specific pair $(\\mathbf{x}, y=1)$ and assume that $g(\\mathbf{x})=8$. In particular this means that this point is classified correctly by $f$. Assume further that we have computed the gradient of $g$ at $\\mathbf{x}$ to be $\\nabla_{\\mathbf{x}} g(\\mathbf{x})=(+1,-2,+3,-4,+5,-6)$. You are allowed to make one step in order to (hopefully) find an adversarial example. In the following four questions, assume $\\epsilon=1$. Which offset $\\delta$ with $\\|\\delta\\|_{\\infty} \\leq 1$ yields the smallest value for $g(\\mathbf{x}+\\delta)$, assuming that $g$ is (locally) linear?\n\nOptions:\nA. $(+1,-2,+3,-4,+5,-6)$\nB. $-(0,0,0,0,0,1)$\nC. $(0,0,0,0,0,1)$\nD. $(-1,-1,-1,-1,-1,-1)$\nE. $(+1,+1,+1,+1,+1,+1)$\nF. $(-1,+1,-1,+1,-1,+1)$\nG. $(+1,-1,+1,-1,+1,-1)$\nH. $(-1,+2,-3,+4,-5,+6)$\n", "answer": "Explanations: 1. $(+1,-2,+3,-4,+5,-6)$: This is the gradient of $g$ at $\\mathbf{x}$ and moving in this direction will maintain the current value of $g(\\mathbf{x})$.\n\n2. $-(0,0,0,0,0,1)$: This offset corresponds to moving $\\delta = (0,0,0,0,0,-1)$, which is in the opposite direction to the gradient. This would likely decrease the value of $g(\\mathbf{x})$.\n\n3. $(0,0,0,0,0,1)$: This offset corresponds to moving $\\delta = (0,0,0,0,0,1)$, which is in the same direction as the gradient. This would likely increase the value of $g(\\mathbf{x})$.\n\n4. $(-1,-1,-1,-1,-1,-1)$: This offset is not in the direction opposite to the gradient and would likely not result in a decrease in $g(\\mathbf{x})$.\n\n5. $(+1,+1,+1,+1,+1,+1)$: This offset is not in the direction opposite to the gradient and would likely not result in a decrease in $g(\\mathbf{x})$.\n\n6. $(-1,+1,-1,+1,-1,+1)$: This offset is not in the direction opposite to the gradient and would likely not result in a decrease in $g(\\mathbf{x})$.\n\n7. $(+1,-1,+1,-1,+1,-1)$: This offset is not in the direction opposite to the gradient and would likely not result in a decrease in $g(\\mathbf{x})$.\n\n8. $(-1,+2,-3,+4,-5,+6)$: This offset corresponds to moving $\\delta = (-1,2,-3,4,-5,6)$, which is in the direction opposite to the gradient. This is the correct choice as it would likely decrease the value of $g(\\mathbf{x})$. \n\nTherefore, the offset $\\delta$ that yields the smallest value for $g(\\mathbf{x}+\\delta)$ is $(-1,+2,-3,+4,-5,+6)$. ///Answer: H"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} statement:?\n\nOptions:\nA. Zero-knowledge interactive proofs are interactive proofs.\nB. Black-box zero-knowledge interactive proofs are zero-knowledge interactive proofs.\nC. $\\Sigma$-protocols are zero-knowledge interactive proofs.\nD. $\\Sigma$-protocols are interactive proofs of knowledge.\n", "answer": "Explanations: The incorrect statement is: $\\Sigma$-protocols are zero-knowledge interactive proofs.\n\nExplanation: $\\Sigma$-protocols are interactive proofs of knowledge, not zero-knowledge interactive proofs. In a $\\Sigma$-protocol, a prover demonstrates knowledge of a secret value without revealing any information about the secret value itself. This is different from zero-knowledge interactive proofs, where the prover convinces the verifier of the truth of a statement without revealing any information beyond the truth of the statement. ///Answer: C"}
{"subject": "", "question": "Question: What is the mean squared error of $f$ for a sample, where $\\textbf{x}$ is an input, $y$ a target and $f(\\textbf{x},W)$ the mapping function  ? \n\t(One answer)?\n\nOptions:\nA.  $||y - f(\\textbf{x},W)||^2 $ \nB.  $||y - f(\\textbf{x},W)|| $\nC.  $-\\log(P(y=i | \\textbf{x})) = -\\log(\\frac{e^{\\textbf{f}_i(\\textbf{x},W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x},W)}})$ \nD.  $P(y=i |\\textbf{x}) = \\frac{e^{\\textbf{f}_i(\\textbf{x},W)}}{\\sum_j e^{\\textbf{f}_j(\\textbf{x},W)}}$ \n", "answer": "Explanations: The correct answer to the question \"What is the mean squared error of $f$ for a sample, where $\\textbf{x}$ is an input, $y$ a target and $f(\\textbf{x},W)$ the mapping function?\" is '$||y - f(\\textbf{x},W)||^2$'.\n\nExplanation on how to solve it:\nTo calculate the mean squared error, you first need to determine the predicted values generated by the mapping function $f(\\textbf{x},W)$ for the given input data $\\textbf{x}$ using the weights $W$. Once you have the predicted values, you can subtract them from the actual target values $y$ to find the differences. Next, square these differences to ensure they are all positive and then calculate the average of these squared differences. This average value is the mean squared error, which provides a measure of how well the mapping function is performing in predicting the outcomes based on the input data. By comparing this error metric across different mapping functions or tuning the weights $W, researchers can optimize the model for better predictive accuracy. ///Answer: A"}
{"subject": "", "question": "Question: CBC can deal with three types of IV. Tick the \\emph{false} answer.\n\nOptions:\nA. A random IV sent in clear.\nB. A secret random IV which is sent encrypted.\nC. A non secret constant IV.\nD. A secret IV which is part of the key.\n", "answer": "Explanations: The false answer is:\n\n4. A secret IV which is part of the key.\n\nExplanation:\n1. A random IV sent in clear: This is a valid use case for CBC mode. The IV is not required to be secret, but it should be unpredictable to ensure the security of the encryption.\n2. A secret random IV which is sent encrypted: This is also a valid use case for CBC mode. The IV can be kept secret and sent encrypted along with the ciphertext to provide an additional layer of security.\n3. A non-secret constant IV: This is not recommended for CBC mode. Reusing a constant IV can lead to security vulnerabilities, as it can leak information about the plaintext. It is crucial to use a unique and random IV for each encryption operation.\n4. A secret IV which is part of the key: This is not a valid practice for CBC mode. The IV should be separate from the key to provide randomness and ensure the security of the encryption. Using the IV as part of the key can weaken the encryption scheme and make it more vulnerable to attacks. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{true} statement.\n\nOptions:\nA. The Legendre Symbol is the generalization of the Jacobi Symbol.\nB. The complexity to compute $\\left ( \\frac{a}{n} \\right )$ is $\\mathcal{O}(\\ell^2)$, where $\\ell$ is the length of $a$ and $n$.\nC. If $\\left ( \\frac{a}{n} \\right )=1$, then $a$ is a quadratic residue in $\\mathbb{Z}_n^*$.\nD. For all $a$ and $b$ ($b$ odd) $\\left ( \\frac{a}{b} \\right )=0$ if and only if $b$ divides $a$.\n", "answer": "Explanations: The answer to the question is: 'If $\\left ( \\frac{a}{n} \\right )=1$, then $a$ is a quadratic residue in $\\mathbb{Z}_n^*$.' ///Answer: C"}
{"subject": "", "question": "Question: (FastText supervised Classifier) The FastText supervised classifier can be modeled as a one-hidden-layer neural network.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: True\n\nReasoning:\nFastText supervised classifier can be modeled as a one-hidden-layer neural network. The model architecture consists of an input layer, a hidden layer, and an output layer. The hidden layer is a linear transformation followed by a non-linear activation function (such as ReLU or Sigmoid). The output layer uses a softmax function to produce class probabilities. This architecture is similar to a one-hidden-layer neural network, making the statement true. ///Answer: A"}
{"subject": "", "question": "Question: In order to achieve a collision resistance equivalent to a $128$ bit key search, the size of the digest must be at least $\\ldots$?\n\nOptions:\nA. $128$ bits.\nB. $2^{256}$ bits.\nC. $256$ bits.\nD. $2^{128}$ bits.\n", "answer": "Explanations: In order to achieve a collision resistance equivalent to a $128$ bit key search, we need the size of the digest to be at least $2^{128}$ bits. This is because a $128$ bit key search means that there are $2^{128}$ possible keys, and in a collision-resistant hash function, the digest size should be at least as large as the number of possible outputs to ensure that finding a collision is computationally infeasible. ///Answer: D"}
{"subject": "", "question": "Question: In ElGamal signature scheme and over the random choice of the public parameters in the random oracle model (provided that the DLP is hard), existential forgery is \\ldots?\n\nOptions:\nA. \\ldots impossible.\nB. \\ldots hard on average.\nC. \\ldots easy on average.\nD. \\ldots easy.\n", "answer": "Explanations: 1. In the ElGamal signature scheme, the security is based on the hardness of the Discrete Logarithm Problem (DLP).\n2. In the random oracle model, the public parameters are chosen randomly.\n3. If the DLP is hard, then it is believed that it is computationally infeasible to forge a signature without the private key.\n\nBased on these points, we can conclude that in ElGamal signature scheme with random choice of public parameters in the random oracle model:\n\n- Existential forgery is impossible. ///Answer: A"}
{"subject": "", "question": "Question: Given the distribution $P_0$ of a normal coin, i.e. $P_0(0)=P_0(1)=\\frac{1}{2}$, and distribution $P_1$ of a biased coin, where $P_1(0)=\\frac{1}{3}$ and $P_1(1) = \\frac{2}{3}$ , the maximal advantage of a distinguisher using a single sample is\\dots?\n\nOptions:\nA. $\\frac{1}{6}$.\nB. $3$.\nC. $\\frac{1}{3}$.\nD. $0$.\n", "answer": "Explanations: The maximal advantage of a distinguisher using a single sample can be calculated by taking the absolute difference between the probabilities assigned to the outcome by the two distributions. \n\nFor outcome 0:\n$|P_0(0) - P_1(0)| = |0.5 - \\frac{1}{3}| = \\frac{1}{6}$\n\nFor outcome 1:\n$|P_0(1) - P_1(1)| = |0.5 - \\frac{2}{3}| = \\frac{1}{6}$\n\nThe maximal advantage is the maximum of these two differences, which is $\\frac{1}{6}$. \n\nTherefore, the correct option is 1) $\\frac{1}{6}$. ///Answer: A"}
{"subject": "", "question": "Question: Using salt for UNIX passwords \\ldots?\n\nOptions:\nA. allows to speed up the verification for the server.\nB. makes the protocol secure against computationally unbounded adversary.\nC. allows to transmit the password over a non confidential channel without compromising the security.\nD. helps preventing dictionary attacks.\n", "answer": "Explanations: Let's discuss the statement: \"Salt helps preventing dictionary attacks.\"\n\nThis statement is true. By adding a random salt to each password before hashing it, the resulting hashed password will be unique even if two users have the same password. This uniqueness prevents attackers from using precomputed dictionaries of hashed passwords to easily crack passwords. Essentially, salt adds a layer of complexity that makes it more difficult for attackers to perform dictionary attacks.\n\nDo you have any questions or would you like to discuss this further? ///Answer: D"}
{"subject": "", "question": "Question: Confidentiality means that:?\n\nOptions:\nA. the message can be read by anyone.\nB. information should not leak to any unexpected party.\nC. the message should make clear who the author is.\nD. the information must be protected against any malicious modification.\n", "answer": "Explanations: Confidentiality is a crucial aspect of information security that ensures that data is only accessed by authorized individuals and remains protected from unauthorized access. Let's analyze each option to determine the correct definition of confidentiality:\n\n1. **Option 1: the message can be read by anyone**\n   - This option does not align with the concept of confidentiality. If a message can be read by anyone, it indicates a lack of confidentiality, as the information is accessible to everyone, including unauthorized parties. Therefore, this option does not correctly define confidentiality.\n\n2. **Option 2: information should not leak to any unexpected party**\n   - This statement accurately describes confidentiality. Confidentiality ensures that sensitive information remains secure and does not leak to unauthorized or unintended parties. It involves restricting access to data only to those who are authorized to view or use it, thus preventing disclosure to unexpected or unauthorized individuals. This definition correctly represents the concept of confidentiality.\n\n3. **Option 3: the message should make clear who the author is**\n   - This statement is more aligned with authentication, which verifies the identity of the sender or author of a message to establish trust. Confidentiality focuses on ensuring that unauthorized parties cannot access the information, rather than identifying the sender. Therefore, this option does not capture the essence of confidentiality.\n\n4. **Option 4: the information must be protected against any malicious modification**\n   - This option describes the concept of integrity, which ensures that data remains accurate, complete, and unaltered. While protecting information against malicious modification is an essential security principle, it specifically pertains to integrity rather than confidentiality.\n\nBased on the definitions provided for each option, the correct definition of confidentiality, as it relates to information security, is:\n**Option 2: information should not leak to any unexpected party**\n\nThis option accurately captures the essence of confidentiality by emphasizing the protection of sensitive information from unauthorized disclosure or access. ///Answer: B"}
{"subject": "", "question": "Question: Due to the birthday paradox, a collision search in a hash function with $n$-bit output has complexity\\dots?\n\nOptions:\nA. $2^{\\sqrt{n}}$\nB. $\\sqrt{2^n}$\nC. $2^n$\nD. $2^{n-1}$\n", "answer": "Explanations: The correct answer is '$\\\\sqrt{2^n}$'. \n\nThe birthday paradox states that in a group of 23 people, there is a 50% chance that at least two people share the same birthday. This concept can be applied to hash functions as well. \n\nIn a hash function with $n$-bit output, there are $2^n$ possible outputs. When searching for a collision (two different inputs that produce the same output), we can use the birthday paradox to estimate the complexity. \n\nBy setting the number of possible outputs ($2^n$) equal to the number of trials needed to find a collision, we can solve for the number of trials. This gives us $\\sqrt{2^n}$ as the complexity of a collision search in a hash function with $n$-bit output. \n\nTherefore, the complexity of a collision search in a hash function with $n$-bit output is '$\\\\sqrt{2^n}$'. ///Answer: B"}
{"subject": "", "question": "Question: Which attribute gives the best split?A1PNa44b44A2PNx51y33A3PNt61j23?\n\nOptions:\nA. A1\nB. A3\nC. A2\nD. All the same\n", "answer": "Explanations: In this context, the attribute that gives the best split would be the one that separates the data into the most distinct groups. To determine this, we can look at the values following the attribute identifiers in each option:\n\n- A1: PNa44b44\n- A2: PNx51y33\n- A3: PNt61j23\n\nBy analyzing the values in each option, we can see that A1 and A2 have similar values, while A3 has distinct values. Therefore, A3 would likely provide the best split in this case as it separates the data into more distinct groups compared to A1 and A2.\n\nCorrect answer: A3 ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{true} assertion related to the ElGamal signature scheme.\n\nOptions:\nA. A signature of a given message is obtained in a deterministic way.\nB. The generator $g$ generates a subgroup of prime order.\nC. The signature verification procedure is probabilistic.\nD. The main computational cost of the signature generation is due to one modular exponentiation.\n", "answer": "Explanations: The correct answer is: The signature verification procedure is probabilistic because the ElGamal signature scheme involves random values during the signature generation process, making the verification procedure probabilistic.\n\nExplanation:\n- Option 1: \"A signature of a given message is obtained in a deterministic way.\" This statement is incorrect because the ElGamal signature scheme involves randomness in the signature generation process, making it non-deterministic.\n- Option 2: \"The generator $g$ generates a subgroup of prime order.\" This statement is not directly related to the ElGamal signature scheme. While it is true that the generator $g$ in the ElGamal encryption scheme generates a subgroup of prime order, this is not specific to the signature scheme.\n- Option 4: \"The main computational cost of the signature generation is due to one modular exponentiation.\" This statement is incorrect because the main computational cost of the ElGamal signature generation is actually due to multiple modular exponentiations, including one for generating the signature and one for hashing the message.\n\nTherefore, the correct choice is option 3 because the ElGamal signature verification procedure involves checking the validity of the signature with some probability due to the randomness introduced during the signature generation process. ///Answer: C"}
{"subject": "", "question": "Question: Consider the following shellcode, which of the following statement(s) is/are correct?\n\n\\begin{lstlisting}[language=nasm,style=nasm]\nneedle: jmp gofar\ngoback: pop %rdi\n    \txor %rax, %rax\n    \tmovb $0x3b, %al\n    \txor %rsi, %rsi\n    \txor %rdx, %rdx\n    \tsyscall\ngofar:  call goback\n.string \"/bin/sh\"\n\\end{lstlisting}?\n\nOptions:\nA. Lines 2-6 are preparing arguments for the syscall invocation.\nB. In the exploit payload, the string \"/bin/sh\" must end with a \"0x0\" byte to ensure it is terminated correctly.\nC. The purpose of line 8 is to push the address of \"/bin/sh\" to the stack and jump to line 2.\nD. Line 3 is not necessary.\n", "answer": "Explanations: {'Correct Answer(s)': [1, 2]}\n\nExplanation:\n1. Lines 2-6 are indeed preparing arguments for the syscall invocation. In this case, %rax is set to 0x3b which corresponds to the syscall number for execve, %rdi is loaded with the address of \"/bin/sh\", and %rsi and %rdx are cleared (set to zero) as they are not used in this syscall.\n2. In the exploit payload, the string \"/bin/sh\" must end with a \"0x0\" byte to ensure it is terminated correctly. This is necessary as strings in C are null-terminated, and the syscall expects a null-terminated string for the path argument.\n3. The purpose of line 8 is not to push the address of \"/bin/sh\" to the stack and jump to line 2. Line 8 is a call to the goback label which pops the address of \"/bin/sh\" into %rdi before executing the execve syscall.\n4. Line 3 is necessary as it sets %rax to zero before setting it to 0x3b in line 4. This ensures that %rax is properly initialized before use in the syscall. ///Answer: A"}
{"subject": "", "question": "Question: Why is natural language processing difficult?\nSelect all that apply.A penalty will be applied for wrong answers.\n\nOptions:\nA. Lack of linguistic competence and resources\nB. Impact of power laws\nC. High dimensionality and sparseness of data\nD. Subjectivity of annotators\n", "answer": "Explanations: Let's analyze each option to determine why it contributes to the difficulty of natural language processing:\n\n1. Lack of linguistic competence and resources:\n- Correct: Natural language processing requires an understanding of linguistic concepts, structures, and nuances. Lack of linguistic competence can make it challenging to accurately process and analyze text data.\n  \n2. Impact of power laws:\n- Correct: Power laws, such as Zipf's law that describes the frequency distribution of words in a language, have a significant impact on natural language processing. The skewed distribution of word frequencies can affect language models, information retrieval, and other NLP tasks.\n\n3. High dimensionality and sparseness of data:\n- Correct: Text data in natural language processing is often represented in high-dimensional spaces, especially when using techniques like word embeddings. Sparse data with many features can lead to computational challenges and require efficient algorithms to process effectively.\n\n4. Subjectivity of annotators:\n- Incorrect: While subjectivity of annotators can introduce challenges in tasks like sentiment analysis or annotation tasks, it is not a direct reason why natural language processing is inherently difficult. Subjectivity can be addressed through proper annotation guidelines and machine learning techniques.\n\nBased on the analysis, the correct options contributing to the difficulty of natural language processing are:\n- Lack of linguistic competence and resources\n- Impact of power laws\n- High dimensionality and sparseness of data ///Answer: B"}
{"subject": "", "question": "Question: Categorical Cross-Entropy loss:?\n\nOptions:\nA. Minimizing the distance between the predicted point and the true point\nB. Maximizing the probability of the correct class\nC. Minimizing the score of false classes when they are close, or bigger than, the score of the true class\nD. Maximizing the accuracy\n", "answer": "Explanations: The correct answer from the provided options for the question on Categorical Cross-Entropy loss is:\n\n- 'Minimizing the score of false classes when they are close, or bigger than, the score of the true class'\n\nThis statement accurately reflects the purpose of Categorical Cross-Entropy loss in penalizing the model when false classes have high probabilities compared to the true class. It aims to minimize the error in predicting the probabilities of all classes, particularly penalizing the model when it assigns high probabilities to incorrect classes.\n\nThe other statements in the question are not entirely accurate in describing the role of Categorical Cross-Entropy loss:\n\n- 'Minimizing the distance between the predicted point and the true point': This statement is not directly related to Categorical Cross-Entropy loss, as the loss function focuses on comparing predicted probabilities to true probabilities rather than the distance between individual points.\n\n- 'Maximizing the probability of the correct class': This statement is not correct because the goal of Categorical Cross-Entropy loss is to minimize the error in predicting the probabilities of all classes, rather than just maximizing the probability of the correct class.\n\n- 'Maximizing the accuracy': While maximizing accuracy is a common objective in classification tasks, Categorical Cross-Entropy loss is not directly focused on maximizing accuracy but rather on optimizing the predicted probabilities of the classes.\n\nTherefore, the correct answer is 'Minimizing the score of false classes when they are close, or bigger than, the score of the true class'. ///Answer: C"}
{"subject": "", "question": "Question: Let $(G,+)$ be a group of order $n$. If $g$ is a generator of this group, then it has order\\dots?\n\nOptions:\nA. $n/2$\nB. $\\sqrt{n}$\nC. $n-1$\nD. $n$\n", "answer": "Explanations: The answer is: D, then the reasoning\n\nIn a group of order $n$, the order of any element must divide the order of the group by Lagrange's theorem. Since $g$ is a generator of the group, its order must be equal to the order of the group, which is $n$. Therefore, the order of $g$ is $n$. ///Answer: D"}
{"subject": "", "question": "Question: Identify the correct statement.\n\nOptions:\nA. None of the other options are correct.\nB. After training, and when the size of the vocabulary is large, a Skip-gram model would have higher space requirements than a GloVe model. We assume both models have the same number of dimensions (features), vocabulary, and are trained on the same corpus.\nC. Language models can be trained using either a multi-class(number of classes equal to the vocabulary size) classifier or a binary classifier to generate text.\nD. Language Models are useless for classification tasks in Natural Language Processing as they are only suited for text generation.\n", "answer": "Explanations: The correct statement is: \"After training, and when the size of the vocabulary is large, a Skip-gram model would have higher space requirements than a GloVe model. We assume both models have the same number of dimensions (features), vocabulary, and are trained on the same corpus.\"\n\nExplanation: Skip-gram and GloVe are both popular algorithms used for word embedding in natural language processing. Skip-gram is a type of neural network language model that learns to predict the context words given a target word, while GloVe (Global Vectors for Word Representation) is a count-based model that learns word vectors based on co-occurrence statistics. \n\nIn terms of space requirements, Skip-gram typically requires larger space compared to GloVe, especially when the size of the vocabulary is large. This is because Skip-gram stores individual word vectors separately, while GloVe uses a co-occurrence matrix which can be more space-efficient for large vocabularies. \n\nTherefore, the statement accurately reflects the difference in space requirements between Skip-gram and GloVe models when trained on the same corpus with the same number of dimensions and vocabulary size. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion about Diffie and Hellman.\n\nOptions:\nA. They wrote an article entitled ``\\emph{New directions in Cryptography}'' in 1976.\nB. They introduced the notion of ``\\emph{trapdoor permutation}''.\nC. They proposed a key agreement protocol.\nD. They invented RSA.\n", "answer": "Explanations: D. They invented RSA. This is the correct answer because RSA (Rivest-Shamir-Adleman) was actually invented by Ron Rivest, Adi Shamir, and Leonard Adleman in 1977, a year after Diffie and Hellman published their groundbreaking paper on public-key cryptography. Diffie and Hellman are known for their contributions to the development of public-key cryptography, including proposing the key agreement protocol and introducing the concept of a \"trapdoor permutation\", but they did not invent RSA. ///Answer: D"}
{"subject": "", "question": "Question: Recall that the hard-margin SVM problem corresponds to:\n\t$$ \\underset{\\substack{\\ww \\in \\R^d, \\ \forall i:\\ y_i \\ww^\top \\xx_i \\geq 1}}{\\min} \\Vert \\ww \\Vert_2.$$\n\tNow consider the $2$-dimensional classification dataset corresponding to the $3$ following datapoints: $\\xx_1 = (-1, 2)$, $\\xx_2 = (1, 2)$, $\\xx_3 = (0, -2)$ and  $y_1 = y_2 = 1$, $y_3 = -1$.\n\tWhich of the following statements is \textbf{true}:\n        ?\n\nOptions:\nA. Our dataset is not linearly separable and hence it does not make sense to consider the hard-margin problem.\nB. There exists a unique $\\ww^\\star$ which linearly separates our dataset.\nC. The unique vector which solves the hard-margin problem for our dataset is $\\ww^\\star = (0, 1)$.\nD. None of the other statements are true.\n", "answer": "Explanations: The given dataset is indeed linearly separable in 2-dimensional space. To determine this, we can visualize the dataset by plotting the points in a 2D plane. The points $\\xx_1 = (-1, 2)$ and $\\xx_2 = (1, 2)$ have the same label $y = 1$, while $\\xx_3 = (0, -2)$ has label $y = -1$. This means that the dataset can be separated by a line passing between the points of different classes.\n\nTo find the separating hyperplane for the dataset, we need to solve the hard-margin SVM problem. The hard-margin SVM problem aims to find the optimal hyperplane that maximally separates the classes with a margin of 1. The decision boundary is defined by $\\ww \\cdot \\xx + b = 0$, where $\\ww$ is the weight vector perpendicular to the decision boundary.\n\nGiven the dataset $\\xx_1 = (-1, 2)$, $\\xx_2 = (1, 2)$, $\\xx_3 = (0, -2)$ and labels $y_1 = y_2 = 1$, $y_3 = -1$, we can determine the weight vector $\\ww^\\star$ that solves the hard-margin problem.\n\nWhen solving the hard-margin problem for this dataset, we find that the weight vector $\\ww^\\star$ is not unique. There can be multiple weight vectors that correctly classify the dataset. In this case, the weight vector $\\ww^\\star = (0, 1)$ is one possible solution that separates the classes with a margin of 1. Other weight vectors that are parallel to this vector would also correctly classify the dataset.\n\nTherefore, the statement \"None of the other statements are true\" is the correct answer. The dataset is linearly separable, there are multiple weight vectors that separate the classes, and $\\ww^\\star = (0, 1)$ is one such weight vector that solves the hard-margin problem for this dataset. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following primitives \\textit{cannot} be instantiated with a cryptographic hash function?\n\nOptions:\nA. A pseudo-random number generator.\nB. A commitment scheme.\nC. A public key encryption scheme.\nD. A key-derivation function.\n", "answer": "Explanations: The answer is: C\n\nChain of thought reasoning:\nAs an academic researcher, it is important to understand the cryptographic properties and limitations of hash functions. Let's examine the options provided:\n\nA. A pseudo-random number generator: A cryptographic hash function can be used to instantiate a pseudo-random number generator by hashing an initial seed value to generate a sequence of pseudo-random numbers. Therefore, a PRNG can be instantiated with a cryptographic hash function.\n\nB. A commitment scheme: A commitment scheme involves committing to a value without revealing it until later. This can be achieved using a cryptographic hash function by hashing the message along with a random value to create a commitment. Therefore, a commitment scheme can be instantiated with a cryptographic hash function.\n\nC. A public key encryption scheme: Public key encryption schemes require a public key for encryption and a corresponding private key for decryption. Cryptographic hash functions do not involve key pairs, so they cannot be directly used to instantiate a public key encryption scheme. Therefore, a public key encryption scheme cannot be instantiated with a cryptographic hash function.\n\nD. A key-derivation function: A key-derivation function is used to derive one or more secret keys from a fixed-length input. This can be achieved using a cryptographic hash function by hashing the input and possibly some additional parameters. Therefore, a key-derivation function can be instantiated with a cryptographic hash function.\n\nBased on this analysis, the correct answer is C. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following is false regarding K-means and DBSCAN?\n\nOptions:\nA. K-means does not handle outliers, while DBSCAN does\nB. K-means takes the number of clusters as parameter, while DBSCAN does not take any parameter\nC. K-means does many iterations, while DBSCAN does not\nD. Both are unsupervised\n", "answer": "Explanations: ##Answer:\nThe false statement regarding K-means and DBSCAN is:\n- K-means does not handle outliers, while DBSCAN does\n\n##Reasons:\n1. **K-means and Outliers:** K-means is sensitive to outliers because it aims to minimize the sum of squared distances between data points and their assigned cluster centroids. Outliers can significantly affect the centroids' position and the clustering results in K-means. On the other hand, DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is robust to outliers as it identifies clusters based on density connectivity and classifies outliers as noise points.\n\n2. **Parameter Requirement:** K-means requires the number of clusters (k) as a parameter input since it partitions the data into k clusters. In contrast, DBSCAN does not require the number of clusters as a parameter. Instead, DBSCAN defines clusters based on density and distance parameters, such as epsilon (\u03b5) and minimum points (MinPts).\n\n3. **Iterations:** K-means algorithm involves an iterative process where data points are assigned to clusters and centroids are updated until convergence criteria are met. This iterative nature of K-means helps to optimize the clustering results. On the other hand, DBSCAN does not require many iterations as it directly identifies core points, border points, and noise points based on the defined parameters.\n\n4. **Unsupervised Nature:** Both K-means and DBSCAN are unsupervised clustering algorithms, meaning they do not require labeled data for training. They aim to group data points based on their similarity or density into distinct clusters without external guidance.\n\nIn summary, the false statement is that K-means does not handle outliers, while DBSCAN does. In reality, K-means is sensitive to outliers, while DBSCAN is designed to handle outliers effectively through its density-based clustering approach. ///Answer: A"}
{"subject": "", "question": "Question: We represent $GF(2^8)$ as $\\mathbb{Z}_2[X]/P(X)$ where $P(X) = X^8 + X^4+X^3+X+1$. Then, $(X^7+X^6)\\times (X + 1)=$\\dots?\n\nOptions:\nA. $X^6+X^5+X^4+X^3+X$.\nB. $X^6 + X^4 + X^3 + X + 1$.\nC. $X^6$.\nD. $X^7+X^6+X^4+X^3+X+1$.\n", "answer": "Explanations: To find the product $(X^7+X^6) \\times (X + 1)$ in $GF(2^8)$ represented as $\\mathbb{Z}_2[X]/P(X)$ where $P(X) = X^8 + X^4 + X^3 + X + 1$, we perform polynomial multiplication and reduction modulo $P(X)$ to simplify the result.\n\nFirst, we multiply $(X^7+X^6) \\times (X + 1)$:\n$(X^7+X^6) \\times (X + 1) = X^8 + X^7 + X^7 + X^6$.\n\nNext, we reduce the result modulo $P(X)$:\n$X^8 + X^7 + X^7 + X^6 = X^8 + X + X^6 = X + X^6$.\n\nTherefore, the correct answer is: $X + X^6$. ///Answer: B"}
{"subject": "", "question": "Question: The k-means algorithm for clustering is guaranteed to converge to a local optimum.\n\nOptions:\nA. TRUE\nB. FALSE\n", "answer": "Explanations: The statement \"The k-means algorithm for clustering is guaranteed to converge to a local optimum\" is TRUE.\n\nExplanation:\n- The k-means algorithm is a popular method used in clustering, which is a type of unsupervised machine learning technique.\n- In k-means, the algorithm aims to partition a set of data points into k clusters in such a way that each data point belongs to the cluster with the nearest mean.\n- The algorithm iteratively assigns data points to clusters and updates the cluster centroids until a stopping criterion is met.\n- Although k-means may not always converge to the global optimum (the best solution overall), it is guaranteed to converge to a local optimum (a good solution within a certain region of the solution space).\n- The final clustering result obtained by k-means depends on the initial placement of cluster centroids, and different initializations may lead to different local optima.\n\nTherefore, the statement that the k-means algorithm for clustering is guaranteed to converge to a local optimum is TRUE. ///Answer: A"}
{"subject": "", "question": "Question: In Text Representation learning, which of the following statements are correct?\n\nOptions:\nA. Learning GloVe word vectors can be done using the singular value decomposition, if the $f_{d n}$ weights are set to 1 for all observed entries.\nB. The skip-gram model for learning original word2vec embeddings does learn a binary classifier for each word.\nC. FastText as discussed in the course learns word vectors and sentence representations which are specific to a supervised classification task.\nD. Logistic regression used for text classification is faster at test time when using word vectors as opposed to bag-of-word representation of the input.\n", "answer": "Explanations: First, let's analyze each statement:\n\nOption 1: Learning GloVe word vectors can be done using the singular value decomposition, if the $f_{d n}$ weights are set to 1 for all observed entries.\nThis statement is incorrect. While GloVe word vectors are indeed based on matrix factorization using singular value decomposition, the $f_{d n}$ weights are not set to 1 for all observed entries. The weights are determined based on the co-occurrence statistics of the words in the corpus.\n\nOption 2: The skip-gram model for learning original word2vec embeddings does learn a binary classifier for each word.\nThis statement is correct. The skip-gram model, a variant of the original word2vec model, does learn a binary classifier for each word to predict its context words.\n\nOption 3: FastText as discussed in the course learns word vectors and sentence representations which are specific to a supervised classification task.\nThis statement is incorrect. FastText, while being able to learn word vectors, is not specifically tailored for supervised classification tasks. It is a model that can be used for various NLP tasks, including text classification, but it is not limited to supervised classification.\n\nOption 4: Logistic regression used for text classification is faster at test time when using word vectors as opposed to bag-of-word representation of the input.\nThis statement is correct. Using word vectors for text classification can often lead to faster test times compared to bag-of-word representations because word vectors capture more semantic information about words and can potentially improve the performance of models like logistic regression.\n\nTherefore, the correct statements from the options provided are:\n- Option 2: The skip-gram model for learning original word2vec embeddings does learn a binary classifier for each word.\n- Option 4: Logistic regression used for text classification is faster at test time when using word vectors as opposed to bag-of-word representation of the input. ///Answer: B"}
{"subject": "", "question": "Question: You are given the following collection of premises:\nIf I go to the museum, it either rains or snows.\nI went to the museum on Saturday or I went to the museum on Sunday.\nIt did not rain and it did not snow on Saturday.\nIt did not rain on Sunday.\nWhich conclusions can be drawn from these premises ? (more than one answer can be correct)?\n\nOptions:\nA. I went to the museum on Sunday.\nB. It snowed on Sunday.\nC. I went to the museum on Saturday.\nD. It was warm on Saturday.\n", "answer": "Explanations: Based on the given premises, we can draw the following conclusions:\n\n1. I went to the museum on Sunday.\n- This conclusion can be drawn from the second premise: \"I went to the museum on Saturday or I went to the museum on Sunday.\"\n\n2. It was warm on Saturday.\n- This conclusion can be inferred from the fact that it did not rain or snow on Saturday. If there was no rain or snow, it can be assumed that it was a warm day.\n\nTherefore, the correct conclusions are:\n- I went to the museum on Sunday.\n- It was warm on Saturday. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is true in the context of inverted files?\n\nOptions:\nA. Index merging compresses an inverted file index on disk and reduces the storage cost\nB. The trie structure used for index construction is also used as a data access structure to terms in the vocabulary\nC. The finer the addressing granularity used in documents, the smaller the posting file becomes\nD. Inverted files are optimized for supporting search on dynamic text collections\n", "answer": "Explanations: The correct statement in the context of inverted files is:\n\n\"The trie structure used for index construction is also used as a data access structure to terms in the vocabulary.\"\n\nInverted files use a trie data structure for index construction. A trie is a tree data structure used to store and search for keys in a dataset. In the context of inverted files, the trie structure is used to organize and efficiently search for terms in the vocabulary. This structure allows for fast access to terms and their corresponding postings in the inverted index, making it a crucial component of inverted file systems. ///Answer: B"}
{"subject": "", "question": "Question: What is the \\textbf{minimal} requirement to make the Vigen\\`ere cipher perfectly secure.\n\nOptions:\nA. Vigen\\`ere is always perfectly secure.\nB. Changing the key after each encryption is enough.\nC. Having a key that has the same length as the plaintext is enough.\nD. We need to use a uniformly distributed fresh key at each encryption that has the same length as the plaintext.\n", "answer": "Explanations: The correct statement is: \"We need to use a uniformly distributed fresh key at each encryption that has the same length as the plaintext.\"\n\nJustification: The Vigen\u00e8re cipher can be made perfectly secure by using a uniformly distributed fresh key at each encryption that has the same length as the plaintext. This ensures that the key remains unpredictable and does not repeat, making it much harder for an attacker to crack the cipher. Changing the key after each encryption and having a key that has the same length as the plaintext are not sufficient to make the Vigen\u00e8re cipher perfectly secure. ///Answer: D"}
{"subject": "", "question": "Question: Given graph 1\u21922, 1\u21923, 2\u21923, 3\u21922, switching from Page Rank to Teleporting\u00a0PageRank will have an influence on the value(s) of:?\n\nOptions:\nA. All the nodes\nB. Node 1\nC. Node 2 and 3\nD. No nodes. The values will stay unchanged.\n", "answer": "Explanations: When transitioning from PageRank to Teleporting PageRank in the provided graph with edges 1\u21922, 1\u21923, 2\u21923, and 3\u21922, the introduction of teleportation probabilities will impact the values of all nodes. Here is a detailed analysis:\n\n1. **PageRank**:\n   - In the traditional PageRank algorithm, the probability of moving from one node to another is solely based on the graph structure, determined by the outgoing links of nodes.\n   - PageRank distributes the probability evenly among outgoing links, influencing the ranking of nodes based on the link structure of the graph.\n\n2. **Teleporting PageRank**:\n   - Teleporting PageRank introduces teleportation probabilities, allowing for randomly jumping from one node to another in the graph with a certain probability.\n   - These teleportation probabilities redistribute the flow of probability in the graph, influencing how the ranking values are calculated for each node.\n\n3. **Impact on Node Values**:\n   - When transitioning to Teleporting PageRank from traditional PageRank, the teleportation probabilities will affect the probability flow and ranking values of all nodes in the graph.\n   - Node 1, Node 2, and Node 3 will experience changes in their ranking values due to the introduction of teleportation probabilities, which alter the probability flow dynamics in the graph.\n\n4. **Conclusion**:\n   - As a result of the switch from PageRank to Teleporting PageRank, the values of all nodes in the graph will be influenced. The teleportation probabilities introduced in Teleporting PageRank will impact the flow of probability and, consequently, the ranking values assigned to each node.\n\nTherefore, the logical conclusion is that transitioning from PageRank to Teleporting PageRank will have an influence on the values of **all the nodes** in the graph. ///Answer: A"}
{"subject": "", "question": "Question: If A={a} and B={b}, select all strings that belongs to\u00a0(A \u2297 B)+\nA penalty will be applied for any wrong answers selected.\n\nOptions:\nA. (aaa,bbb)\nB. (a,b)\nC. (aaaaa,bbbb)\nD. (a,bb)\nE. (aaa,bb)\nF. (aa,bbb)\n", "answer": "Explanations: To determine which strings belong to the set (A \u2297 B)+, we first need to understand what the operation \u2297 represents. In this case, \u2297 denotes the concatenation operation, where two strings are combined end-to-end.\n\nGiven that A={a} and B={b}, the set (A \u2297 B)+ represents the set of all strings that can be formed by concatenating strings from A and B one or more times.\n\nLet's break down the options:\n\n1. (aaa,bbb): This string is not a valid concatenation of strings from A and B, as it contains multiple 'a's and 'b's. Therefore, it does not belong to (A \u2297 B)+.\n\n2. (a,b): This string is a valid concatenation of strings from A and B. Since it contains exactly one 'a' and one 'b', it belongs to (A \u2297 B)+.\n\n3. (aaaaa,bbbb): Similar to option 1, this string contains multiple 'a's and 'b's, so it does not belong to (A \u2297 B)+.\n\n4. (a,bb): This string is a valid concatenation of strings from A and B. It contains one 'a' and two 'b's, so it belongs to (A \u2297 B)+.\n\n5. (aaa,bb): This string is not a valid concatenation as it contains multiple 'a's and 'b's, so it does not belong to (A \u2297 B)+.\n\n6. (aa,bbb): This string is a valid concatenation of strings from A and B. It contains two 'a's and three 'b's, so it belongs to (A \u2297 B)+.\n\nTherefore, the strings that belong to (A \u2297 B)+ are options 2 (a,b) and 4 (a,bb). Options 1, 3, 5, and 6 do not belong to (A \u2297 B)+. ///Answer: B"}
{"subject": "", "question": "Question: We want to generate a $\\ell$-bit prime. The complexity is roughly\\dots?\n\nOptions:\nA. $\\ell^2$\nB. $\\ell^3$\nC. $\\ell^4$\nD. $\\ell^5$\n", "answer": "Explanations: Generating an $\\ell$-bit prime number involves finding a prime number within the range of $2^{\\ell-1}$ to $2^{\\ell} - 1$. The process of generating primes of a specific bit length involves probabilistic primality testing algorithms like the Miller-Rabin test or deterministic algorithms like the AKS primality test.\n\nThe complexity of generating a prime number of $\\ell$ bits can be approximated as $\\ell^2$ due to the iterative nature of primality testing algorithms and the magnitude of the numbers involved in the process.\n\nThe $\\ell^2$ complexity arises from the fact that primality testing algorithms typically involve multiple iterations or checks for each candidate number to determine its primality status. As the size of the number (and thus the number of bits) increases, the number of iterations or operations required also grows quadratically with the bit length.\n\nTherefore, the complexity of generating an $\\ell$-bit prime number is roughly $\\ell^2$. ///Answer: A"}
{"subject": "", "question": "Question: A page that points to all other pages but is not pointed by any other page would have...\n\nOptions:\nA. Zero hub\nB. Nonzero pagerank\nC. Nonzero authority\nD. None of the above\n", "answer": "Explanations: In this question, we are dealing with the concepts of \"hub\" and \"authority\" in the context of the PageRank algorithm used by search engines like Google.\n\n1. \\textbf{Zero hub}: A page that points to all other pages but is not pointed to by any other page would be considered a hub in the context of the PageRank algorithm. A hub is a page that has outgoing links to other pages but does not necessarily receive incoming links. So, this option is incorrect.\n\n2. \\textbf{Nonzero pagerank}: The PageRank algorithm assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of measuring its relative importance within the set. A page that points to all other pages but is not pointed to by any other page would still have a nonzero PageRank because it is considered important due to its outgoing links. Therefore, this option is correct.\n\n3. \\textbf{Nonzero authority}: In the context of the HITS algorithm (Hypertext Induced Topic Selection), authority refers to pages that are linked to by many hubs. Since the page in question is not pointed to by any other page, it would not be considered an authority. Thus, this option is incorrect.\n\nTherefore, the correct answer is \\textbf{Nonzero pagerank}. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. The Advantage can be computed \\ldots?\n\nOptions:\nA. \\ldots with a differential characteristic.\nB. \\ldots as a distance.\nC. \\ldots with a frequentist approach.\nD. \\ldots with a Bayesian approach.\n", "answer": "Explanations: The false assertion is:\n\noption 3: ... with a frequentist approach.\n\nThe Advantage in cryptography is commonly computed using differential and linear characteristics, which are techniques often associated with a deterministic approach rather than a frequentist approach. \n\nOptions 1, 2, and 4 are true:\n- Option 1: The Advantage can be computed with a differential characteristic.\n- Option 2: The Advantage can be computed as a distance.\n- Option 4: The Advantage can be computed with a Bayesian approach. ///Answer: C"}
{"subject": "", "question": "Question: Which is true?\n\nOptions:\nA. Distant supervision requires rules for bootstrapping\nB. Classifiers produced with distant supervision are more precise than rules\nC. Distant supervision can help to detect rules\n", "answer": "Explanations: The correct option is: 3. Distant supervision can help to detect rules\n\nDistant supervision is a method in machine learning where training data is automatically labeled based on existing knowledge bases or heuristics. This can help to identify patterns or rules that can be used to improve the performance of classifiers. However, it does not necessarily guarantee that the classifiers produced with distant supervision will be more precise than manually crafted rules. ///Answer: C"}
{"subject": "", "question": "Question: For two $\\ell$-bit numbers, the \\textbf{tightest} complexity to compute their multiplication is?\n\nOptions:\nA. $\\mathcal{O}(\\ell)$.\nB. $\\mathcal{O}(\\sqrt{\\ell})$.\nC. $\\mathcal{O}(\\ell^2)$.\nD. $\\mathcal{O}(\\ell^2 \\log{\\ell})$.\n", "answer": "Explanations: $\\mathcal{O}(\\ell^2)$ Justification: The tightest complexity to compute the multiplication of two $\\ell$-bit numbers is $\\mathcal{O}(\\ell^2)$ because when multiplying two numbers, each bit in one number needs to be multiplied by each bit in the other number, resulting in a complexity of $\\ell \\times \\ell = \\ell^2$. ///Answer: C"}
{"subject": "", "question": "Question: Consider the document: \u201cInformation retrieval is the task of finding the documents satisfying the information needs of the user\u201d\nUsing MLE to estimate the unigram probability model, what is P(the|Md) and P(information|Md)?\n\nOptions:\nA. 1/16 and 1/16\nB. 1/12 and 1/12\nC. 1/4 and 1/8\nD. 1/3 and 1/6\n", "answer": "Explanations: To estimate the unigram probability model using Maximum Likelihood Estimation (MLE), we need to calculate the probability of each word occurring in the document. The unigram model assumes that each word is independent of the others, so we can calculate the probability of each word individually.\n\nGiven the document: \"Information retrieval is the task of finding the documents satisfying the information needs of the user\"\n\nLet's calculate the probabilities of the words \"the\" and \"information\" occurring in the document:\n\n1. Calculate the total number of words (N) in the document:\nN = 17\n\n2. Calculate the frequency of the word \"the\" in the document:\nfreq(\"the\") = 2\n\n3. Calculate the frequency of the word \"information\" in the document:\nfreq(\"information\") = 1\n\n4. Calculate the probability of the word \"the\" occurring in the document (P(the|Md)):\nP(the|Md) = freq(\"the\") / N\nP(the|Md) = 2 / 17\nP(the|Md) \u2248 0.1176\n\n5. Calculate the probability of the word \"information\" occurring in the document (P(information|Md)):\nP(information|Md) = freq(\"information\") / N\nP(information|Md) = 1 / 17\nP(information|Md) \u2248 0.0588\n\nTherefore, the estimated probabilities using MLE for the words \"the\" and \"information\" in the given document are approximately 0.1176 and 0.0588, respectively. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following is an acceptable commitment scheme, i.e., one that verifies the hiding and binding property (for a well chosen primitive and suitable $x$ and $r$):?\n\nOptions:\nA. $Commit(x;r) = Enc_r(x)$, where $Enc_r$ is a symmetric encryption scheme with key $r$.\nB. $Commit(x;r) = H(x)$, where $H$ is a hash function.\nC. $Commit(x;r) = x \\oplus r$, where $\\oplus$ is the bitwise xor operation.\nD. $Commit(x;r) = H(r\\|x)$, where $H$ is a hash function and $\\|$ denotes the concatenation.\n", "answer": "Explanations: Let's break down each option considering the security properties and the requirements of a commitment scheme in a different way:\n\n1. $Commit(x;r) = Enc_r(x)$, where $Enc_r$ is a symmetric encryption scheme with key $r$:\nThis option is not suitable for a commitment scheme because encryption is reversible with the key. This means that an attacker could potentially recover the committed value by decrypting the commitment using the key $r, violating the hiding property of commitments.\n\n2. $Commit(x;r) = H(x)$, where $H$ is a hash function:\nHash functions are commonly used in commitment schemes due to their one-way property. This option could be a valid commitment scheme as long as the hash function used is collision-resistant. It provides a way to commit to a value without revealing it, thus satisfying the requirements of a commitment scheme.\n\n3. $Commit(x;r) = x \\oplus r$, where $\\oplus$ is the bitwise XOR operation:\nThis option is not secure for a commitment scheme because the XOR operation is not hiding. An attacker with the commitment and the key $r$ could easily determine the committed value by XORing with $r$, violating the hiding property of commitments.\n\n4. $Commit(x;r) = H(r\\|x)$, where $H$ is a hash function and $\\|$ denotes the concatenation:\nThis option seems secure at first glance since it uses a hash function for committing. However, the mistake here lies in the use of concatenation $r\\|x$. This could potentially lead to a violation of the hiding property if an attacker can find a collision by trying different values of $x$ until they produce the same hash output. This flaw undermines the security of the commitment scheme.\n\nIn conclusion, option 2 ($Commit(x;r) = H(x)$) is the most suitable for a commitment scheme out of the given options, as it leverages the one-way property of hash functions to provide hiding and binding properties required for a secure commitment scheme. ///Answer: B"}
{"subject": "", "question": "Question: The training loss of logistic regression is always zero.\n\nOptions:\nA. TRUE\nB. FALSE\n", "answer": "Explanations: The correct answer to the question \"The training loss of logistic regression is always zero\" is FALSE. \n\nI chose this answer because, as explained in the analysis, it is highly unlikely for the training loss of logistic regression to be exactly zero in practice. The training process involves finding the best parameters to minimize the error between the predicted probabilities and the actual outcomes in the training data, and there will always be some noise or uncertainty in the data that the model cannot perfectly capture. Therefore, the training loss will typically approach zero but will not be exactly zero.\n\nThe answer TRUE is incorrect because, as discussed, achieving a training loss of exactly zero is not expected due to the limitations and complexities of real-world data. ///Answer: B"}
{"subject": "", "question": "Question: Which one of the following notions means that ``the information should make clear who the author of it is''?\n\nOptions:\nA. authentication\nB. steganograhy\nC. privacy\nD. confidentiality\n", "answer": "Explanations: ##Answer:\nAuthentication\n\n##Reasons:\n1. Authentication refers to the process of verifying the identity of a user or system. In the context of authorship identification, authentication ensures that the information clearly indicates who the author is.\n2. When information is authenticated, it means that there are mechanisms in place to confirm the author's identity, such as digital signatures, biometric authentication, or login credentials.\n3. In contrast, steganography is the practice of concealing messages within other messages to hide their existence, while privacy and confidentiality are related to protecting information from unauthorized access or disclosure. ///Answer: A"}
{"subject": "", "question": "Question: The feature ``deck structure type'' can have the following values: \n    Cast-in-place Concrete,\n    Concrete Precast Panel,\n    Open Grating,\n    Closed Grating,\n    Steel plate,\n    Corrugated Steel,\n    Aluminum and\n    Timber.\n\tFor logistic regression, what is the best encoding for these values? (One or multiple answers)?\n\nOptions:\nA. assign an integer to each option\nB. one-hot encoding\nC. polynomial encoding\nD. logistic regression does not require an encoding\n", "answer": "Explanations: For logistic regression, the best encoding for categorical variables like \"deck structure type\" from the options you provided would be **one-hot encoding** (Option 1).\n\nOne-hot encoding is commonly used for logistic regression when dealing with categorical variables with multiple levels. It creates binary columns for each category, where a \"1\" indicates the presence of that category and \"0\" indicates the absence. This encoding helps the logistic regression model to interpret the categorical variable correctly without introducing any ordinal relationship between the categories.\n\nTherefore, option 1: one-hot encoding is the most suitable for encoding the \"deck structure type\" feature in logistic regression analysis. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following apply to recent Android-based mobile systems but not\n    to Linux-based desktop systems?\n\nOptions:\nA. Apps should use the binder interface to communicate with other\n            apps.\nB. By default, each app runs as its own user.\nC. All apps run in a strict container with only limited system\n            calls available.\nD. Arbitrary apps can exchange files through shared\n            directories.\n", "answer": "Explanations: [ ] Apps should use the binder interface to communicate with other apps.\n[X] By default, each app runs as its own user.\n[X] All apps run in a strict container with only limited system calls available.\n[ ] Arbitrary apps can exchange files through shared directories.\n\nThe binder interface is used for inter-process communication in Android systems, but not specific to Android only. The other three statements are specific to recent Android-based mobile systems. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements regarding topic models is false?\n\nOptions:\nA. Topic models map documents to dense vectors\nB. In LDA, topics are modeled as distributions over documents\nC. LDA assumes that each document is generated from a mixture of topics with a probability distribution\nD. Topics can serve as features for document classification\n", "answer": "Explanations: 1. Topic models map documents to dense vectors - This statement is true. Topic models like Latent Dirichlet Allocation (LDA) map documents to dense vectors based on the distribution of topics within the document.\n\n2. In LDA, topics are modeled as distributions over documents - This statement is true. In LDA, topics are indeed modeled as distributions over documents, meaning that each topic is represented as a probability distribution over words in the document.\n\n3. LDA assumes that each document is generated from a mixture of topics with a probability distribution - This statement is also true. LDA assumes that each document is generated from a mixture of topics with a probability distribution, which is a key aspect of how the model works.\n\n4. Topics can serve as features for document classification - This statement is true. Topics extracted from topic models can indeed serve as features for document classification tasks, as they represent the underlying themes or concepts present in the document.\n\nTherefore, the false statement regarding topic models is not present in the options provided. ///Answer: None"}
{"subject": "", "question": "Question: Tick the \\textbf{correct} assertion.\n\nOptions:\nA. Forward secrecy means that an email should still be encrypted when we forward it to someone else.\nB. Forward secrecy means that the communication will remain secure even when the long term secrets keys are disclosed.\nC. Semi-static Diffie-Hellman provides forward secrecy.\nD. Static Diffie-Hellman provides forward secrecy.\n", "answer": "Explanations: The correct assertion is:\n2. Forward secrecy means that the communication will remain secure even when the long-term secret keys are disclosed.\n\nForward secrecy, also known as perfect forward secrecy (PFS), is a property of cryptographic systems where the compromise of a long-term secret key does not compromise past session keys. This ensures that even if an attacker gains access to the long-term secret keys in the future, they cannot decrypt past communications that were encrypted using session keys. This property enhances the security of communication systems. ///Answer: B"}
{"subject": "", "question": "Question: Why is natural language processing difficult?Select all that apply.You will get a penalty for wrong answers.\n\nOptions:\nA. Lack of linguistic competence and resources\nB. Impact of power laws\nC. High dimensionality and sparseness of data\nD. Subjectivity of annotators\n", "answer": "Explanations: The reasons why natural language processing (NLP) is difficult include:\n\n1. Lack of linguistic competence and resources: Correct. This is a challenge in NLP as language is complex and can vary in structure and usage.\n\n2. Impact of power laws: Correct. Power laws can lead to imbalanced datasets or distributions, which can affect model performance.\n\n3. High dimensionality and sparseness of data: Correct. NLP tasks often deal with high-dimensional and sparse data, making it challenging to extract meaningful patterns.\n\n4. Subjectivity of annotators: Correct. Annotators may have different interpretations or biases when labeling data, leading to inconsistencies in annotations.\n\nTherefore, the correct options are:\n- Lack of linguistic competence and resources\n- Impact of power laws\n- High dimensionality and sparseness of data\n- Subjectivity of annotators ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion.\n\nOptions:\nA. RSA-PSS is a signature scheme.\nB. RSA-OAEP is an encryption scheme.\nC. The RSA based encryption scheme of the standard PKCS \\#1 v1.5 is vulnerable to a side channel attack.\nD. The RSA based scheme ISO/IEC 9796 is an encryption scheme.\n", "answer": "Explanations: The false assertion among the given options is: \"The RSA based encryption scheme of the standard PKCS \\#1 v1.5 is vulnerable to a side channel attack.\"\n\nExplanation:\n1. RSA-PSS is a signature scheme: True. RSA-PSS (Probabilistic Signature Scheme) is a widely used signature scheme based on RSA.\n2. RSA-OAEP is an encryption scheme: True. RSA-OAEP (Optimal Asymmetric Encryption Padding) is an encryption scheme that aims to provide security against chosen-ciphertext attacks.\n3. The RSA based encryption scheme of the standard PKCS \\#1 v1.5 is vulnerable to a side channel attack: False. The RSA encryption scheme of PKCS #1 v1.5 is known to be vulnerable to certain attacks, but it is not specifically a side channel attack. It is vulnerable to attacks like the Bleichenbacher attack.\n4. The RSA based scheme ISO/IEC 9796 is an encryption scheme: False. ISO/IEC 9796 is actually a digital signature scheme, not an encryption scheme.\n\nIn summary, the false assertion is that the RSA based encryption scheme of the standard PKCS #1 v1.5 is vulnerable to a side channel attack. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following is/are true about fuzzing with sanitizers?\n\nOptions:\nA. Some fuzzers use fork servers to reduce sanitizer overhead.\nB. The set of sanitizers used during a fuzzing campaign must be\n               carefully chosen (tradeoff between bug visibility/execution\n               speed).\nC. Some fuzzers dynamically tweak sanitizers to speed up fuzzing.\nD. ASAN instrumentation has a negligible startup overhead.\n", "answer": "Explanations: Let's break down each statement about fuzzing with sanitizers step-by-step to determine which one(s) are true:\n\n1. Some fuzzers use fork servers to reduce sanitizer overhead:\n   - This statement is true. Fuzzers like AFL (American Fuzzy Lop) utilize fork servers to reduce the overhead caused by sanitizers. By using fork servers, the fuzzer can isolate the target process and apply the sanitizer once, thereby reducing the performance impact compared to instrumenting each fuzzing iteration.\n\n2. The set of sanitizers used during a fuzzing campaign must be carefully chosen (tradeoff between bug visibility/execution speed):\n   - This statement is true. When conducting a fuzzing campaign, it is essential to carefully select the sanitizers to be used. Different sanitizers offer varying levels of bug visibility and can impact the execution speed of the fuzzing process. Therefore, there is a tradeoff between the thoroughness of bug detection provided by the sanitizers and the speed of the fuzzing campaign.\n\n3. Some fuzzers dynamically tweak sanitizers to speed up fuzzing:\n   - This statement is true. Certain fuzzers have the capability to dynamically adjust or tweak the sanitizers during the fuzzing process to enhance performance. By modifying sanitizer settings or selectively enabling/disabling certain checks, these fuzzers can optimize the fuzzing speed while maintaining effective bug detection capabilities.\n\n4. ASAN instrumentation has a negligible startup overhead:\n   - This statement is false. AddressSanitizer (ASan) instrumentation does introduce some overhead, including during startup. ASan adds additional checks and metadata to the binary, which can impact the program's startup time. While the benefits of detecting memory errors are significant, there is a noticeable startup overhead associated with ASan instrumentation.\n\nIn conclusion, the true statements about fuzzing with sanitizers are:\n1. Some fuzzers use fork servers to reduce sanitizer overhead.\n2. The set of sanitizers used during a fuzzing campaign must be carefully chosen (tradeoff between bug visibility/execution speed).\n3. Some fuzzers dynamically tweak sanitizers to speed up fuzzing. ///Answer: A"}
{"subject": "", "question": "Question: In a Ranked Retrieval result, the result at position k is non-relevant and at k+1 is relevant. Which of the following is always true (P@k and R@k are the precision and recall of the result set consisting of the k top ranked documents)?\n\nOptions:\nA. P@k-1 > P@k+1\nB. P@k-1 = P@k+1\nC. R@k-1 < R@k+\nD. R@k-1 = R@k+1\n", "answer": "Explanations: R@k-1 < R@k+1. This is always true because recall (R) measures the proportion of relevant documents that are retrieved out of all the relevant documents in the collection. Since at position k the result is non-relevant and at k+1 it is relevant, the recall at k+1 will always be higher than the recall at k. ///Answer: C"}
{"subject": "", "question": "Question: DES uses a key of size\\dots?\n\nOptions:\nA. $48$ bits\nB. $56$ bits\nC. $64$ bits\nD. $128$ bits\n", "answer": "Explanations: To answer this question, we need to recall the key sizes used in the Data Encryption Standard (DES). DES uses a key size of 56 bits. This means that the correct answer is:\n\n(b) $56$ bits\n\nThe key size of 56 bits in DES is used to encrypt and decrypt data. It is important to remember this key size when working with DES encryption and decryption processes. ///Answer: B"}
{"subject": "", "question": "Question: Once software has been shipped, what does the Software Development Lifecycle require you to do to maintain security guarantees?\n\nOptions:\nA. Track the evolution of third party dependencies\nB. Deploy updates timely and safely\nC. Provide new features to attract new users\nD. Ensure the software works on newer machines\n", "answer": "Explanations: After the software has been shipped, the Software Development Lifecycle (SDLC) requires ongoing maintenance to ensure security guarantees. This includes:\n\n1. Regularly monitoring and updating the software to address any newly discovered security vulnerabilities or threats.\n2. Implementing patches and updates to fix any security issues that may arise.\n3. Conducting regular security audits and assessments to identify and address any potential weaknesses in the software.\n4. Implementing secure coding practices and following security best practices to prevent security vulnerabilities.\n5. Educating users on best practices for using the software securely and securely configuring the software.\n6. Continuously improving and updating security measures to adapt to evolving threats and risks.\n7. Collaborating with stakeholders, such as cybersecurity experts and IT professionals, to stay informed about emerging security threats and trends.\n\nBy following these steps, you can help maintain the security guarantees of the software throughout its lifecycle. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements about libFuzzer is/are correct?\n\nOptions:\nA. Unit tests may serve as foundation to create libFuzzer fuzzing\n            stubs.\nB. In libFuzzer\u2019s default mode (not fork-mode), the tested APIs\n            must not contain \\texttt{exit()}.\nC. It is better to put narrow targets into the fuzzing stubs, e.g.,\n            if a target can parse several data formats, split it into several\n            targets, one per format.\nD. libFuzzer can only test single-threaded targets.\n", "answer": "Explanations: Let's analyze each statement to determine its correctness:\n\n1. \"Unit tests may serve as a foundation to create libFuzzer fuzzing stubs.\"\nThis statement is correct. Unit tests can provide a good starting point for creating libFuzzer fuzzing stubs as they often contain valuable input data and test cases that can be used to bootstrap the fuzzing process.\n\n2. \"In libFuzzer\u2019s default mode (not fork-mode), the tested APIs must not contain `exit()`.\"\nThis statement is correct. In libFuzzer's default mode (in-process fuzzing), the tested APIs should avoid containing calls to `exit()`. This is because calling `exit()` would terminate the entire process, including the fuzzing engine, prematurely ending the fuzzing process.\n\n3. \"It is better to put narrow targets into the fuzzing stubs, e.g., if a target can parse several data formats, split it into several targets, one per format.\"\nThis statement is correct. It is generally recommended to have more focused and narrow targets in the fuzzing stubs rather than having broad targets. By splitting a target that handles multiple data formats into separate targets, each focusing on one format, you can increase the effectiveness of the fuzzing process in finding bugs specific to each format.\n\n4. \"libFuzzer can only test single-threaded targets.\"\nThis statement is incorrect. While each individual libFuzzer process is single-threaded, libFuzzer can handle both single-threaded and multi-threaded targets. It can fuzz code that runs in multiple threads, allowing for testing of multi-threaded applications.\n\nIn conclusion, three out of the four statements about libFuzzer are correct based on its functionalities and best practices in fuzz testing. It is important to consider these aspects when utilizing libFuzzer for vulnerability detection and bug finding in software. ///Answer: B"}
{"subject": "", "question": "Question: Consider the\u00a0table of term frequencies for 3 documents D1, D2, and D3D1\u00a0\u00a0\u00a0\u00a0\u00a0 D2\u00a0\u00a0\u00a0\u00a0\u00a0 D3car740auto330insurance\u00a0\u00a0\u00a0\u00a0 037Considering the bag of words model , with TF-IDF weightning and cosine similarity metric, which document (D1, D2 or D3) is most relevant to the following query:\"car insurance\"?\n\nOptions:\nA. D1\nB. D2\nC. D3\n", "answer": "Explanations: Based on the TF-IDF weighting and cosine similarity metric, we can calculate the relevance of each document to the query \"car insurance\" by first converting the term frequencies into TF-IDF weights. \n\nTF-IDF weight for a term in a document = (Term frequency in the document) * log(Total number of documents / Number of documents containing the term)\n\nFor the query \"car insurance\", we need to calculate the TF-IDF weights for the terms \"car\" and \"insurance\" in each document:\n\nD1: \n- TF-IDF weight for \"car\" = 740 * log(3/1) = 740 * log(3) = 740 * 0.477 = 353.22\n- TF-IDF weight for \"insurance\" = 0 * log(3/1) = 0\n\nD2:\n- TF-IDF weight for \"car\" = 330 * log(3/1) = 330 * 0.477 = 157.41\n- TF-IDF weight for \"insurance\" = 37 * log(3/1) = 37 * 0.477 = 17.67\n\nD3:\n- TF-IDF weight for \"car\" = 0\n- TF-IDF weight for \"insurance\" = 37 * log(3/1) = 37 * 0.477 = 17.67\n\nNext, we calculate the cosine similarity between the query vector and the document vectors:\n\nCosine similarity = (TF-IDF weight of term1 in query * TF-IDF weight of term1 in document) + (TF-IDF weight of term2 in query * TF-IDF weight of term2 in document)\n\nFor D1:\n- Cosine similarity = (353.22 * 0) + (0 * 0) = 0\n\nFor D2:\n- Cosine similarity = (157.41 * 17.67) + (17.67 * 157.41) = 2780.05\n\nFor D3:\n- Cosine similarity = (0 * 17.67) + (0 * 17.67) = 0\n\nTherefore, based on the cosine similarity metric, document D2 is most relevant to the query \"car insurance.\" ///Answer: B"}
{"subject": "", "question": "Question: Standard encryption threats do not include:?\n\nOptions:\nA. Known-plaintext attacks.\nB. Chosen-plaintext attacks.\nC. Universal forgeries.\nD. Key-recovery attacks.\n", "answer": "Explanations: Standard encryption threats include known-plaintext attacks, chosen-plaintext attacks, and key-recovery attacks. However, universal forgeries are not typically considered a standard encryption threat. Therefore, the correct answer is: Universal forgeries. ///Answer: C"}
{"subject": "", "question": "Question: Let \\( f : A \rightarrow B \\) be a function from A to B such that \\(f (a) = |a| \\).\nf is a bijection if:?\n\nOptions:\nA. \\( A=  [0, 1] \\) and  \\(B=  [-1, 0] \\)\nB. \\( A=  [-1, 0] \\) and  \\(B=  [-1, 0] \\)\nC. \\( A=  [-1, 0] \\) and  \\(B=  [0, 1] \\)\nD. \\( A=  [-1, 1] \\) and  \\(B=  [-1, 1] \\)\n", "answer": "Explanations: To determine if the function \\( f(a) = |a| \\) is a bijection for a given domain and range, we need to consider both injectivity (one-to-one) and surjectivity (onto) of the function.\n\nInjectivity (One-to-One):\nA function is injective (one-to-one) if each element of the domain maps to a unique element in the codomain. In other words, if two elements in the domain map to the same element in the codomain, the function is not injective.\n\nSurjectivity (Onto):\nA function is surjective (onto) if every element in the codomain is mapped to by at least one element in the domain. In other words, the function covers all elements of the codomain.\n\nNow, let's analyze each option:\n\nA. \\( A= [0, 1] \\) and \\( B= [-1, 0] \\):\n- In this case, the function \\( f(a) = |a| \\) maps the interval \\([0, 1]\\) to the interval \\([0, 1]\\). It is injective because each unique value in the domain maps to a unique value in the codomain. However, it is not surjective because it does not cover all values in the codomain \\([-1, 0]\\). Therefore, this option is not correct.\n\nB. \\( A= [-1, 0] \\) and \\( B= [-1, 0] \\):\n- In this case, the function \\( f(a) = |a| \\) maps the interval \\([-1, 0]\\) to the interval \\([0, 1]\\). It is injective because each unique value in the domain maps to a unique value in the codomain. Moreover, it is surjective because it covers all values in the codomain \\([-1, 0]\\). Therefore, this option is correct.\n\nC. \\( A= [-1, 0] \\) and \\( B= [0, 1] \\):\n- In this case, the function \\( f(a) = |a| \\) maps the interval \\([-1, 0]\\) to the interval \\([0, 1]\\). It is injective because each unique value in the domain maps to a unique value in the codomain. However, it is not surjective because it does not cover all values in the codomain \\([0, 1]\\). Therefore, this option is not correct.\n\nD. \\( A= [-1, 1] \\) and \\( B= [-1, 1] \\):\n- In this case, the function \\( f(a) = |a| \\) maps the interval \\([-1, 1]\\) to the interval \\([0, 1]\\). It is not injective because both -1 and 1 in the domain map to the same value (1) in the codomain. Therefore, this option is not correct.\n\nTherefore, the correct answer is:\nB. \\( A= [-1, 0] \\) and \\( B= [-1, 0] \\) ///Answer: B"}
{"subject": "", "question": "Question: Let $p$ and $q$ be two prime numbers and $n=pq$. Let $K_p=(e,n)$ and $K_s=(d,n)$ be the RSA public and private keys respectively. Recall that the encryption of a message $m$ is $c=m^e \\bmod{n}$ and the decryption is $m=c^d \\bmod{n}$. Which assertion is \\emph{always true}?\n\nOptions:\nA. $ed=1 \\pmod{n}$\nB. $ed=1 \\pmod{\\varphi(n)}$\nC. $e=d \\pmod{n}$\nD. $e=d \\pmod{\\varphi(n)}$\n", "answer": "Explanations: To determine which assertion is always true in the RSA encryption scheme with given parameters, we need to consider the properties of the RSA algorithm and the relationships between the variables involved. Let's examine each option one by one:\n\nOption 0: $ed=1 \\pmod{n}$\n- This assertion states that the product of the public and private exponents is congruent to 1 modulo $n$.\n- Using the RSA scheme, $ed$ should indeed be equal to 1 modulo $n$ for decryption to work correctly. Therefore, option 0 is always true.\n\nOption 1: $ed=1 \\pmod{\\varphi(n)}$\n- This assertion states that the product of the public and private exponents is congruent to 1 modulo Euler's totient function of $n$, denoted $\\varphi(n)$.\n- In RSA, $ed$ is chosen such that it is congruent to 1 modulo $\\varphi(n)$. This relationship is crucial for the correctness and security of the RSA encryption scheme. Therefore, option 1 is always true.\n\nOption 2: $e=d \\pmod{n}$\n- This assertion states that the public and private exponents are congruent modulo $n$.\n- In the RSA scheme, the public and private exponents are not required to be congruent modulo $n$. They serve different purposes in encryption and decryption. Therefore, option 2 is not always true.\n\nOption 3: $e=d \\pmod{\\varphi(n)}$\n- This assertion states that the public and private exponents are congruent modulo Euler's totient function of $n$, $\\varphi(n)$.\n- In the RSA scheme, the public and private exponents are typically chosen such that they are congruent modulo $\\varphi(n)$. This relationship is essential for the security of the RSA algorithm. Therefore, option 3 is always true.\n\nIn conclusion, options 0, 1, and 3 are always true in the context of the RSA encryption scheme, while option 2 is not always true. ///Answer: B"}
{"subject": "", "question": "Question: Let the samples $\\left\\{\\left(y_{n}, x_{n}\\right)\\right\\}$ come from some fixed joint distribution $p(x, y)$, where $x_{n}$ and $y_{n}$ are scalars and both have zero mean. Consider linear regression, i.e., we want to predict $Y$ from $X$ by means of $f(x)=\\alpha x$ and we consider a square loss. Meaningful regression is possible?\n\nOptions:\nA. (a)  only if $X$ \"causes\" $Y$\nB. (b)  as long as $Y$ and $X$ have non-zero correlation\nC. (c)  only if $Y$ and $X$ are positively correlated, i.e., $\\mathbb{E}[X Y]>0$\nD. (d)  only if $Y$ and $X$ are negatively correlated, i.e., $\\mathbb{E}[X Y]<0$\n", "answer": "Explanations: To determine which option is correct, let's consider the relationship between the predictor $X$ and the response variable $Y$ in the context of linear regression with a square loss function.\n\nIn linear regression, the goal is to find the best-fitting line that minimizes the square loss between the predicted values and the actual values of $Y$. The linear regression model assumes that the relationship between $X$ and $Y$ can be captured by a linear function $f(x) = \\alpha x$.\n\nGiven that both $X$ and $Y$ have zero mean, the correlation between $X$ and $Y$ is a measure of the linear relationship between the two variables. \n\nNow, let's evaluate each option:\n\nOption 0: (a) only if $X$ \"causes\" $Y\"\n- This option is not directly related to the correlation between $X$ and $Y$ and does not provide a clear criterion for meaningful regression.\n\nOption 1: (b) as long as $Y$ and $X$ have non-zero correlation\n- This option suggests that as long as $X$ and $Y$ have a non-zero correlation, meaningful regression is possible. However, this is not sufficient to determine if linear regression with a square loss will be successful.\n\nOption 2: (c) only if $Y$ and $X$ are positively correlated, i.e., $\\mathbb{E}[X Y]>0\n- This option implies that meaningful regression is possible only if $X$ and $Y$ are positively correlated. In the context of linear regression, a positive correlation indicates that there is a positive linear relationship between $X$ and $Y$, which can be captured by the linear regression model.\n\nOption 3: (d) only if $Y$ and $X$ are negatively correlated, i.e., $\\mathbb{E}[X Y]<0\n- This option suggests that meaningful regression is possible only if $X$ and $Y$ are negatively correlated. In the context of linear regression, a negative correlation indicates a negative linear relationship between $X$ and $Y.\n\nBased on the discussion above, we can conclude that Option 2: (c) only if $Y$ and $X$ are positively correlated, i.e., $\\mathbb{E}[X Y]>0 is the most appropriate choice. In linear regression, a positive correlation between the predictor $X$ and the response variable $Y$ indicates a meaningful relationship that can be captured by the linear regression model. ///Answer: C"}
{"subject": "", "question": "Question: Let $X$, $Y$, and $K$ be respectively the plaintext, ciphertext, and key distributions. $H$ denotes the Shannon entropy. The consequence of perfect secrecy is \\dots?\n\nOptions:\nA. $H(K) \\geq H(X)$\nB. $H(K) \\leq H(X)$\nC. $H(K,X) \\leq H(X)$\nD. $H(Y) \\leq H(X)$\n", "answer": "Explanations: Given the multiple choice options {'$H(Y) \\\\leq H(X)$', '$H(K) \\\\geq H(X)$', '$H(K,X) \\\\leq H(X)$', '$H(K) \\\\leq H(X)$'}, we need to determine the consequence of perfect secrecy in encryption schemes based on the relationships between the entropy of the plaintext distribution $H(X)$, the entropy of the ciphertext distribution $H(Y)$, and the entropy of the key distribution $H(K)$.\n\nThe correct choice is '$H(K) \\geq H(X)$'.\n\nHere is a step-by-step solution to explain why this is the correct consequence of perfect secrecy:\n\n1. Perfect secrecy in cryptography implies that the ciphertext distribution $Y$ is independent of the plaintext distribution $X$, such that $H(Y|X) = H(Y)$.\n\n2. By the definition of conditional entropy, $H(Y|X) = H(X,Y) - H(X)$. Since $Y$ is independent of $X$ under perfect secrecy, $H(X,Y) = H(X) + H(Y)$.\n\n3. Therefore, $H(Y) = H(X) + H(Y) - H(X)$, which simplifies to $H(Y) = H(X) + H(Y) - H(X)$.\n\n4. Rearranging the terms, we get $H(Y) - H(X) = H(Y) - H(X)$. This implies that $H(Y) = H(X)$, which means that the entropy of the ciphertext distribution is equal to the entropy of the plaintext distribution under perfect secrecy.\n\n5. In the context of perfect secrecy, the key distribution $K$ plays a crucial role in determining the security of the encryption scheme. The key should be able to provide enough randomness to ensure that the ciphertext distribution is independent of the plaintext distribution.\n\n6. To achieve perfect secrecy, the key distribution $K$ should have at least as much entropy as the plaintext distribution $X$, i.e., $H(K) \\geq H(X)$. This ensures that the key provides enough randomness to encrypt the plaintext without leaking any information.\n\nTherefore, the consequence of perfect secrecy is '$H(K) \\geq H(X)$', as the key distribution must have sufficient entropy to maintain the security of the encryption scheme and achieve perfect secrecy. ///Answer: B"}
{"subject": "", "question": "Question: Suppose that an item in a leaf node N exists in every path. Which one is correct?\n\nOptions:\nA. N co-occurs with its prefix in every transaction.\nB. For every node p that is a parent of N in the fp tree, confidence(p->n) = 1 c.\nC. N\u2019s minimum possible support is equal to the number of paths.\nD. The item N exists in every candidate set.\n", "answer": "Explanations: 1. N co-occurs with its prefix in every transaction: This statement is not necessarily true. Just because an item exists in every path in the FP-tree does not guarantee that it co-occurs with its prefix in every transaction. The presence of an item in a path does not imply a direct relationship with its prefix in every transaction.\n\n2. For every node p that is a parent of N in the FP tree, confidence(p->n) = 1: This statement is not necessarily true. The confidence between a parent node and its child node in the FP-tree is calculated based on the support of the combined itemset, not just the presence of an item in every path. Therefore, it is not guaranteed that the confidence will always be 1.\n\n3. N\u2019s minimum possible support is equal to the number of paths: This statement is correct. If an item exists in every path in the FP-tree, then its support will be equal to the number of paths. This is because support is calculated as the number of transactions containing the item divided by the total number of transactions, and in this case, the item is present in every transaction (path).\n\n4. The item N exists in every candidate set: This statement is not necessarily true. Just because an item exists in every leaf node in the FP-tree does not mean it will be present in every candidate set. Candidate sets are generated based on frequent itemsets, and the presence of an item in every path does not guarantee that it will always be a frequent item in every candidate set.\n\nTherefore, the correct statement is: N\u2019s minimum possible support is equal to the number of paths, as it directly relates to the concept of support and the item's presence in every transaction (path) in the FP-tree. ///Answer: C"}
{"subject": "", "question": "Question: Let $\\mathcal{C}$ be the $(6,3)$ linear code on $\\mathbb{F}_3$ whose parity-check matrix is  \begin{equation*}      H = \begin{pmatrix}   2 &0 &1 &1 &1 &0 \\     1 &2 &0 &0 &1 &1 \\     0 &0 &0 &1 &1 &1    \\end{pmatrix}. \\end{equation*} True or false: The matrix      \begin{equation*}     \tilde{H} = \begin{pmatrix}     1 &0 &2 &2 &2 &0 \\      2 &1 &0 &0 &2 &2 \\      0 &0 &0 &2 &2 &2      \\end{pmatrix}      \\end{equation*} is also a valid parity-check matrix for $\\mathcal{C}$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The matrix $\\tilde{H}$ can be obtained from $H$ by performing row operations, which do not change the code generated by the parity-check matrix. Therefore, $\\tilde{H}$ is also a valid parity-check matrix for the code $\\mathcal{C$. \n\nTherefore, the correct answer is: \n1. True ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{true} assertion.\n\nOptions:\nA. In an universal forgery the adversary has stronger objectives than in an existential forgery.\nB. In a MAC forgery under known message attack the adversary is able to request the authentication of several messages.\nC. In an existential forgery the adversary is able to forge a valid MAC for an arbitrary message.\nD. A MAC provides authentication, integrity and confidentiality.\n", "answer": "Explanations: 1. Universal Forgery: In cryptography, a universal forgery is an attack on a Message Authentication Code (MAC) where the adversary has the ability to forge authentic messages for any message of their choice without observing any valid MAC queries.\n\n2. Known Message Attack: In cryptography, a known message attack is a type of attack where the adversary has access to the MACs of chosen messages. This allows the adversary to forge MACs for new messages that were not part of the original set of messages.\n\n3. Existential Forgery: In cryptography, an existential forgery is an attack on a MAC where the adversary can produce a valid MAC for a message that has not been authenticated before. This does not require the adversary to forge a MAC for a specific message but rather for any message that was not part of the original messages.\n\n4. MAC (Message Authentication Code): A MAC is a short piece of information used to authenticate a message and to provide integrity. It is generated by a function that combines the message with a secret key, ensuring that only parties with the key can verify the authenticity of the message.\n\nAnswer to the question: \nThe true assertion among the given options is:\n3. In an existential forgery, the adversary is able to forge a valid MAC for an arbitrary message. ///Answer: C"}
{"subject": "", "question": "Question: Consider the list : 5, 12, 7, 8, 41\n\nIn which sorting algorithm(s) do you directly compare 5 and 41 ?\n\nWe are considering the version of insertion sort seen in the slides of the lecture: we compare \\( a_j \\) to \\( a_{j-1} \\), then \\( a_{j-2} \\) etc ...\n\n?\n\nOptions:\nA. Bubble sort\nB. Insertion sort\nC. They are directly compared in both algorithms\nD. They are not compared in any of the algorithms\n", "answer": "Explanations: The correct answer is: **They are not compared in any of the algorithms**\n\nExplanation:\n1. **Bubble Sort**: In Bubble Sort, elements are compared adjacent to each other and swapped if they are in the wrong order. The comparison is only made between adjacent elements, and the target element is not compared directly with any element other than its adjacent element.\n2. **Insertion Sort**: In Insertion Sort, the algorithm iterates through the array one element at a time and inserts the current element into its correct position among the previously sorted elements. The comparison is made between the current element and the elements in the sorted portion of the array to determine the correct position for insertion. The target element is not directly compared to all elements in the array, but rather to a subset of the sorted elements.\n\nTherefore, in neither Bubble Sort nor Insertion Sort do we directly compare the target element with all other elements in the array during the sorting process. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. A Universal Forgery is \\ldots?\n\nOptions:\nA. \\ldots a forgery where the adversary is able to forge a valid MAC/signature for an arbitrary message.\nB. \\ldots a forgery where the adversary is able to forge a valid MAC/signature for a new message.\nC. \\ldots a forgery where the adversary has to recover the secret key.\nD. \\ldots a forgery where the adversary plays chess.\n", "answer": "Explanations: A Universal Forgery is a forgery where the adversary is able to forge a valid MAC/signature for an arbitrary message. This means that the adversary can create a valid MAC or signature for any message of their choosing, without needing to know the secret key or any specific details about the message beforehand. This type of forgery demonstrates a fundamental weakness in the security of the MAC or signature scheme. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is true?\n\nOptions:\nA. High precision implies low recall\nB. High precision hurts recall\nC. High recall hurts precision\nD. High recall implies low precisions\n", "answer": "Explanations: Out of the given statements, the correct one is: High recall hurts precision.\n\nPrecision and recall are two important metrics used to evaluate the performance of a classification model, particularly in the context of binary classification tasks.\n\nPrecision is the ratio of the number of true positive predictions to the total number of positive predictions made by the model. It measures the accuracy of positive predictions. High precision means that when the model predicts a positive class, it is highly likely to be correct.\n\nRecall, on the other hand, is the ratio of the number of true positive predictions to the total number of actual positive instances in the data. It measures the ability of the model to capture all positive instances in the data. High recall means that the model is able to find most of the positive instances in the data.\n\nWhen we talk about high recall hurting precision, it means that when a model is optimized for high recall (i.e., finding as many positive instances as possible), it may end up classifying some negative instances as positive as well. This can lead to a decrease in precision because the model is making more false positive predictions along with true positive predictions.\n\nIn summary:\n- High precision implies low recall (not true)\n- High precision hurts recall (not true)\n- High recall hurts precision (true)\n- High recall implies low precision (not true) ///Answer: C"}
{"subject": "", "question": "Question: If for the \u03c72 statistics for a binary feature we obtain P(\u03c72 | DF = 1) < 0.05 this means?\n\nOptions:\nA. That the two features are correlated.\nB. That the class label is independent of the feature\nC. That the class label correlates with the feature\nD. None of the above\n", "answer": "Explanations: The correct option is: That the class label correlates with the feature.\n\nWhen the p-value for the \u03c72 statistics for a binary feature is less than 0.05, it indicates that there is a statistically significant relationship between the feature and the class label. In other words, the class label is correlated with the feature. This means that the feature is likely to be a good predictor of the class label and can provide valuable information for classification tasks. ///Answer: C"}
{"subject": "", "question": "Question: When representing the adjacency list of a Web page in a connectivity server by using a reference list from another Web page, the reference list is searched only in a neighboring window of the Web page's URL, because:?\n\nOptions:\nA. Subsequent URLs in an adjacency list have typically small differences\nB. Typically many URLs in a web page are similar to each other\nC. Often many URLs among two pages with similar URL are similar\nD. Most extra nodes are found in the neighboring window\n", "answer": "Explanations: The correct answer is:\n\n- Typically many URLs in a web page are similar to each other\n- Often many URLs among two pages with similar URL are similar\n\nExplanation:\n\nWhen representing the adjacency list of a Web page in a connectivity server using a reference list from another Web page, searching the reference list only in a neighboring window of the Web page's URL is sufficient because typically many URLs in a web page are similar to each other. This means that if two web pages have similar URLs, the content and references within them are likely related or similar as well. Therefore, searching for references within a neighboring window based on the URL similarity can provide relevant and useful information for connectivity servers without having to search through the entire web. ///Answer: B"}
{"subject": "", "question": "Question: Select the correct canonical representation(s) of the English verb \"to dance\" conjugated at the third person singular indicative present.\u00a0You will get a penalty for wrong ticks.\n\nOptions:\nA. dance+V+3+S+Ind+Pres\nB. dances+V+3+S+Ind+Pres\nC. dancing+N+3+S+Ind+Pres\nD. dancing+V+3+S+Ind+Pres\nE. danc+V+3+S+IndPres\nF. dances+N+P\n", "answer": "Explanations: The correct canonical representation of the English verb \"to dance\" conjugated at the third person singular indicative present is: dances+V+3+S+Ind+Pres\n\nIn English grammar, when conjugating a verb in the third person singular (he, she, it), an \"s\" is typically added to the base form of the verb. So, \"dances\" is the correct form for the verb \"to dance\" in this case. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following problems has not been shown equivalent to the others?\n\nOptions:\nA. The RSA Key Recovery Problem.\nB. The RSA Decryption Problem.\nC. The RSA Factorization Problem.\nD. The RSA Order Problem.\n", "answer": "Explanations: To determine which problem has not been shown equivalent to the others, we need to understand the definitions of each problem and how they are related in the context of RSA encryption.\n\n1. The RSA Key Recovery Problem: This problem involves recovering the private key from the corresponding public key in an RSA encryption system. It is considered equivalent to the RSA Factorization Problem because both problems involve breaking the RSA encryption scheme by finding the prime factors of the modulus.\n\n2. The RSA Decryption Problem: This problem involves decrypting a ciphertext using the private key in an RSA encryption system. It is equivalent to the RSA Key Recovery Problem because both problems involve recovering the private key.\n\n3. The RSA Factorization Problem: This problem involves finding the prime factors of the modulus in an RSA encryption system. As mentioned earlier, it is equivalent to the RSA Key Recovery Problem.\n\n4. The RSA Order Problem: This problem involves finding the order of an element in the multiplicative group modulo n, where n is the RSA modulus. This problem is not directly related to the other RSA problems mentioned above, and therefore it has not been shown equivalent to the others.\n\nIn conclusion, the RSA Order Problem has not been shown equivalent to the RSA Key Recovery Problem, the RSA Decryption Problem, and the RSA Factorization Problem. ///Answer: D"}
{"subject": "", "question": "Question: A model predicts $\\mathbf{\\hat{y}} = [1, 0, 1, 1, 1]$. The ground truths are $\\mathbf{y} = [1, 0, 0, 1, 1]$.\n\nWhat is the accuracy?\n\nOptions:\nA. 0.5\nB. 0.75\nC. 0.8\nD. 0.875\n", "answer": "Explanations: To determine the accuracy of the model's predictions, we need to compare the predicted values $\\mathbf{\\hat{y}}$ with the actual ground truth values $\\mathbf{y}$. In this case, the model predicts $\\mathbf{\\hat{y}} = [1, 0, 1, 1, 1]$, and the ground truth values are $\\mathbf{y} = [1, 0, 0, 1, 1]$.\n\nWe will follow the steps to calculate the accuracy without explicitly mentioning the principles:\n\nFirst, we need to understand the problem: We are comparing the predicted values to the actual ground truth values to determine the accuracy of the model.\n\nNext, we devise a plan: The accuracy is calculated as the number of correct predictions divided by the total number of predictions.\n\nThen, we carry out the plan: By comparing the predicted values $\\mathbf{\\hat{y}}$ with the ground truth values $\\mathbf{y}$, we find that the model made 4 correct predictions out of 5.\n\nFinally, we look back at our process: The accuracy is computed as the number of correct predictions (4) divided by the total number of predictions (5), which equals 0.8.\n\nTherefore, the accuracy of the model's predictions is 0.8.\n\nCorrect option: '0.8' ///Answer: C"}
{"subject": "", "question": "Question: Recall that we say that a kernel $K: \\R \times \\R \rightarrow \\R $ is valid if there exists $k \\in \\mathbb{N}$ and $\\Phi: \\R \rightarrow \\R^k$ such that for all $(x, x') \\in \\R \times \\R $, $K(x, x') =  \\Phi(x)^\top \\Phi(x')$. The kernel $K(x, x') = \\cos(x + x')$ is a valid kernel.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The given kernel \\( K(x, x') = \\cos(x + x') \\) is a valid kernel because it corresponds to an inner product in a higher-dimensional space induced by the mapping \\( \\Phi: \\mathbb{R} \\rightarrow \\mathbb{R}^2 \\) where \\( \\Phi(x) = [\\cos(x), \\sin(x)] \\). This satisfies the condition for being a valid kernel. Therefore, the statement is True. ///Answer: A"}
{"subject": "", "question": "Question: Full-text retrieval refers to the fact that?\n\nOptions:\nA. the document text is grammatically fully analyzed for indexing\nB. queries can be formulated as texts\nC. all words of a text are considered as potential index terms\nD. grammatical variations of a word are considered as the same index terms\nE. \n", "answer": "Explanations: {\n    \"explaination\": \"Full-text retrieval refers to the process of searching for documents based on the actual content of the documents, rather than just based on metadata or tags associated with the documents. This means that the entire text of a document is analyzed and indexed, allowing for more accurate and comprehensive search results. When the document text is grammatically fully analyzed for indexing, queries can be formulated as texts, and all words of a text are considered as potential index terms. Additionally, in full-text retrieval, grammatical variations of a word are often considered as the same index terms to improve the search results.\",\n    \"answer\": \"the document text is grammatically fully analyzed for indexing, queries can be formulated as texts, all words of a text are considered as potential index terms, grammatical variations of a word are considered as the same index terms\"\n} ///Answer: A"}
{"subject": "", "question": "Question: Select what statements are true about probabilistic parsing.A penalty will be applied for any wrong answers selected.\n\nOptions:\nA. The purpose of introducing probabilities at the syntactic level is to enable the parser to choose among different parses of an ambiguous sentence.\nB. The probabilities aims at capturing linguistic phenomena that are hard to express in operational terms but can be estimated from linguistic corpora.\nC. Probabilizing does not depend on the way the syntactic parser is used (either as a recognizer or as an analyzer).\n", "answer": "Explanations: The first statement is true: The purpose of introducing probabilities at the syntactic level is to enable the parser to choose among different parses of an ambiguous sentence.\n\nThe second statement is true: The probabilities aim at capturing linguistic phenomena that are hard to express in operational terms but can be estimated from linguistic corpora.\n\nThe third statement is false: Probabilizing does depend on the way the syntactic parser is used (either as a recognizer or as an analyzer).\n\nTherefore, the correct statements are the first and second statements. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} statement for \\emph{independent} random variables $X$ and $Y$.\n\nOptions:\nA. $Var(XY) = Var(X)Var(Y)$.\nB. $E[XY] = E[X]E[Y]$.\nC. $\\Pr[X=x\\text{ and } Y = y] = \\Pr[X=x]\\Pr[Y=y]$.\nD. $E[X+Y] = E[X] + E[Y]$.\n", "answer": "Explanations: To determine the incorrect statement for independent random variables $X$ and $Y$, we can analyze each statement:\n\n1. $Var(XY) = Var(X)Var(Y)$: This statement is true for independent random variables $X$ and $Y$.\n   \n2. $E[XY] = E[X]E[Y]$: This statement is true for independent random variables $X$ and $Y$.\n\n3. $\\Pr[X=x$ and $Y = y] = \\Pr[X=x]\\Pr[Y=y]$: This statement is also true for independent random variables $X$ and $Y$ as independence implies the joint probability is the product of individual probabilities.\n\n4. $E[X+Y] = E[X] + E[Y]$: This statement is incorrect for independent random variables $X$ and $Y$. The correct formula for the expectation of the sum of two random variables is $E[X+Y] = E[X] + E[Y]$ only if $X$ and $Y$ are uncorrelated, not necessarily independent.\n\nTherefore, the incorrect statement for independent random variables $X$ and $Y$ is: $E[X+Y] = E[X] + E[Y]$.\n1. $Var(XY) = Var(X)Var(Y)$: This statement is true for independent random variables because the variance of the product of independent random variables is the product of their variances.\n\n2. $E[XY] = E[X]E[Y]$: This statement is true for independent random variables because the expectation of the product of independent random variables is the product of their expectations.\n\n3. $\\Pr[X=x$ and $Y = y] = \\Pr[X=x]\\Pr[Y=y]$: This statement is true for independent random variables because the joint probability of independent random variables is the product of their individual probabilities.\n\n4. $E[X+Y] = E[X] + E[Y]$: This statement is incorrect for independent random variables. While it is true that the expectation of the sum of independent random variables is the sum of their individual expectations, the statement does not specify independence, just that they are random variables. \n\nTherefore, after careful consideration, the incorrect statement for independent random variables $X$ and $Y$ is $E[X+Y] = E[X] + E[Y]. ///Answer: D"}
{"subject": "", "question": "Question: In deep learning, which of these are hyper-parameters?\n\nOptions:\nA. The learning rate\nB. The weights $\\mathbf{W}^{[l]}$ and biases $\\mathbf{b}^{[l]}$\nC. The type of weight initialization\nD. The number of layers\n", "answer": "Explanations: 1. The learning rate: This is a hyper-parameter because it is set before training the model and determines how much the weights are updated during each iteration of training. It can greatly affect the performance of the model.\n\n2. The weights $\\mathbf{W}^{[l]}$ and biases $\\mathbf{b}^{[l]}$: These are not hyper-parameters, as they are the parameters that are learned during training to minimize the loss function.\n\n3. The type of weight initialization: This is a hyper-parameter because it determines how the initial weights are set before training begins, which can impact how quickly the model converges and the final performance.\n\n4. The number of layers: This is a hyper-parameter because it determines the architecture of the neural network and can greatly impact the model's ability to learn complex patterns in the data. ///Answer: A"}
{"subject": "", "question": "Question: One can find a collision in a hash function $h\\colon \\{0,1\\}^* \\rightarrow \\{0,1\\}^n$ with expected time complexity\\dots?\n\nOptions:\nA. $\\Theta(\\sqrt{n})$.\nB. $\\Theta(n)$.\nC. $\\Theta(2^n)$.\nD. $\\Theta(2^{n/2})$.\n", "answer": "Explanations: The best answer is $\\Theta(2^{n/2})$. This option represents the most efficient time complexity for finding a collision in a hash function, as it strikes a balance between computational complexity and practical feasibility. The other options either overestimate or underestimate the time required for this task. ///Answer: D"}
{"subject": "", "question": "Question: Determine which of the following conditional statements evaluate to true (more than one answer can be correct):?\n\nOptions:\nA. If 5-7 > 0, then 3-7>0\nB. If 5-7 < 0, then 3-7>0\nC. If 5-7 > 0, then 3-7<0\nD. If 5-7 < 0, then 3-7<0\nE. None of the other options\n", "answer": "Explanations: Let's analyze each conditional statement:\n\n1. If 5-7 > 0, then 3-7>0: \n5-7 equals -2, which is not greater than 0. Therefore, the condition is false, and the statement as a whole is true because the condition is not met.\n\n2. If 5-7 < 0, then 3-7>0:\n5-7 equals -2, which is indeed less than 0. However, 3-7 equals -4, which is not greater than 0. Therefore, the condition is met, but the statement as a whole is false.\n\n3. If 5-7 > 0, then 3-7<0:\nAs we saw in statement 1, 5-7 is -2, which is not greater than 0. Therefore, the condition is false, and the statement as a whole is true because the condition is not met.\n\n4. If 5-7 < 0, then 3-7<0:\nBoth 5-7 and 3-7 result in negative numbers, so both conditions are met. Therefore, the statement as a whole is true.\n\nSo, the conditional statements that evaluate to true are:\n1. If 5-7 > 0, then 3-7>0\n2. If 5-7 > 0, then 3-7<0\n\nTherefore, the correct answers are:\n['If 5-7 > 0, then 3-7>0', 'If 5-7 > 0, then 3-7<0'] ///Answer: A"}
{"subject": "", "question": "Question: Let $\\mathcal{R}_{p}(f, \\varepsilon)$ be the $\\ell_{p}$ adversarial risk of a classifier $f: \\mathbb{R}^{d} \\rightarrow\\{ \\pm 1\\}$, i.e., $$ \\mathcal{R}_{p}(f, \\varepsilon)=\\mathbb{E}_{(\\mathbf{x}, y) \\sim \\mathcal{D}}\\left[\\max _{\\tilde{\\mathbf{x}}:\\|\\mathbf{x}-\\tilde{\\mathbf{x}}\\|_{p} \\leq \\varepsilon} \\mathbb{1}_{\\{f(\\tilde{\\mathbf{x}}) \\neq y\\}}\\right], $$ for $p=1,2, \\infty$. Which of the following relationships between the adversarial risks is true?\n\nOptions:\nA. $\\mathcal{R}_{2}(f, \\varepsilon) \\leq \\mathcal{R}_{1}(f, 2 \\varepsilon)$\nB. $\\mathcal{R}_{\\infty}(f, \\varepsilon) \\leq \\mathcal{R}_{2}(f, \\sqrt{d} \\varepsilon)$\nC. $\\mathcal{R}_{\\infty}(f, \\varepsilon) \\leq \\mathcal{R}_{1}(f, \\varepsilon)$\nD. $\\mathcal{R}_{\\infty}(f, \\varepsilon) \\leq \\mathcal{R}_{2}(f, \\varepsilon / d)$\n", "answer": "Explanations: To solve this question, the student should understand the concept of adversarial risk in the context of $\\ell_p$ norms. \n\n1. The first option states that $\\mathcal{R}_{2}(f, \\varepsilon) \\leq \\mathcal{R}_{1}(f, 2 \\varepsilon)$. This is true because the $\\ell_2$ ball of radius $\\varepsilon$ is contained within the $\\ell_1$ ball of radius $2\\varepsilon$. Thus, the $\\ell_2$ adversarial risk is less than or equal to the $\\ell_1$ adversarial risk.\n\n2. The second option states that $\\mathcal{R}_{\\infty}(f, \\varepsilon) \\leq \\mathcal{R}_{2}(f, \\sqrt{d} \\varepsilon)$. This is also true because the $\\ell_{\\infty}$ ball of radius $\\varepsilon$ is contained within the $\\ell_2$ ball of radius $\\sqrt{d}\\varepsilon$. Thus, the $\\ell_{\\infty}$ adversarial risk is less than or equal to the $\\ell_2$ adversarial risk.\n\n3. The third option states that $\\mathcal{R}_{\\infty}(f, \\varepsilon) \\leq \\mathcal{R}_{1}(f, \\varepsilon)$. This is not true in general because the $\\ell_{\\infty}$ adversarial risk can be larger than the $\\ell_1$ adversarial risk due to the different shapes of the $\\ell_{\\infty}$ and $\\ell_1$ balls.\n\n4. The fourth option states that $\\mathcal{R}_{\\infty}(f, \\varepsilon) \\leq \\mathcal{R}_{2}(f, \\varepsilon / d)$. This is true because the $\\ell_{\\infty}$ ball of radius $\\varepsilon$ is contained within the $\\ell_2$ ball of radius $\\varepsilon/\\sqrt{d}$. Thus, the $\\ell_{\\infty}$ adversarial risk is less than or equal to the $\\ell_2$ adversarial risk.\n\nTherefore, the correct answers are the first, second, and fourth options. The student can arrive at these conclusions by understanding the properties of different $\\ell_p$ norms and how they relate to each other in terms of adversarial risk. ///Answer: A"}
{"subject": "", "question": "Question: Which of these plaintexts can be the result of the decryption of \\texttt{SERGEV} using a simple subsitution cipher?\n\nOptions:\nA. \\texttt{VERNAM}\nB. \\texttt{ENIGMA}\nC. \\texttt{TURING}\nD. \\texttt{CAESAR}\n", "answer": "Explanations: **Correct Answer: 4) \\texttt{CAESAR}**\n\nTo determine which plaintext can result from decrypting \\texttt{SERGEV} using a simple substitution cipher, we can analyze the letters in both the ciphertext and the potential plaintext options.\n\n- When we decrypt \\texttt{SERGEV} using a simple substitution cipher, we need to map each letter in the ciphertext to a corresponding letter in the plaintext.\n- Let's look at the first letter in the ciphertext: \\texttt{S}. If it decrypts to \\texttt{C}, then the potential plaintext \\texttt{CAESAR} fits.\n- Moving on to the second letter in the ciphertext, \\texttt{E}. If it decrypts to \\texttt{A}, the potential plaintext remains \\texttt{CAESAR}.\n- The third letter in the ciphertext, \\texttt{R}, decrypts to \\texttt{E}.\n- The fourth letter in the ciphertext, \\texttt{G}, decrypts to \\texttt{S}.\n- The fifth letter in the ciphertext, \\texttt{E}, decrypts to \\texttt{A}.\n- Therefore, the decryption of \\texttt{SERGEV} using a simple substitution cipher results in \\texttt{CAESAR}.\n\n**Assumptions:** This solution assumes a standard simple substitution cipher where each letter in the plaintext is consistently replaced by the same letter in the ciphertext. ///Answer: D"}
{"subject": "", "question": "Question: (Backpropagation) Training via the backpropagation algorithm always learns a globally optimal neural network if there is only one hidden layer and we run an infinite number of iterations and decrease the step size appropriately over time.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: B: False\n\nWhile backpropagation is a powerful and widely used algorithm for training neural networks, it does not guarantee convergence to a globally optimal solution, even with one hidden layer and infinite iterations. The optimization process can get stuck in local minima or plateaus, and the choice of hyperparameters such as learning rate and batch size can also impact the final solution. Therefore, the statement is false. ///Answer: B"}
{"subject": "", "question": "Question: If for the \u03c72 statistics for a binary feature, we obtain P(\u03c72 |DF = 1) < 0.05, this means:?\n\nOptions:\nA. That the class labels depends on the feature\nB. That the class label is independent of the feature\nC. That the class label correlates with the feature\nD. No conclusion can be drawn\n", "answer": "Explanations: \nWhen the \u03c72 statistic for a binary feature is less than 0.05, it indicates that there is a statistically significant relationship between the feature and the class labels. This means that the class labels are dependent on the feature, as the low p-value suggests that the null hypothesis of independence between the feature and class labels can be rejected. Therefore, the correct option is: That the class labels depends on the feature. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{\\emph{incorrect}} assertion. A $\\Sigma$-protocol \\dots?\n\nOptions:\nA. has special soundness.\nB. is zero-knowledge.\nC. is a 3-move interaction.\nD. has the verifier polynomially bounded.\n", "answer": "Explanations: A: True. A $\\Sigma$-protocol has special soundness, which means that given two accepting conversations with the same commitment but different randomness, one can efficiently extract the witness. This property ensures that the prover cannot cheat by providing different responses to the same challenge.\n\nB: False. A $\\Sigma$-protocol is not necessarily zero-knowledge. While many $\\Sigma$-protocols are zero-knowledge, it is not a requirement for a $\\Sigma$-protocol. Zero-knowledge protocols provide stronger privacy guarantees, but not all $\\Sigma$-protocols achieve zero-knowledge.\n\nC: False. A $\\Sigma$-protocol is not always a 3-move interaction. Some $\\Sigma$-protocols may involve more than three moves. The number of moves in a $\\Sigma$-protocol can vary depending on the specific protocol design.\n\nD: False. A $\\Sigma$-protocol does not necessarily have the verifier polynomially bounded. This is a characteristic of an argument system, not a $\\Sigma$-protocol. In an argument system, the verifier's computation is polynomially bounded, but this is not a requirement for a $\\Sigma$-protocol.\n\nTherefore, the correct answer is A: has special soundness. This property distinguishes a $\\Sigma$-protocol from the other options provided. ///Answer: B:"}
{"subject": "", "question": "Question: Dude said \u201cI like bowling\u201d. With how many statements can we express this sentence using \u200b RDF Reification?\n\nOptions:\nA. We cannot\nB. 1\nC. 3\nD. 5\n", "answer": "Explanations: First, let's break down the original statement \"Dude said 'I like bowling'\" into its components:\nSubject: Dude\nPredicate: said\nObject: \"I like bowling\"\n\nNow, we can create multiple statements using RDF reification to express the same information. Here are the possible options:\n\n1. We cannot express this statement using RDF reification.\n2. 1 statement with reification:\n   - Statement: {Dude, said, \"I like bowling\"}\n   - Reification: {reification1, hasSubject, Dude}, {reification1, hasPredicate, said}, {reification1, hasObject, \"I like bowling\"}\n3. 3 statements with reification:\n   - Statement 1: {Dude, said, \"I like bowling\"}\n   - Reification 1: {reification1, hasSubject, Dude}, {reification1, hasPredicate, said}, {reification1, hasObject, \"I like bowling\"}\n   - Reification 2: {reification2, hasStatement, reification1}, {reification2, hasTimestamp, \"2022-11-25\"}\n   - Reification 3: {reification3, hasStatement, reification1}, {reification3, hasLocation, \"Bowling Alley\"}\n4. 5 statements with reification:\n   - Statement 1: {Dude, said, \"I like bowling\"}\n   - Reification 1: {reification1, hasSubject, Dude}, {reification1, hasPredicate, said}, {reification1, hasObject, \"I like bowling\"}\n   - Reification 2: {reification2, hasStatement, reification1}, {reification2, hasTimestamp, \"2022-11-25\"}\n   - Reification 3: {reification3, hasStatement, reification1}, {reification3, hasLocation, \"Bowling Alley\"}\n   - Reification 4: {reification4, hasStatement, reification1}, {reification4, hasConfidenceLevel, \"High\"}\n   - Reification 5: {reification5, hasStatement, reification1}, {reification5, hasSource, \"Dude's friend\"}\n\nTherefore, we can express the sentence \"Dude said 'I like bowling'\" using RDF reification with 1, 3, or 5 statements. ///Answer: D."}
{"subject": "", "question": "Question: Tick the \\emph{minimal} assumption on the required channel to exchange the key of a Message Authentication Code (MAC):?\n\nOptions:\nA. nothing.\nB. authentication and integrity only.\nC. confidentiality only.\nD. authentication, integrity, and confidentiality.\n", "answer": "Explanations: \nFirst, we need authentication to ensure that both parties are who they claim to be. Second, we need integrity to ensure that the key exchange process has not been tampered with. And finally, we do not necessarily need confidentiality for the key exchange process of a MAC.\n\nTherefore, the minimal assumption on the required channel to exchange the key of a MAC would be:\n\n2. authentication and integrity only. ///Answer: B"}
{"subject": "", "question": "Question: Which adversarial model corresponds to Linear Cryptanalysis?\n\nOptions:\nA. Chosen Ciphertext Attack\nB. Chosen Plaintext Attack\nC. Known Ciphertext Attack\nD. Known Plaintext Attack\n", "answer": "Explanations: Linear Cryptanalysis corresponds to the Known Plaintext Attack.\n\nA. Chosen Ciphertext Attack - False\nB. Chosen Plaintext Attack - False\nC. Known Ciphertext Attack - False\nD. Known Plaintext Attack - True ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion. In RSA with public key $(e,N)$ and private key $(d,N)$ \\ldots?\n\nOptions:\nA. the key generation requires that $\\gcd(e,N) = 1$.\nB. decryption does not work for some $y \\in \\mathbb{Z}_N$ for which $\\gcd(y,N) \\not= 1$.\nC. if we know $\\phi(N)$, the ciphertext $y$ and the public key, we can decrypt $y$.\nD. in order to decrypt we need to have $e^d \\equiv 1 \\pmod{N}$.\n", "answer": "Explanations: A: the key generation requires that $\\gcd(e,N) = 1$.\n\nThis assertion is correct. In RSA, the public key $(e,N)$ and the private key $(d,N)$ are generated such that $e$ and $N$ are coprime, meaning their greatest common divisor is 1. This condition is necessary for the encryption and decryption processes to work correctly.\n\nB: decryption does not work for some $y \\in \\mathbb{Z}_N$ for which $\\gcd(y,N) \\not= 1$.\n\nThis assertion is correct. In RSA, decryption works correctly for ciphertexts $y$ where $\\gcd(y, N) = 1$. If $\\gcd(y, N) \\neq 1$, decryption may not be possible or may result in an incorrect message.\n\nC: if we know $\\phi(N)$, the ciphertext $y$ and the public key, we can decrypt $y.\n\nThis assertion is incorrect. Knowing $\\phi(N)$, the ciphertext $y$, and the public key $(e, N)$ is not sufficient to decrypt $y. Decryption requires the private key $(d, N)$, which is kept secret. The knowledge of $\\phi(N)$ alone does not allow decryption.\n\nD: in order to decrypt we need to have $e^d \\equiv 1 \\pmod{N}$.\n\nThis assertion is correct. In RSA, the private key $d$ is the modular multiplicative inverse of the public key exponent $e$ modulo $\\phi(N)$. This means that $e \\cdot d \\equiv 1 \\pmod{\\phi(N)}$. Since $\\phi(N)$ and $N$ are related, this also implies $e^d \\equiv 1 \\pmod{N}$, which is necessary for decryption to work correctly in RSA. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following statements has been proved.\n\nOptions:\nA. $\\textrm{IP} \\subseteq \\textrm{NP}$\nB. $\\textrm{IP} = \\textrm{PSPACE}$\nC. $\\textrm{P} \\neq \\textrm{NP}$\nD. $\\textrm{SAT} \\in \\textrm{P}$\n", "answer": "Explanations: 1. $\\textrm{IP} \\subseteq \\textrm{NP}$: This statement has not been proved. The relationship between IP (Interactive Polynomial Time) and NP (Nondeterministic Polynomial Time) is still an open question in complexity theory.\n\n2. $\\textrm{IP} = \\textrm{PSPACE}$: This statement has been proved. It was proven by Adi Shamir in 1990 that IP is equal to PSPACE (Polynomial Space).\n\n3. $\\textrm{P} \\neq \\textrm{NP}$: This statement has not been proved. The famous P vs. NP problem is still an unsolved problem in computer science and mathematics.\n\n4. $\\textrm{SAT} \\in \\textrm{P}$: This statement has not been proved. The SAT (Boolean Satisfiability Problem) is a well-known NP-complete problem, and no polynomial-time algorithm has been found to solve it yet.\n\nTherefore, the only statement that has been proved is $\\textrm{IP} = \\textrm{PSPACE}$. ///Answer: B"}
{"subject": "", "question": "Question: Consider the exhaustive search of a uniformly distributed key in a set of size $N$. Think of the possible strategies and their complexities. Which of the following is \\textbf{not} possible (We assume that memory access is constant.)?\n\nOptions:\nA. Find the key with precomputation: $0$, memory: $O(1)$, time: $O(N)$.\nB. Find the key with precomputation: $O(N)$, memory: $O(N)$, time: $O(1)$.\nC. Find the key with precomputation: $O(N)$, memory: $O(N^{2/3})$, time: $O(N^{2/3})$.\nD. Find the key with precomputation: $0$, memory: $O(N)$, time: $O(1)$.\n", "answer": "Explanations: Answer: 'Find the key with precomputation: $O(N)$, memory: $O(N)$, time: $O(1)$.'\n\nExplanation:\nTo determine which option is not possible when assuming constant memory access, we need to analyze the strategies and their complexities. \n\nOption 1: 'Find the key with precomputation: $0$, memory: $O(1)$, time: $O(N)' - This option suggests that no precomputation is needed, memory usage is constant, and the time complexity is linear. This is a valid strategy for exhaustive search.\n\nOption 2: 'Find the key with precomputation: $O(N)$, memory: $O(N)$, time: $O(1)' - This option involves precomputing information that scales with the size of the set, using memory proportional to $N$, but achieves constant time complexity. This strategy is not typically feasible as precomputation time and memory usage increase with the size of the set.\n\nOption 3: 'Find the key with precomputation: $O(N)$, memory: $O(N^{2/3})$, time: $O(N^{2/3})' - This option involves precomputing information that scales with the set size to the power of 2/3, using memory that also scales to the power of 2/3, and achieving a time complexity that matches the memory usage. This option is feasible but may not be the most efficient in terms of memory and time complexity.\n\nOption 4: 'Find the key with precomputation: $0$, memory: $O(N)$, time: $O(1)' - This option suggests no precomputation is needed, memory usage scales with $N$, and the time complexity is constant. This is a valid and efficient strategy for an exhaustive search with uniformly distributed keys in a set of size $N.\n\nTherefore, the option that is not possible when assuming constant memory access is: 'Find the key with precomputation: $O(N)$, memory: $O(N)$, time: $O(1)'. This is because achieving constant time complexity with memory usage that scales linearly with the size of the set is not a practical or feasible strategy. ///Answer: B"}
{"subject": "", "question": "Question: MD5 is?\n\nOptions:\nA. a secure block cipher\nB. a broken block cipher\nC. a secure hash function\nD. a broken hash function\n", "answer": "Explanations: The correct answer is: 4\n\nMD5 is a broken hash function due to its vulnerabilities, making it insecure for cryptographic purposes. Hash functions like MD5 are used to verify data integrity by producing a fixed-size string of bytes from an input. MD5 has been found to have weaknesses, leading to the recommendation of more secure alternatives like SHA-256 in modern security applications. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion.\n\nOptions:\nA. Black-box ZK (zero knowledge) is a stronger notion than (simple) ZK.\nB. We can give a black-box ZK protocol deciding 3-COL (coloring graphs with 3 colours).\nC. The NP language has no ZK proofs.\nD. We can give a ZK protocol deciding ISO (graph isomorphisms).\n", "answer": "Explanations: Option 2: \"We can give a black-box ZK protocol deciding 3-COL (coloring graphs with 3 colours).\" is the false assertion.\n\nExplanation: \n\n1. \"Black-box ZK (zero knowledge) is a stronger notion than (simple) ZK.\" - This statement is true. Black-box zero knowledge is a more powerful and secure form of zero-knowledge proof where the verifier is not able to learn any additional information beyond the validity of the statement being proven. It is considered stronger than simple zero-knowledge.\n\n2. \"We can give a black-box ZK protocol deciding 3-COL (coloring graphs with 3 colours).\" - This statement is false. It is a known open problem whether there exists a black-box zero-knowledge proof for 3-coloring of graphs. This problem is related to the complexity class NP-complete and is a challenging area of research in theoretical computer science.\n\n3. \"The NP language has no ZK proofs.\" - This statement is false. The class NP (nondeterministic polynomial time) includes problems for which zero-knowledge proofs can be constructed. Zero-knowledge proofs are a fundamental concept in cryptography and are used to prove the validity of statements without revealing any additional information.\n\n4. \"We can give a ZK protocol deciding ISO (graph isomorphisms).\" - This statement is true. Zero-knowledge proofs can be constructed for the graph isomorphism problem, which involves determining whether two given graphs are isomorphic (structurally identical) or not. Zero-knowledge proofs provide a way to prove this without disclosing the actual isomorphism. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements is correct in the context of \u00a0information extraction?\n\nOptions:\nA. A confidence measure that prunes too permissive patterns discovered with bootstrapping can help reducing semantic drift\nB. The bootstrapping technique requires a dataset where statements are labelled\nC. Distant supervision typically uses low-complexity features only, due to the lack of training data\nD. For supervised learning, sentences in which NER has detected no entities are used as negative samples\n", "answer": "Explanations: The correct statement in the context of information extraction is:\n'A confidence measure that prunes too permissive patterns discovered with bootstrapping can help reduce semantic drift'.\n\nThis is the correct answer because in information extraction, particularly in the context of bootstrapping, it is essential to have mechanisms in place to control the quality of the extracted information. Semantic drift refers to a situation where the extracted information starts deviating from the intended meaning over iterations. By using a confidence measure to prune permissive patterns, it helps in filtering out incorrect or noisy information, thereby reducing semantic drift and improving the overall quality of the extracted data. ///Answer: A"}
{"subject": "", "question": "Question: In a FP tree, the leaf nodes are the ones with:?\n\nOptions:\nA. Lowest confidence\nB. Lowest support\nC. Least in the alphabetical order\nD. None of the above\n", "answer": "Explanations: The correct answer to the question is 'Lowest support'.\n\nStep-by-step solution:\n1. Understand the concept of FP tree (Frequent Pattern tree) in association rule mining. FP tree is a data structure used to represent frequent itemsets in a dataset.\n2. In an FP tree, the nodes represent items, and the edges represent the frequency of occurrence of those items.\n3. Leaf nodes in an FP tree represent the frequent itemsets with the lowest support, which is the frequency of occurrence of an itemset in a dataset.\n4. Support is calculated as the ratio of the number of transactions containing the itemset to the total number of transactions.\n5. The leaf nodes in an FP tree have the lowest support because they represent infrequent itemsets that occur least frequently in the dataset.\n6. Therefore, among the options provided, the correct answer is 'Lowest support'.\n\nIn summary, the leaf nodes in an FP tree are the ones with the lowest support, indicating infrequent itemsets in the dataset. This is a fundamental concept in association rule mining and understanding the structure of FP trees. ///Answer: B"}
{"subject": "", "question": "Question: To obtain a security of $2^{80}$ in a hash function against collisions one needs a hash output of size?\n\nOptions:\nA. $80$ bits.\nB. $40$ bits.\nC. $120$ bits.\nD. $160$ bits.\n", "answer": "Explanations: To obtain a security of $2^{80}$ in a hash function against collisions, you need a hash output size of at least 160 bits.\n\nExplanation:\n\nWhen we talk about security against collisions in a hash function, we are concerned with the likelihood of two different inputs producing the same hash output. The security against collisions is typically measured in terms of the size of the hash output.\n\nThe security against collisions is related to the birthday paradox, which states that in a set of randomly chosen elements, there is a high probability that two elements will share the same value when the number of elements reaches a certain threshold. In the context of hash functions, this means that as we increase the number of possible hash outputs, the likelihood of collisions occurring also increases.\n\nTo achieve a security level of $2^{80}$ against collisions, we need to ensure that the number of possible hash outputs is at least $2^{80}$. This means that the hash output size should be large enough to accommodate $2^{80}$ different hash values without collisions.\n\nSince $2^{80}$ is equivalent to $2^{80}$ different possibilities, we need a hash output size that can accommodate at least $2^{80}$ different values. The smallest hash output size that can accommodate $2^{80}$ different values is 160 bits, as $2^{160}$ is greater than $2^{80}$.\n\nTherefore, to obtain a security level of $2^{80}$ in a hash function against collisions, one needs a hash output size of 160 bits. ///Answer: D"}
{"subject": "", "question": "Question: In a challenge/response protocol,\\dots?\n\nOptions:\nA. the secret is sent in clear over the channel.\nB. the response is derived using a pseudo-random function.\nC. the response is a hash of the secret (i.e., $h(w)$ for a secret $w$).\nD. the size of the challenge space can be 1.\n", "answer": "Explanations: To evaluate the possible answers provided:\n\n1. \"The secret is sent in clear over the channel.\"\nThis statement is incorrect. In a challenge/response protocol, it is crucial for the secret to never be sent in plaintext over the channel. The whole purpose of such protocols is to ensure secure communication by not revealing the secret directly.\n\n2. \"The response is derived using a pseudo-random function.\"\nThis statement is correct. In many challenge/response protocols, the response is indeed derived using a pseudo-random function to ensure unpredictability and security. Pseudo-random functions are used to generate responses that are not easily guessable by an attacker.\n\n3. \"The response is a hash of the secret (i.e., $h(w)$ for a secret $w$).\"\nThis statement is incorrect. While hashing the secret may be part of the process in some protocols, it is not a universal characteristic of challenge/response protocols. The response can be derived in various ways depending on the specific protocol being used.\n\n4. \"The size of the challenge space can be 1.\"\nThis statement is incorrect. The challenge space refers to the set of possible challenges that can be presented to the entity being authenticated. Having a challenge space of 1 would make the protocol trivial and insecure, as there would be no variability in the challenges presented.\n\nTherefore, the correct answer is:\n{'Correct Answer(s)': 2} ///Answer: B"}
{"subject": "", "question": "Question: Thick the \\emph{incorrect} assertion.\n\nOptions:\nA. The goal of SAS-based cryptography is to reduce the length of the string that has to be authenticated.\nB. One way to authenticate a SAS is to use your phone.\nC. One can obtain a secure channel from a narrowband authenticated channel using SAS-based cryptography.\nD. SAS-based cryptography always requires the SAS to be collision-resistant.\n", "answer": "Explanations: When analyzing the statements regarding SAS-based cryptography, we can determine the correctness of each assertion:\n\n1. The goal of SAS-based cryptography is to reduce the length of the string that has to be authenticated.\n   - This statement is correct. SAS (Short Authentication String)-based cryptography aims to reduce the length of the string that needs to be authenticated, making it more user-friendly and efficient for verifying the authenticity of a connection.\n\n2. One way to authenticate a SAS is to use your phone.\n   - This statement is correct. Using a mobile device or phone to authenticate a SAS is a common and practical method in SAS-based cryptography. For example, when pairing Bluetooth devices, a short code displayed on one device can be entered on the other device to establish a secure connection.\n\n3. One can obtain a secure channel from a narrowband authenticated channel using SAS-based cryptography.\n   - This statement is correct. SAS-based cryptography can enhance the security of a narrowband authenticated channel by establishing a secure communication channel. It provides a way to securely exchange keys or establish trust between communicating parties over a potentially insecure channel.\n\n4. SAS-based cryptography always requires the SAS to be collision-resistant.\n   - This statement is incorrect. While collision resistance is a desirable property in many cryptographic schemes, SAS-based cryptography does not always necessitate the SAS to be collision-resistant. The primary focus of SAS is on reducing the length of authentication strings while maintaining security and usability.\n\nTherefore, the incorrect assertion among the given options is:\n4. SAS-based cryptography always requires the SAS to be collision-resistant. ///Answer: D"}
{"subject": "", "question": "Question: ASLR stands for?\n\nOptions:\nA. Address Space Layout Randomization\nB. Address Structure Layout Randomization\nC. Augmented Security Layer Randomization\nD. Address Space Linker Randomization\nE. Automatic Structure Layout Randomization\n", "answer": "Explanations: - ASLR stands for Address Space Layout Randomization.\n- It is a security technique used to prevent buffer overflow attacks by randomizing the memory locations of key data areas. \n- This makes it harder for an attacker to predict the memory layout of a process, making it more difficult to exploit vulnerabilities. ///Answer: A"}
{"subject": "", "question": "Question: The worst case complexity of an exaustive search against DES is\\dots?\n\nOptions:\nA. $1$\nB. $\\frac{2^{64}}{2}$\nC. $2^{56}$\nD. $2^{64}$\n", "answer": "Explanations: D). $2^{64}$ \n\nIn DES, there are 64 bits in the key. This means that there are $2^{64}$ possible keys that could be used to encrypt the data. In an exhaustive search attack, the attacker tries every possible key until they find the correct one. Therefore, the worst case complexity of an exhaustive search against DES is $2^{64}$. ///Answer: D"}
{"subject": "", "question": "Question: Which flag prevents user programs from reading and writing kernel data?\n\nOptions:\nA. PTE_P\nB. PTE_U\nC. PTE_D\nD. PTE_W\n", "answer": "Explanations: The flag that prevents user programs from reading and writing kernel data is PTE_U (User/Supervisor flag). This flag distinguishes between user and supervisor mode, allowing only the kernel to access certain memory areas. ///Answer: B"}
{"subject": "", "question": "Question: In Text Representation learning, which of the following statements is correct?\n\nOptions:\nA. Learning GloVe vectors can be done using SGD in a streaming fashion, by streaming through the input text only once.\nB. Every recommender systems algorithm for learning a matrix factorization $\\boldsymbol{W} \\boldsymbol{Z}^{\\top}$ approximating the observed entries in least square sense does also apply to learn GloVe word vectors.\nC. FastText performs unsupervised learning of word vectors.\nD. If you fix all word vectors, and only train the remaining parameters, then FastText in the two-class case reduces to being just a linear classifier.\n", "answer": "Explanations: The correct statement from the given options is:\n\n1. 'Learning GloVe vectors can be done using SGD in a streaming fashion, by streaming through the input text only once.'\n\nExplanation:\nThis statement is correct. GloVe (Global Vectors for Word Representation) is a method for learning word vectors by factorizing the co-occurrence matrix of words. It can indeed be trained using stochastic gradient descent (SGD) in a streaming fashion, where the model updates its parameters as it processes the input text in a single pass.\n\nThe other statements are incorrect:\n\n2. 'Every recommender systems algorithm for learning a matrix factorization $\\\\boldsymbol{W} \\\\boldsymbol{Z}^{\\\\top}$ approximating the observed entries in least square sense does also apply to learn GloVe word vectors.'\n   - This statement is incorrect. Recommender systems algorithms for matrix factorization are different from the methods used to learn word vectors like GloVe. They have different objectives and techniques specific to their respective tasks.\n\n3. 'FastText performs unsupervised learning of word vectors.'\n   - This statement is incorrect. FastText is a method that performs supervised learning of word vectors, not unsupervised learning. It is capable of learning word representations by taking into account subword information.\n\n4. 'If you fix all word vectors, and only train the remaining parameters, then FastText in the two-class case reduces to being just a linear classifier.'\n   - This statement is incorrect. When all word vectors are fixed and only the remaining parameters are trained, FastText does not reduce to just a linear classifier. It still maintains its unique capabilities in handling subword information for word representation learning.\n\nTherefore, the correct statement is the first one about GloVe vectors being learnable using SGD in a streaming fashion by processing the input text only once. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{correct} statement. $\\Sigma$-protocols \\ldots?\n\nOptions:\nA. are defined for any language in \\textrm{PSPACE}.\nB. have a polynomially unbounded extractor that can yield a witness.\nC. respect the property of zero-knowledge for any verifier.\nD. consist of protocols between a prover and a verifier, where the verifier is polynomially bounded.\n", "answer": "Explanations: Let's analyze each option one by one:\n\n1. $\\Sigma$-protocols are not necessarily defined for any language in PSPACE. They are typically used for languages in NP or co-NP, where a prover can convince a verifier that a statement is true without revealing any additional information about the statement. So, option 1 is not correct.\n\n2. $\\Sigma$-protocols do not necessarily have a polynomially unbounded extractor that can yield a witness. In $\\Sigma$-protocols, the prover convinces the verifier of the truth of a statement without revealing any additional information beyond the validity of the statement. The extractor is usually polynomially bounded. So, option 2 is not correct.\n\n3. $\\Sigma$-protocols do respect the property of zero-knowledge for any verifier. Zero-knowledge protocols ensure that the verifier learns nothing beyond the fact that the statement is true. This property is important in ensuring the privacy and security of the protocol. So, option 3 is correct.\n\n4. $\\Sigma$-protocols consist of protocols between a prover and a verifier, where the verifier is polynomially bounded. The verifier's computational complexity is an important aspect of $\\Sigma$-protocols, as it ensures that the protocol is efficient and practical. So, option 4 is correct.\n\nTherefore, the correct statement is: 3. respect the property of zero-knowledge for any verifier. and 4. consist of protocols between a prover and a verifier, where the verifier is polynomially bounded. ///Answer: C"}
{"subject": "", "question": "Question: (Text Representation Learning, GloVe) Learning GloVe word vectors is identical to approximating the observed entries of the word/context co-occurence counts by $\\mathbf{W} \\mathbf{Z}^{\\top}$, in the least square sense, if the $f_{d n}$ weights are set to 1 for all observed entries.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False: Although setting the $f_{d n}$ weights to 1 for all observed entries can help in approximating the observed entries of the word/context co-occurrence counts by $\\mathbf{W} \\mathbf{Z}^{\\top}$ in the least square sense, it does not necessarily mean that learning GloVe word vectors is identical to this approximation process. There are other factors and calculations involved in the learning of GloVe word vectors that go beyond just setting the weights to 1 for observed entries. ///Answer: B"}
{"subject": "", "question": "Question: Decision trees...\n\nOptions:\nA. ... can be used for both classification and regression.\nB. ... can be easily explained.\nC. ... need water and sunlight to grow.\nD. ... have several different roots.\n", "answer": "Explanations: 1. True - Decision trees can be used for both classification (predicting a discrete outcome) and regression (predicting a continuous outcome).\n2. True - Decision trees are easy to interpret and explain, making them a popular choice for model visualization.\n3. False - Decision trees are a machine learning algorithm and do not require water or sunlight to grow.\n4. False - Decision trees have a single root node from which branches out into different decision paths based on feature values. ///Answer: A"}
{"subject": "", "question": "Question: What are the complexities for the single-target dictionary attacks, when there are $N$ keys?\n\nOptions:\nA. Preprocessing: $N$, Memory: $N$, Time: 1\nB. Preprocessing: $N$, Memory: $1$, Time: $N$\nC. Preprocessing: $1$, Memory: $N$, Time: $N$\nD. Preprocessing: $0$, Memory: $1$, Time: $\\sqrt{N}$\n", "answer": "Explanations: When discussing the complexities of single-target dictionary attacks with $N$ keys, we are analyzing the different trade-offs between preprocessing time, memory usage, and time taken to conduct the attack. Let's break down each option given in the question:\n\n1. Preprocessing: $N$, Memory: $N$, Time: $1$ - In this scenario, the preprocessing time is $N$, meaning that a significant amount of time is spent upfront to prepare for the attack. The memory usage is also $N$, indicating that a substantial amount of memory is required during the attack process. However, the actual time taken to conduct the attack is only $1$, suggesting that once the preprocessing is completed, the attack itself is relatively quick.\n\n2. Preprocessing: $N$, Memory: $1$, Time: $N$ - Here, the preprocessing time remains at $N$, meaning that a substantial amount of time is still required upfront. However, the memory usage is reduced to $1, indicating that less memory is needed during the attack. The trade-off is that the time taken to conduct the attack is now $N$, which may result in a longer overall process.\n\n3. Preprocessing: $1$, Memory: $N$, Time: $N$ - In this case, the preprocessing time is reduced to $1, suggesting that minimal time is needed to prepare for the attack. However, the memory usage is $N$, meaning that a significant amount of memory is required during the attack process. The time complexity of the attack remains at $N$, which could be a limitation depending on the specific circumstances.\n\n4. Preprocessing: $0$, Memory: $1$, Time: $\\sqrt{N}$ - Finally, in this option, there is no preprocessing required, meaning that the attack can begin immediately. The memory usage is minimal at $1, indicating that only a small amount of memory is needed during the attack. However, the time complexity of the attack is determined by the square root of the number of keys, $\\sqrt{N}$, which may or may not be efficient depending on the specific scenario.\n\nEach of these scenarios presents different trade-offs in terms of preprocessing time, memory usage, and time taken to conduct the attack. The choice of which approach to take will depend on various factors such as available resources, time constraints, and the specific characteristics of the target system being attacked. ///Answer: B"}
{"subject": "", "question": "Question: Which statement about Singular Value Decomposition (SVD) is true?\n\nOptions:\nA. The original matrix M can be reconstructed from the matrices K and D computed by the SVD\nB. SVD, after eliminating the largest singular values, can be interpreted as a least square approximation to the original matrix\nC. The singular values can be computed from the Eigenvalues of the original matrix M\nD. SVD can be computed by constructing Eigenvectors of matrices derived from the original matrix M\n", "answer": "Explanations: The correct statement about Singular Value Decomposition (SVD) is D: SVD can be computed by constructing Eigenvectors of matrices derived from the original matrix M.\n\nSVD is a factorization method in linear algebra that decomposes a matrix into three matrices - U, \u03a3, and V*. The singular values in \u03a3 are related to the Eigenvalues of the original matrix M, but they are not directly computed from them.\n\nTo compute SVD, we need to construct the Eigenvectors of matrices derived from M, such as M^T*M or M*M^T, and then use these Eigenvectors to form the matrices U and V in the SVD decomposition.\n\nTherefore, statement D is the correct description of how SVD is computed. ///Answer: D"}
{"subject": "", "question": "Question: The number of prime numbers in $\\{2,3,\\dots ,N\\}$ when $N$ increases towards the infinity tends to\\dots?\n\nOptions:\nA. $\\log N$.\nB. $N/2$.\nC. $\\sqrt{N}$.\nD. $\\frac{N}{\\log N}$.\n", "answer": "Explanations: As $N$ increases towards infinity, the number of prime numbers in the set $\\{2, 3, \\ldots, N\\}$ tends to $\\frac{N}{\\log N}$.\n\nExplanation:\n\n- The Prime Number Theorem states that the asymptotic density of prime numbers among the natural numbers approaches $\\frac{1}{\\log N}$ as $N$ tends to infinity.\n  \n- The number of primes up to a given limit $N$ can be estimated using the Prime Number Theorem, which approximates the density of primes.\n  \n- The prime counting function $\\pi(N)$ represents the number of primes less than or equal to $N$.\n  \n- The Prime Number Theorem states that $\\pi(N)$ is asymptotically equivalent to $\\frac{N}{\\log N}$ as $N$ approaches infinity.\n  \nTherefore, as $N$ increases towards infinity, the number of prime numbers in the set $\\{2, 3, \\ldots, N\\}$ tends to $\\frac{N}{\\log N}$. ///Answer: D"}
{"subject": "", "question": "Question: (Bayes Nets) We are given a Bayes net involving the variables $X_{1}, \\cdots, X_{n}$. We determine, using our standard rules, that $X_{1} \\perp X_{2} \\mid X_{3}$. Assume now that you delete some edges in the original Bayes net. For the modified Bayes net, is it always true that $X_{1} \\perp X_{2} \\mid X_{3}$ ?\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: It is not always true that $X_{1} \\perp X_{2} \\mid X_{3}$ holds in the modified Bayes net after deleting some edges.\n\nThe statement $X_{1} \\perp X_{2} \\mid X_{3}$ in a Bayesian network means that $X_{1}$ is conditionally independent of $X_{2}$ given $X_{3}$. In the original Bayes net, this independence holds because $X_{1}$ and $X_{2}$ are d-separated by $X_{3}$, meaning that there are no active paths between $X_{1}$ and $X_{2}$ once $X_{3}$ is observed.\n\nHowever, when we delete some edges in the original Bayes net to create a modified Bayes net, we can introduce new active paths between $X_{1}$ and $X_{2}$. These new active paths can violate the conditional independence relationship $X_{1} \\perp X_{2} \\mid X_{3}$.\n\nTherefore, in the modified Bayes net, after deleting some edges, it is not always true that $X_{1} \\perp X_{2} \\mid X_{3}$. The conditional independence relationship may not hold due to the introduction of new active paths between $X_{1}$ and $X_{2}$.\n\nIn summary, the conditional independence relationships in a Bayesian network are determined by the network structure, and deleting edges can alter these relationships by introducing new active paths. ///Answer: B"}
{"subject": "", "question": "Question: In JOS, after finishing the execution of a user-level page fault handler, how is the program control flow transferred back to the program? (You may get insights from the code snippet of _pagefault_upcall.)?\n\nOptions:\nA. The control flow will be transferred to kernel first, then to Env that caused the page fault.\nB. The control flow will be transferred to Env that caused the page fault directly.\n", "answer": "Explanations: After finishing the execution of a user-level page fault handler in JOS, the program control flow is transferred back to the program by first transferring control to the kernel, which then redirects the flow back to the specific Env that caused the page fault. This two-step process ensures proper handling of the page fault at the kernel level before returning control to the user-level program. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion. Assume that $C$ is an arbitrary random permutation.\n\nOptions:\nA. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty}(C)$\nB. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\mathsf{Dec}^{n/2}_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty}(C)$\nC. $E(\\mathsf{DP}^{C}(a,b)) < \\frac{1}{2}$\nD. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\cdot\\right|\\right|_a}(C)$\n", "answer": "Explanations: To solve this question, we need to understand the concepts of best-case adversary advantage and computational indistinguishability.\n\nFirst, let's define the relevant concepts:\n1. **BestAdv**: This stands for the best-case adversary advantage, which measures how well an adversary can distinguish between a distribution generated by a challenger $C$ and a distribution generated by an ideal functionality $C^*$. It is denoted as $\\mathsf{BestAdv}_n(C,C^*)$.\n2. **Dec**: This represents a decision algorithm that aims to distinguish between two distributions. It is denoted as $\\mathsf{Dec}^n_{\\left|\\left|\\cdot\\right|\\right|}(C)$, where $\\left|\\left|\\cdot\\right|\\right|$ is a norm representing the statistical distance or computational distance between distributions.\n3. **DP**: This refers to the distinguishing advantage of an algorithm $\\mathsf{DP}$ in distinguishing distributions $a$ and $b$, denoted as $E(\\mathsf{DP}^{C}(a,b))$.\n4. **Computational Indistinguishability**: Two distributions are computationally indistinguishable if no efficient algorithm can distinguish between them with a non-negligible advantage.\n\nGiven the context provided in the propositions, we are asked to identify the correct assertion related to the properties of a random permutation $C$.\n\nLet's analyze each proposition in the context of a random permutation $C$:\n\n1. **$\\mathsf{BestAdv}_n(C,C^*)=\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty}(C)$**:\n   - This proposition equates the best-case adversary advantage to a decision algorithm based on the infinity norm. For a random permutation $C$, this assertion may not hold in general as the best-case advantage may not always be directly related to the infinity norm.\n\n2. **$\\mathsf{BestAdv}_n(C,C^*)=\\mathsf{Dec}^{n/2}_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty}(C)$**:\n   - This proposition is similar to the first one but suggests halving the decision parameter. Again, for a random permutation, this may not always hold true.\n\n3. **$E(\\mathsf{DP}^{C}(a,b)) < \\frac{1}{2}$**:\n   - This proposition states that the distinguishing advantage of an algorithm $\\mathsf{DP}$ between distributions $a$ and $b$ is less than $\\frac{1}{2}$. For a random permutation $C$, this could be a valid assertion since random permutations are designed to be indistinguishable from ideal functionalities.\n\n4. **$\\mathsf{BestAdv}_n(C,C^*)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\cdot\\right|\\right|_a}(C)$**:\n   - This proposition relates the best-case adversary advantage to a decision algorithm based on a norm $a$. For a random permutation $C$, this assertion may or may not hold depending on the specific norm $a$ chosen.\n\nBased on the properties of random permutations and the concepts of best-case adversary advantage, decision algorithms, and computational indistinguishability, the most appropriate and likely correct assertion for a random permutation $C$ is:\n- **$E(\\mathsf{DP}^{C}(a,b)) < \\frac{1}{2}$**\n\nThis choice aligns with the idea that random permutations should be indistinguishable from ideal functionalities, leading to a distinguishing advantage below $\\frac{1}{2}$. ///Answer: C"}
{"subject": "", "question": "Question: Select the statements that are true.A penalty will be applied to any incorrect answers selected.\n\nOptions:\nA. Information retrieval is the selection of documents relevant to a query from an unstructured collection of documents.\nB. Different IR systems can differ in the way they represent documents, represent queries, and define the relevance measure between documents and queries.\nC. The vector space model represents documents as vectors derived from the distribution of indexing terms in the document.\nD. The dimensionality of the vector space does not depend on the size of the indexing vocabulary.\nE. Use of filters during indexing results in less informative indexes.\n", "answer": "Explanations: 1. True - Information retrieval does involve selecting documents relevant to a query from an unstructured collection of documents. This is the basic premise of any IR system.\n\n2. True - Different IR systems can indeed differ in the way they represent documents, represent queries, and define the relevance measure between documents and queries. This is because there are various approaches and algorithms that can be used in information retrieval.\n\n3. True - The vector space model is a widely used model in information retrieval where documents are represented as vectors derived from the distribution of indexing terms in the document. This allows for similarity calculations between documents and queries.\n\n4. False - The dimensionality of the vector space does depend on the size of the indexing vocabulary. The more terms in the vocabulary, the higher the dimensionality of the vector space.\n\n5. False - Use of filters during indexing can actually result in more informative indexes. Filters can help in removing noise or irrelevant information, thus improving the quality of the index and ultimately leading to more relevant search results.\n\nIn conclusion, the true statements are 1, 2, and 3, while statements 4 and 5 are false. It is important to understand the nuances of information retrieval systems and the various techniques used in order to effectively select the correct statements. ///Answer: A"}
{"subject": "", "question": "Question: Let $H:\\{0,1\\}^* \\rightarrow \\{0,1\\}^n$ be a hash function and $x_1,x_2\\in\\{0,1\\}^{2n}$ two random different messages. In cryptography, we usually assume that the probability of collision, i.e. $\\Pr[H(x_1)=H(x_2)]$, is close to \\ldots?\n\nOptions:\nA. $2^{-\\frac{n}{2}}$.\nB. $2^{-n}$.\nC. $2^{-2n}$.\nD. $0$.\n", "answer": "Explanations: To determine the probability of a collision for a hash function, where $H: \\{0,1\\}^* \\rightarrow \\{0,1\\}^n$, when considering two different random messages $x_1, x_2 \\in \\{0,1\\}^{2n}$, we can analyze the possible outcomes.\n\nLet's calculate the probability of a collision occurring between $x_1$ and $x_2$:\n\nThe total number of possible hash values in $\\{0,1\\}^n$ is $2^n$. Since both $x_1$ and $x_2$ are random and different messages, each hash value in $\\{0,1\\}^n$ has an equal chance of being associated with $x_1$ or $x_2$.\n\nTherefore, the probability of a collision happening between $x_1$ and $x_2$ is the probability that the hash of $x_1$ is equal to the hash of $x_2, which is the ratio of the number of collisions to the total number of possible outcomes.\n\nThe probability of a collision is given by:\n\\[ \\Pr[H(x_1) = H(x_2)] = \\frac{\\text{Number of Collisions}}{\\text{Total Number of Possible Outcomes}} = \\frac{1}{2^n}. \\]\n\nGiven the choices provided:\n\nA. $2^{-\\frac{n}{2}}$: This probability is not accurate for the collision probability between two different random messages in the context of hash functions.\n\nB. $2^{-n}$: This choice accurately represents the probability of a collision between two different random messages with hash functions where each hash value is equally likely.\n\nC. $2^{-2n}$: This probability overestimates the likelihood of a collision between two different random messages in the given context.\n\nD. $0$: This choice is incorrect as there is a non-zero probability of a collision between two different random messages in hash functions.\n\nTherefore, the correct answer is:\n\nB. $2^{-n}$.\n\nThis choice accurately represents the probability of a collision between two different random messages $x_1$ and $x_2$ hashed by a hash function $H$ with an output size of $n$ bits. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following is/are typical AddressSanitizer use cases?\n\nOptions:\nA. Out-of-bounds violations visibility increase during fuzzing\nB. Use-after-free mitigation in production\nC. Uninitialized variable detection during unit tests\nD. NULL-pointer dereference detection during fuzzing\n", "answer": "Explanations: I have chosen options 0) Out-of-bounds violations visibility increase during fuzzing and 3) NULL-pointer dereference detection during fuzzing.\n\nAddressSanitizer is a tool used for detecting memory errors such as out-of-bounds violations, use-after-free, uninitialized variables, and NULL-pointer dereference. It is commonly used during fuzzing to increase visibility of memory errors and detect vulnerabilities. ///Answer: A"}
{"subject": "", "question": "Question: Thang, Jeremie and Tugrulcan have built their own search engines. For a query Q, they got precision scores of 0.6, 0.7, 0.8 \u00a0respectively. Their F1 scores (calculated by same parameters) are same. Whose search engine has a higher recall on Q?\n\nOptions:\nA. Thang\nB. Jeremie\nC. Tugrulcan\nD. We need more information\n", "answer": "Explanations: To determine which search engine has a higher recall on query Q, we need to consider the precision and F1 scores of each search engine. Recall is calculated as the ratio of true positive results to the sum of true positive and false negative results.\n\nGiven that the F1 scores are the same for all three search engines, we can conclude that the harmonic mean of precision and recall is equal for each. This implies that the recall and precision values are inversely proportional to each other.\n\nSince Thang has the lowest precision score of 0.6, he is likely to have the highest recall among the three search engines. Therefore, the answer is option 0) Thang. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following encryption schemes is deterministic?\n\nOptions:\nA. RSA-OAEP\nB. Plain ElGamal\nC. Plain Rabin\nD. PKCS\\#1.5\n", "answer": "Explanations: A deterministic encryption scheme is one where the same plaintext message will always result in the same ciphertext when encrypted multiple times with the same key. \n\nOut of the options provided: \n- RSA-OAEP (Optimal Asymmetric Encryption Padding) is a probabilistic encryption scheme, meaning it is not deterministic.\n- Plain ElGamal is also a probabilistic encryption scheme, so it is not deterministic.\n- Plain Rabin encryption is deterministic because it directly applies a mathematical function to the plaintext without any additional randomness.\n- PKCS#1.5 (Public Key Cryptography Standards 1.5) is a padding scheme used with RSA encryption, and it includes random padding to prevent certain attacks, making it a probabilistic encryption scheme and not deterministic.\n\nTherefore, the encryption scheme that is deterministic out of the options provided is Plain Rabin. ///Answer: C"}
{"subject": "", "question": "Question: Which MAC construction is NOT based on the CBC mode?\n\nOptions:\nA. HMAC\nB. EMAC\nC. OMAC\nD. ISO/IEC 9797\n", "answer": "Explanations: The MAC (Message Authentication Code) construction that is NOT based on the CBC (Cipher Block Chaining) mode is option d) ISO/IEC 9797.\n\nExplanation:\n1. HMAC (Hash-based Message Authentication Code): HMAC is a widely used MAC construction that is based on a cryptographic hash function, not the CBC mode. It involves combining a secret key with the message to generate a hash value that provides authentication and integrity.\n2. EMAC (Encrypt-and-MAC): EMAC is a MAC construction that involves encrypting the message with a block cipher and then deriving the MAC value from the encrypted message. It is also not based on the CBC mode.\n3. OMAC (One-key MAC): OMAC is a family of MAC constructions that use a single key for both encryption and authentication. Like HMAC and EMAC, OMAC is not based on the CBC mode.\n4. ISO/IEC 9797: ISO/IEC 9797 is a standard that specifies various modes of operation for block ciphers, including the Electronic Codebook (ECB) mode and the Cipher Feedback (CFB) mode, but not the CBC mode. Therefore, ISO/IEC 9797 is the MAC construction that is NOT based on the CBC mode.\n\nIn conclusion, among the options provided, the MAC construction that is NOT based on the CBC mode is ISO/IEC 9797. ///Answer: D"}
{"subject": "", "question": "Question: When constructing a word embedding, what is TRUE regarding negative samples?\n\nOptions:\nA. They are oversampled if less frequent\nB. Their frequency is decreased down to its logarithm\nC. They are words that do not appear as context words\nD. They are selected among words that are not stop-words\n", "answer": "Explanations: Among the options provided, the correct statement regarding negative samples when constructing a word embedding is:\n\n- They are words that do not appear as context words\n\nNegative samples are words that are not considered as context words in the skip-gram model used for training word embeddings. In the skip-gram model, the objective is to predict context words given a target word, so negative samples are words that are not in the context of the target word. This helps the model to learn the relationship between the target word and words that do not co-occur with it in the specified context window.\n\nTherefore, the statement \"They are words that do not appear as context words\" is the true statement regarding negative samples in the context of word embeddings. ///Answer: C"}
{"subject": "", "question": "Question: In JOS and x86, please select all valid options for a system call.\n\nOptions:\nA. A system call is for handling interrupts like dividing zero error and page fault.\nB. In user mode, before and after a system call instruction(such as int 0x30), the stack pointer(esp in x86) stays the same.\nC. During the execution of a system call, when transfering from user mode to kernel mode, the stack pointer(esp in x86) stays the same.\n", "answer": "Explanations: Answer: B. In user mode, before and after a system call instruction(such as int 0x30), the stack pointer(esp in x86) stays the same.\n\nExplanation: When a system call is executed, the CPU switches from user mode to kernel mode to access privileged instructions and resources. In x86 architecture, the int 0x30 instruction is used to trigger a software interrupt for system calls. During this transition, the stack pointer (esp register) remains the same before and after the system call instruction. This is because the stack pointer is used to store the return address and other information necessary for the system call, and it does not need to be changed during this process. Therefore, option B is a valid option for a system call in JOS and x86. ///Answer: B"}
{"subject": "", "question": "Question: Assume a user program executes following tasks. Select all options that will use a system call.\n\nOptions:\nA. Read the user's input \"Hello world\" from the keyboard.\nB. Write \"Hello world\" to a file.\nC. Encrypt \"Hello world\" by AES.\nD. Send \"Hello world\" to another machine via Network Interface Card.\n", "answer": "Explanations: Option 1: Read the user's input \"Hello world\" from the keyboard.\nOption 2: Write \"Hello world\" to a file.\nOption 4: Send \"Hello world\" to another machine via Network Interface Card. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is \\textbf{not} a finite field.\n\nOptions:\nA. The set of integers modulo $p$, where $p$ is a prime number.\nB. $Z_2[X]/(X^2+X+1)$.\nC. The elliptic curve $E_{2,1}$ over $GF(7)$.\nD. $GF(p^n)$, where $p$ is a prime number and $n$ a positive integer.\n", "answer": "Explanations: B: $Z_2[X]/(X^2+X+1)$ is not a finite field. \n\nA finite field is a field that has a finite number of elements. Option A, the set of integers modulo $p$ where $p$ is a prime number, is a finite field because it has $p$ elements. Option C, the elliptic curve $E_{2,1}$ over $GF(7)$, is also a finite field because it has $7$ elements. Option D, $GF(p^n)$, where $p$ is a prime number and $n$ is a positive integer, is a finite field because it has $p^n$ elements.\n\nHowever, option B, $Z_2[X]/(X^2+X+1)$, is not a finite field because the polynomial $X^2+X+1$ is not irreducible over $Z_2$. In a finite field, the field polynomial must be irreducible, but in this case, $X^2+X+1$ can be factored as $(X+1)(X+1)$ in $Z_2$, so it is not irreducible. ///Answer: B:"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion regarding the Diffie-Hellman key exchange?\n\nOptions:\nA. It is secure under a passive adversary.\nB. It is secure when working in a subgroup of $\\mathbb{Z}_p^*$ of big prime order.\nC. It is secure under an active Man-In-The-Middle attack.\nD. It is not secure when working in $\\mathbb{Z}_n$ with $n$ having only small prime factors.\n", "answer": "Explanations: The incorrect assertion regarding the Diffie-Hellman key exchange is:\n\n3. It is secure under an active Man-In-The-Middle attack.\n\nIn Diffie-Hellman key exchange, two parties can securely agree on a shared secret over an insecure channel. However, it is vulnerable to Man-In-The-Middle (MITM) attacks if proper precautions are not taken. An active MITM attacker can intercept the communication between the two parties, establish separate key exchanges with each party, and then relay the messages between the parties without their knowledge. This way, the attacker can obtain the shared secret key without either party realizing it.\n\nTo prevent MITM attacks in Diffie-Hellman key exchange, additional measures such as authentication and key verification are needed. One common solution is to use digital signatures or certificates to ensure the authenticity of the exchanged keys. By verifying the identity of the communicating parties, MITM attacks can be mitigated.\n\nUnderstanding the theory behind Diffie-Hellman key exchange is crucial to correctly identify the incorrect assertion in the given options. Diffie-Hellman relies on the discrete logarithm problem, where it is computationally difficult to determine the shared secret key even if the exchanged public parameters are known. The security of Diffie-Hellman key exchange depends on the proper implementation and protection against potential attacks, such as MITM attacks. ///Answer: C"}
{"subject": "", "question": "Question: In a bruteforce attack on $N$ keys, when the distribution $D$ of the keys is \\emph{known} and when \\emph{all keys have a non-zero probability to occur},\\dots?\n\nOptions:\nA. the best strategy is to try each key uniformly at random.\nB. the best strategy is to sort each key by decreasing order of likelihood and try them.\nC. the \\emph{worst} case complexity depends on the distribution $D$.\nD. the \\emph{expected} complexity when using the \\emph{best strategy} is $(N+1)/2$ for all distributions $D$.\n", "answer": "Explanations: The correct option is (2) the best strategy is to sort each key by decreasing order of likelihood and try them. \n\nWhen the distribution of the keys is known and all keys have a non-zero probability to occur, the best strategy is to first try the keys that are most likely to occur. By sorting the keys in decreasing order of likelihood, we increase the chances of finding the correct key earlier in the brute force attack. This approach can potentially reduce the number of keys that need to be tried before finding the correct one, leading to a more efficient search process.\n\nTrying each key uniformly at random (option 1) may not be the most efficient strategy when the distribution of the keys is known, as it does not take into account the likelihood of each key occurring.\n\nThe worst case complexity does depend on the distribution D (option 3), as the distribution can impact the order in which keys should be tried in order to minimize the number of attempts needed.\n\nThe expected complexity when using the best strategy is not always (N+1)/2 for all distributions D (option 4). The expected complexity can vary depending on the specific distribution of the keys and the order in which they are tried. Sorting the keys by likelihood provides a more tailored approach that can improve the expected complexity in many cases. ///Answer: B"}
{"subject": "", "question": "Question: Select the likelihood of the following word to belong to a given language L if a language identification system that uses 3-grams of characters is used.\u00a0bubbleConsider the following probabilities when doing the computation:\\( P(\\text{bu|L})= 6 \\times 10^{-2} \\)\\(\u00a0P(\\text{b|ub,L})= 4 \\times 10^{-2}\u00a0\\)\\( P(\\text{bub|L})= 2 \\times 10^{-2} \\)\\(\u00a0P(\\text{b|bub,L})= 7 \\times 10^{-2}\u00a0\\)\\(\u00a0P(\\text{l|ubb,L})= 1 \\times 10^{-4}\u00a0\\)\\(\u00a0P(\\text{l|bb,L})= 3 \\times 10^{-4}\u00a0\\)\\(\u00a0P(\\text{e|bbl,L})= 8 \\times 10^{-3}\u00a0\\)\\(\u00a0P(\\text{e|bl,L})= 5 \\times 10^{-3} \\)?\n\nOptions:\nA. \\( 2\u00a0\\times\u00a04\u00a0\\times 3\u00a0\\times 5\u00a0\\times 10^{-11}\u00a0 \\)\nB. \\( 2\u00a0\\times\u00a07\u00a0\\times 1\u00a0\\times 5\u00a0\\times 10^{-11}\u00a0 \\)\nC. \\( 6 \\times\u00a04\u00a0\\times 3\u00a0\\times 8\u00a0\\times 10^{-11}\u00a0 \\)\nD. \\( 6\u00a0\\times\u00a07\u00a0\\times 1 \\times 8 \\times 10^{-11}\u00a0 \\)\n", "answer": "Explanations: To calculate the likelihood of the word \"bubble\" belonging to language L using a language identification system that uses 3-grams of characters, we need to multiply the probabilities of each consecutive 3-gram appearing in the word.\n\nGiven the probabilities provided:\n- \\( P(\\text{bu|L}) = 6 \\times 10^{-2} \\)\n- \\( P(\\text{b|ub,L}) = 4 \\times 10^{-2} \\)\n- \\( P(\\text{bub|L}) = 2 \\times 10^{-2} \\)\n- \\( P(\\text{b|bub,L}) = 7 \\times 10^{-2} \\)\n- \\( P(\\text{l|ubb,L}) = 1 \\times 10^{-4} \\)\n- \\( P(\\text{l|bb,L}) = 3 \\times 10^{-4} \\)\n- \\( P(\\text{e|bbl,L}) = 8 \\times 10^{-3} \\)\n- \\( P(\\text{e|bl,L}) = 5 \\times 10^{-3} \\)\n\nWe need to find the product of the probabilities of each 3-gram in the word \"bubble\":\n\\( 6 \\times 4 \\times 2 \\times 7 \\times 3 \\times 5 \\times 8 \\times 5 \\times 10^{-11} = 2 \\times 4 \\times 3 \\times 5 \\times 10^{-11} \\)\n\nTherefore, the correct answer is: \\( 2 \\times 4 \\times 3 \\times 5 \\times 10^{-11} \\) ///Answer: A"}
{"subject": "", "question": "Question: What is $\\lambda (91)$, i.e. the exponent of $\\mathbb{Z}_{91}^*$?\n\nOptions:\nA. 90\nB. 12\nC. 72\nD. 6\n", "answer": "Explanations: To find the exponent of $\\\\mathbb{Z}_{91}^*$, we need to calculate $\\\\lambda(91)$, which represents the order of the group $\\\\mathbb{Z}_{91}^*$. \\n\\nFirst, let\\'s decompose $91$ into its prime factors: $91 = 7 \\\\times 13$. Since $7$ and $13$ are distinct primes, we can calculate $\\\\lambda(91)$ using the formula for the Carmichael function $\\\\lambda(n)$.\\n\\nThe Carmichael function $\\\\lambda(n)$ is defined as the smallest positive integer $k$ such that $a^k \\\\equiv 1 \\\\pmod{n}$ for all integers $a$ coprime to $n$. \\n\\nFor a number $n$ of the form $n = p_1^{a_1} \\\\cdot p_2^{a_2} \\\\cdot \\\\ldots \\\\cdot p_k^{a_k}$, where $p_1, p_2, \\\\ldots, p_k$ are distinct primes, $\\\\lambda(n)$ is given by:\\n$\\\\lambda(n) = \\\\text{lcm}(\\\\lambda(p_1^{a_1}), \\\\lambda(p_2^{a_2}), \\\\ldots, \\\\lambda(p_k^{a_k}))$.\\n\\nNow, we calculate $\\\\lambda(7)$ and $\\\\lambda(13)$. Since both $7$ and $13$ are prime numbers, we have:\\n$\\\\lambda(7) = 6$ and $\\\\lambda(13) = 12$.\\n\\nTherefore, $\\\\lambda(91) = \\\\text{lcm}(\\\\lambda(7), \\\\lambda(13)) = \\\\text{lcm}(6, 12) = 12$.\\n\\nHence, the correct answer is $\\\\boxed{1: \"12\"}$. ///Answer: A"}
{"subject": "", "question": "Question: The type statement in RDF would be expressed in the relational data model by a table?\n\nOptions:\nA. with one attribute\nB. with two attributes\nC. with three attributes\nD. cannot be expressed in the relational data model\n", "answer": "Explanations: In RDF (Resource Description Framework), a triple consists of a subject, predicate, and object. This can be represented as a statement in the form of (subject, predicate, object). \n\nIn the relational data model, data is organized into tables with rows and columns. Each row represents a record, and each column represents an attribute.\n\nFor a type statement in RDF, it would require at least three attributes to represent the subject, predicate, and object. Therefore, the correct answer is:\n\n3 with three attributes\n\nThis is because each attribute would represent one part of the triple (subject, predicate, object) in the RDF type statement. So, in the relational data model, a table with three attributes would be needed to accurately represent the type statement in RDF. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion.\n\nOptions:\nA. Commitment schemes never use randomness.\nB. A commitment scheme can be based on the hardness of the discrete logarithm problem.\nC. A commitment scheme should be hiding and binding.\nD. Perdersen Commitment uses two large primes.\n", "answer": "Explanations: The incorrect assertion is:\n\nOption 0: Commitment schemes never use randomness.\n\nExplanation: Commitment schemes often rely on randomness to achieve their security properties, such as hiding the committed value and preventing the committer from changing their commitment after it has been made. ///Answer: A"}
{"subject": "", "question": "Question: Select which statements are true about the CYK algorithm.A penalty will be applied for any incorrect answers.\n\nOptions:\nA. It is a top-down chart parsing algorithm.\nB. Its time complexity is \\( O(n^3) \\), where\u00a0\\( n \\) is the length of sequence of words to be parsed.\nC. Its time complexity decreases when the grammar is regular.\nD. The Context-Free Grammar used with the CYK algorithm has to be converted into extended Chomsky normal form.\nE. It not only generates the syntactic interpretations of the sequence to be analyzed but also generates the syntactic interpretations of all the sub-sequences of the sequence to be analyzed.\n", "answer": "Explanations: 1. True - The CYK algorithm is a top-down chart parsing algorithm.\n2. True - The time complexity of the CYK algorithm is \\( O(n^3) \\), where \\( n \\) is the length of the sequence of words to be parsed.\n3. False - The time complexity of the CYK algorithm does not decrease when the grammar is regular.\n4. True - The Context-Free Grammar used with the CYK algorithm needs to be converted into extended Chomsky normal form.\n5. True - The CYK algorithm generates syntactic interpretations of all sub-sequences of the sequence to be analyzed. \n\nOverall, statements 1, 2, 4, and 5 are true. The penalty will be applied for any incorrect answers. ///Answer: B"}
{"subject": "", "question": "Question: Alice has been working on a classification problem, and has been using the binary cross-entropy loss function, defined as: $\\mathcal{L}_{\\text{BCE}}(\\mathbf{y}, \\mathbf{\\hat{y}})=- \\frac{1}{N}\\sum^{N}_{i=1} y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1- \\hat{y}^{(i)})$.\\\\\n\n    Despite trying many models, she hasn't been able to reduce the training loss. Her friend Frank suggests using a new loss function he invented, which he proudly calls the \"Frank Loss\", defined as: $\\mathcal{L}_{\\text{Frank}}(\\mathbf{y}, \\mathbf{\\hat{y}})= -e^{-1} + \\mathcal{L}_{\\text{BCE}}(\\mathbf{y}, \\mathbf{\\hat{y}})$.\n    After switching to the Frank loss, Alice notices that the training loss is immediately lower! How will that affect the training accuracy?\n\nOptions:\nA. The training accuracy will increase.\nB. The training accuracy will decrease.\nC. The training accuracy will stay the same.\nD. It is impossible to say without more information.\n", "answer": "Explanations: The training accuracy will stay the same. \n\nExplanation: The Frank Loss is simply the binary cross-entropy loss function with a constant term added to it. Adding a constant term to the loss function does not change the overall optimization landscape, so the minimum of the loss function will remain the same. Therefore, the training accuracy will stay the same. ///Answer: C"}
{"subject": "", "question": "Question: Assume Alice and Bob are exchanging messages that are encrypted with the Vernam cipher. Their communication therefore necessarily has \\dots?\n\nOptions:\nA. confidentiality only.\nB. confidentiality and integrity.\nC. confidentiality, integrity and authenticity.\nD. none of the mentioned properties.\n", "answer": "Explanations: To determine the properties of the Vernam cipher, let's break it down step by step:\n1. The Vernam cipher is a symmetric key encryption method that uses a one-time pad.\n2. It provides confidentiality because the encryption key (one-time pad) is used to securely encrypt the message.\n3. It does not inherently provide integrity or authenticity checks because it focuses solely on encrypting the message with the one-time pad.\n\nBased on the properties of the Vernam cipher, the correct answer is: 'confidentiality only.' ///Answer: A"}
{"subject": "", "question": "Question: Let $E$ be a finite ground set and let $\\mathcal{I}$ be a family of ground sets. Which of the following definitions of $\\mathcal{I}$ guarantees that $M = (E, \\mathcal{I})$ is a matroid? \\begin{enumerate} \\item $E$ is the edges of an undirected bipartite graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is an acyclic edge set}\\}$. \\item $E$ is the edges of an undirected graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is an acyclic edge set}\\}$. \\item $E$ is the edges of an undirected bipartite graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is a matching}\\}$. \\item $E$ is the edges of an undirected  graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is a matching}\\}$. \\item $E = \\{1, 2, \\ldots, n\\}$ is the set of indices of  vectors $v_1, \\ldots, v_n \\in \\mathbb{R}^d$ and \\\\$\\mathcal{I} = \\{X \\subseteq E : \\mbox{the vectors $\\{v_i : i \\in X\\}$ are linearly \\emph{dependent}}\\}$. \\item $E = \\{1, 2, \\ldots, n\\}$ is the set of indices of  vectors $v_1, \\ldots, v_n \\in \\mathbb{R}^d$ and \\\\$\\mathcal{I} = \\{X \\subseteq E : \\mbox{the vectors $\\{v_i : i \\in X\\}$ are linearly \\emph{independent}}\\}$. \\end{enumerate} The definitions of $\\mathcal{I}$ that guarantees that $M = (E, \\mathcal{I})$ is a matroid are:?\n\nOptions:\nA. (a), (c),  (f)\nB. (a),  (b), (c), (d), (f)\nC. (a),  (b), (c), (f)\nD. (a),  (b), (e)\nE. (a), (c), (d), (f)\nF. (a),  (b), (c), (d), (e)\nG. (a), (c), (d), (e)\nH. (a),  (f)\nI. (a),  (b), (c), (e)\nJ. (a),  (b), (f)\nK. (a), (c),  (e)\nL. (a),  (e)\n", "answer": "Explanations: Answer: \n\nF) (a), (b), (c), (d), (e)\n\nExplanation: \n\nTo determine which definitions of $\\mathcal{I}$ guarantee that $M = (E, \\mathcal{I})$ is a matroid, we need to check if the given properties hold for a matroid:\n\n1. **Non-emptiness**: The empty set is always in $\\mathcal{I}$.\n2. **Hereditary property**: If $X \\in \\mathcal{I}$ and $Y \\subseteq X$, then $Y \\in \\mathcal{I}$.\n3. **Exchange property**: If $A, B \\in \\mathcal{I}$ with $|A| < |B|$, then there exists $x \\in B \\setminus A$ such that $A \\cup \\{x\\} \\in \\mathcal{I}$.\n\nLet's evaluate each option:\n\n(a) $E$ is the edges of an undirected bipartite graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is an acyclic edge set}\\}$.\n- This definition satisfies all three properties of a matroid. An acyclic edge set in a bipartite graph is equivalent to a forest, which is the property of a matroid.\n\n(b) $E$ is the edges of an undirected graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is an acyclic edge set}\\}$.\n- This definition also satisfies all three properties of a matroid. An acyclic edge set in a general graph is a forest, which is the property of a matroid.\n\n(c) $E$ is the edges of an undirected bipartite graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is a matching}\\}$.\n- This definition satisfies all three properties of a matroid. A matching in a bipartite graph satisfies the hereditary and exchange properties.\n\n(d) $E$ is the edges of an undirected graph and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{$X$ is a matching}\\}$.\n- This definition also satisfies all three properties of a matroid. A matching in a general graph satisfies the hereditary and exchange properties.\n\n(e) $E = \\{1, 2, \\ldots, n\\}$ is the set of indices of vectors $v_1, \\ldots, v_n \\in \\mathbb{R}^d$ and $\\mathcal{I} = \\{X \\subseteq E : \\mbox{the vectors $\\{v_i : i \\in X\\}$ are linearly independent}\\}$.\n- This definition satisfies all three properties of a matroid. Linear independence of vectors satisfies the hereditary and exchange properties.\n\nTherefore, options (a), (b), (c), (d), and (e) all define $\\mathcal{I}$ in a way that guarantees $M = (E, \\mathcal{I})$ is a matroid. ///Answer: F"}
{"subject": "", "question": "Question: Assume we work with $64$-bit block cipher. In differential cryptanalysis, for which of the following circuits is the output difference going to be different from the input difference?\n\nOptions:\nA. a NOT gate\nB. a XOR to a constant gate\nC. a circuit which rotates by $64$ positions to the left\nD. a linear circuit\n", "answer": "Explanations: In differential cryptanalysis, it is essential to understand the behavior of different types of circuits when input differences are applied. Let's analyze the options provided in the context of a 64-bit block cipher:\n\n1. NOT gate:\n   - A NOT gate simply flips the bits of the input. When an input difference is applied to a NOT gate, the output difference will be the complement of the input difference. As a result, the output difference is different from the input difference. Therefore, the output difference will be different from the input difference for a NOT gate.\n\n2. XOR to a constant gate:\n   - An XOR operation with a constant works by flipping specific bits based on the constant value. When an input difference is XORed with a constant, the output difference will depend on the constant and may or may not be the same as the input difference. For certain constant values, the output difference will be different from the input difference.\n\n3. Circuit which rotates by 64 positions to the left:\n   - A rotation by 64 positions to the left on a 64-bit block is effectively a circular shift where bits are rotated to the left with wrap-around. When an input difference is applied and the bits are rotated by 64 positions to the left, the pattern of differences is preserved because it is a circular shift. Therefore, the output difference will be the same as the input difference for a left rotation by 64 positions.\n\n4. Linear circuit:\n   - Linear operations in a cryptographic circuit do not introduce non-linearity. Therefore, applying an input difference through a linear circuit results in an output difference that is the same as the input difference. Linear circuits preserve input differences and do not introduce differences in the output.\n\nIn conclusion, for the given options:\n- Option 1: a NOT gate will have a different output difference from the input difference.\n- Option 2: a XOR to a constant gate may have a different output difference depending on the constant.\n- Option 3: a circuit which rotates by 64 positions to the left will have the same output difference as the input difference.\n- Option 4: a linear circuit will have the same output difference as the input difference.\n\nTherefore, the output difference will be different from the input difference for options 1 and 2, while options 3 and 4 will maintain the input-output difference relationship indicated. ///Answer: A"}
{"subject": "", "question": "Question: If you write \"hello\" to a file in a JOS file system. Right after the write operation, the computer crashes. Is the content \"hello\" persisted (or written) on the disk?\n\nOptions:\nA. Yes\nB. No\n", "answer": "Explanations: No. In a JOS file system, the data is only guaranteed to be persisted on the disk once the file system has flushed the data to the disk. If the computer crashes immediately after the write operation, it is likely that the data \"hello\" has not been flushed to the disk and therefore not persisted. This is due to the fact that the write operation may have only been completed in memory and not yet written to the disk before the crash occurred. ///Answer: B"}
{"subject": "", "question": "Question: In the physical representation of an inverted file, the size of the index file is typically in the order of (where n is the number of documents):?\n\nOptions:\nA. O(log(n))\nB. O(sqrt(n))\nC. O(n)\nD. O(n2)\n", "answer": "Explanations: In the physical representation of an inverted file, the index file contains the information about the terms and their corresponding posting lists. The size of the index file in the inverted file structure depends on the number of unique terms present in the collection of documents, not just the total number of documents (n).\n\nThe size of the index file is typically in the order of O(n) where n is the number of unique terms in the collection. This is because each term needs to be represented in the index, and for each term, the corresponding posting list (containing document IDs or other information) needs to be stored.\n\nTherefore, the correct answer is: O(n) ///Answer: C"}
{"subject": "", "question": "Question: A shadow stack...\n\nOptions:\nA. offers stronger protection than stack canaries.\nB. needs to be protected against or hidden from arbitrary writes.\nC. is an exact copy of the original stack allowing for a complete integrity verification.\nD. is used by exception handling for stack trace generation.\nE. is not usable in combination with CFI.\n", "answer": "Explanations: A shadow stack is a separate stack that stores return addresses and is used as a security feature to protect against control-flow hijacking attacks. It offers stronger protection than stack canaries because it actively maintains the integrity of return addresses. It needs to be protected against or hidden from arbitrary writes to prevent attackers from tampering with the stored return addresses. It is not an exact copy of the original stack but a separate data structure used for integrity verification of return addresses. It is not typically used by exception handling for stack trace generation. However, it can be used in combination with Control Flow Integrity (CFI) to provide enhanced security protections.\n\nTherefore, the correct options are:\n- offers stronger protection than stack canaries.\n- needs to be protected against or hidden from arbitrary writes.\n- is not usable in combination with CFI. ///Answer: A"}
{"subject": "", "question": "Question: In a MAC forgery, the adversary tries to\\dots?\n\nOptions:\nA. recover the message $m$ given the tag \\textsf{MAC}_K(m).\nB. decrypt a given message.\nC. forge the secret key.\nD. compute the MAC of a message whose MAC was never computed before.\n", "answer": "Explanations: In a MAC forgery, the adversary tries to:\n- compute the MAC of a message whose MAC was never computed before. \n\nMAC forgery involves creating a valid MAC without knowing the secret key, in order to impersonate a legitimate party or tamper with the message. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. In \\emph{all} finite Abelian groups $(G,+)$ \\ldots?\n\nOptions:\nA. the order of any element divides the order of the group.\nB. all the elements are generators.\nC. the operation $+$ is commutative.\nD. the neutral element has order $1$.\n", "answer": "Explanations: Let\\'s analyze each option one by one:\\n\\n0: \"The order of any element divides the order of the group.\" - True. This is true because in a finite Abelian group, the order of an element must divide the order of the group by Lagrange\\'s theorem.\\n\\n1: \"All the elements are generators.\" - False. Not all elements in a finite Abelian group are generators. For example, in a group of order 4 with the operation being addition modulo 4, the element 2 is not a generator.\\n\\n2: \"The operation $+$ is commutative.\" - True. In all Abelian groups, the operation is commutative by definition.\\n\\n3: \"The neutral element has order 1.\" - True. The neutral element in any group has order 1 since any element raised to the power of 1 is the element itself.\\n\\nTherefore, the incorrect assertion is option 1: \"All the elements are generators. ///Answer: B"}
{"subject": "", "question": "Question: Select \\emph{incorrect} statement. Generic attacks on DES include?\n\nOptions:\nA. time memory tradeof against 2 key Triple DES.\nB. collision attack against 3 key Triple DES.\nC. meet in the middle attack against 3 key Triple DES.\nD. known plaintext attack by Van Oorschot-Wiener agains 2 key Triple DES.\n", "answer": "Explanations: The incorrect statement is Option 2: collision attack against 3 key Triple DES. \n\nExplanation:\n1. Option 1: Time memory tradeoff against 2 key Triple DES - This is a valid generic attack on DES. Time-memory tradeoff attacks involve pre-computing data in order to reduce the time required for a cryptographic attack.\n2. Option 2: Collision attack against 3 key Triple DES - This is incorrect. Collision attacks involve finding two inputs that produce the same output, but Triple DES is designed to prevent such attacks.\n3. Option 3: Meet in the middle attack against 3 key Triple DES - This is a valid generic attack on DES. Meet-in-the-middle attacks involve breaking a cryptographic algorithm by finding a collision in the middle of the encryption process.\n4. Option 4: Known plaintext attack by Van Oorschot-Wiener against 2 key Triple DES - This is a valid generic attack on DES. Known plaintext attacks involve exploiting the knowledge of plaintext-ciphertext pairs to decrypt encrypted data. ///Answer: B"}
{"subject": "", "question": "Question: Does the following code snippet contain bugs? If so, which line(s)?\n\n\\begin{lstlisting}[language=C,style=c]\nint main(int argc, char** argv) {\n    if (argc != 2) { return -1; }\n    int len = strlen(argv[1]);\n    for (int i; i < len; ++i) {\n        printf(\"%c\", argv[1][i]);\n    }\n    return 0;\n}\n\\end{lstlisting}?\n\nOptions:\nA. Line 4 has a bug / vuln\nB. There is no bug / vuln in the code snippet\nC. Line 1 has a bug / vuln\nD. Line 2 has a bug / vuln\nE. Line 3 has a bug / vuln\nF. Line 5 has a bug / vuln\nG. Line 6 has a bug / vuln\nH. Line 7 has a bug / vuln\nI. Line 8 has a bug / vuln\n", "answer": "Explanations: Starting from line 1, there doesn't seem to be any bugs related to the main function declaration. Moving on to line 2, the if statement checks if the number of command line arguments is not equal to 2, which seems correct. Line 3 initializes the variable len with the length of the second command line argument, which also appears correct. \n\nLine 4 is where the bug is present. The variable i is declared but not initialized in the for loop. This can lead to undefined behavior since it is used without being properly initialized. \n\nLines 5 to 7 seem fine as they print each character of the second command line argument. Finally, line 8 returns 0 if the program runs successfully.\n\nTherefore, the correct answer is:\n- Line 4 has a bug / vuln ///Answer: F"}
{"subject": "", "question": "Question: When searching for an entity \ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64 that has a given relationship \ud835\udc5f with a given entity \ud835\udc52?\n\nOptions:\nA. We search for \ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64 that have a similar embedding vector to \ud835\udc52\nB. We search for \ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64 that have a similar embedding vector to \ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51 which has relationship \ud835\udc5f with \ud835\udc52\nC. We search for pairs (\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64, \ud835\udc52) that have similar embedding to (\ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51, \ud835\udc52)\nD. We search for pairs (\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64, \ud835\udc52) that have similar embedding to (\ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51, \ud835\udc52) for \ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51 which has relationship \ud835\udc5f with \ud835\udc52\n", "answer": "Explanations: To answer this question, we need to consider the best approach for searching for an entity \ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64 that has a given relationship \ud835\udc5f with a given entity \ud835\udc52. Let's analyze each option:\n\nOption 0: We search for \ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64 that have a similar embedding vector to \ud835\udc52\n- This option suggests finding entities with similar embedding vectors to the given entity \ud835\udc52. However, it does not explicitly consider the relationship \ud835\udc5f between entities, which is crucial in this context. Therefore, this option is not the most suitable.\n\nOption 1: We search for \ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64 that have a similar embedding vector to \ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51 which has relationship \ud835\udc5f with \ud835\udc52\n- This option proposes searching for entities that have similar embedding vectors to the related entity \ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51, which is connected to the given entity \ud835\udc52 through the relationship \ud835\udc5f. This method considers both embedding similarity and the relationship \ud835\udc5f, making it a more reasonable choice.\n\nOption 2: We search for pairs (\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64, \ud835\udc52) that have similar embedding to (\ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51, \ud835\udc52)\n- This option suggests searching for pairs of entities that have similar embeddings to the pair (\ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51, \ud835\udc52). While it considers the embedding similarity, it does not directly account for the relationship \ud835\udc5f between the entities. Therefore, it may not be the most effective approach.\n\nOption 3: We search for pairs (\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64, \ud835\udc52) that have similar embedding to (\ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51, \ud835\udc52) for \ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51 which has relationship \ud835\udc5f with \ud835\udc52\n- This option is similar to Option 2 but specifies that the search should be based on the relationship \ud835\udc5f between \ud835\udc52\ud835\udc5c\ud835\udc59\ud835\udc51 and \ud835\udc52. By considering both embedding similarity and the specific relationship \ud835\udc5f, this approach seems to be the most appropriate for finding entities with the desired relationship.\n\nIn conclusion, Option 3 is the most suitable choice as it takes into account both embedding similarity and the specified relationship \ud835\udc5f in the search for a new entity \ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc64 related to a given entity \ud835\udc52. ///Answer: D"}
{"subject": "", "question": "Question: The group $\\mathbb{Z}_{60}^*$ has \\ldots?\n\nOptions:\nA. 16 elements.\nB. 60 elements.\nC. 59 elements.\nD. 32 elements.\n", "answer": "Explanations: ##Answer:\n16 elements.\n\n##Reasons:\n1. The group $\\mathbb{Z}_{60}^*$, also known as the multiplicative group modulo 60, consists of integers relatively prime to 60.\n2. To find the number of elements in $\\mathbb{Z}_{60}^*$, we need to calculate the Euler's totient function $\\phi(60)$, which represents the number of positive integers less than 60 that are coprime to 60.\n3. The prime factorization of 60 is $2^2 \\times 3 \\times 5$. Therefore, $\\phi(60) = 60 \\times \\left(1 - \\frac{1}{2}\\right) \\times \\left(1 - \\frac{1}{3}\\right) \\times \\left(1 - \\frac{1}{5}\\right) = 16$.\n4. Hence, the group $\\mathbb{Z}_{60}^*$ has 16 elements, which are integers from 1 to 60 that are coprime to 60. ///Answer: A:"}
{"subject": "", "question": "Question: Your aim is to evaluate a Tweet analysis system, the \npurpose of which is to detect whether a tweet is offensive. For each Tweet processed, such a system outputs one of the following classes: \"hateful\", \n\"offensive\" and \"neutral\".To perform your evaluation, you \ncollect a large set of Tweets and have it annotated by two human \nannotators. This corpus contains 1% of \"hateful\" and 4% of \n\"offensive\" Tweets.What metrics do you think are appropriate to evaluate such a system?(penalty for wrong ticks)?\n\nOptions:\nA. Cohen's kappa\nB. accuracy\nC. precision\nD. recall\nE. standard deviation\n", "answer": "Explanations: In this scenario where we are evaluating a Tweet analysis system that classifies tweets into \"hateful\", \"offensive\", and \"neutral\" categories, the appropriate metrics to consider are:\n\n1. Cohen's kappa: Cohen's kappa is a metric that measures inter-annotator agreement between two human annotators. It is useful for assessing the level of agreement between human annotators in labeling the tweets, which can provide insight into the reliability of the annotations.\n\n2. Accuracy: Accuracy is a metric that measures the overall correctness of the classification system by calculating the percentage of correctly classified tweets out of the total number of tweets. It provides a general overview of how well the system is performing across all classes.\n\n3. Precision: Precision is a metric that calculates the proportion of correctly classified \"hateful\" or \"offensive\" tweets out of all the tweets that the system classified as \"hateful\" or \"offensive\". It provides insight into the system's ability to correctly identify tweets that are truly hateful or offensive.\n\n4. Recall: Recall is a metric that calculates the proportion of correctly classified \"hateful\" or \"offensive\" tweets out of all the tweets that are actually hateful or offensive in the dataset. It provides insight into the system's ability to correctly capture all instances of hateful or offensive tweets.\n\nTherefore, the appropriate metrics to evaluate the Tweet analysis system in this scenario would be Cohen's kappa, accuracy, precision, and recall. Standard deviation is not typically used as an evaluation metric in this context. ///Answer: A"}
{"subject": "", "question": "Question: My Bluetooth device has the PIN 0000. Then\\dots?\n\nOptions:\nA. the PIN will be hard to guess.\nB. the pairing is \\textbf{not} vulnerable to a sniffing attack.\nC. the protocol will never be secure.\nD. if I do the pairing in a bunker while I am alone, the protocol is secure.\n", "answer": "Explanations: When dealing with Bluetooth device pairing and the use of a PIN (Personal Identification Number), it's important to understand the implications of using a simple, default, or easily guessable PIN like \"0000\". Let's evaluate the given options:\n\n1: The PIN will be hard to guess:\n- This statement is false. The PIN \"0000\" is extremely easy to guess as it is a very common default or simple PIN, making it vulnerable to brute-force attacks.\n\n2: The pairing is \\textbf{not} vulnerable to a sniffing attack:\n- This statement is false. Using a weak PIN like \"0000\" makes the pairing vulnerable to sniffing attacks because an attacker can potentially eavesdrop on the communication and intercept the PIN exchange.\n\n3: The protocol will never be secure:\n- This statement is not entirely accurate. The security of the protocol depends on various factors, including the strength of the PIN used for pairing. In this case, using \"0000\" as the PIN significantly reduces security.\n\n4: If I do the pairing in a bunker while I am alone, the protocol is secure:\n- This statement is false. Even in a physically secure environment, using a weak PIN like \"0000\" leaves the protocol vulnerable to attacks, especially if the devices are later moved to less secure environments.\n\nTherefore, none of the options are entirely correct; however, the most relevant option based on the weaknesses of using \"0000\" as a PIN is:\n2: The pairing is \\textbf{not} vulnerable to a sniffing attack. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{incorrect} assertion. Using sequential composition,\\dots?\n\nOptions:\nA. one can make a protocol more Zero-Knowledge.\nB. one can improve the soundness of a protocol.\nC. one can improve the completeness of a protocol.\nD. one has to repeat a protocol a given number of times using fresh random coins.\n", "answer": "Explanations: To determine the incorrect assertion, let's analyze each option:\n\n1. One can make a protocol more Zero-Knowledge: This is a correct assertion. Sequential composition can indeed be used to make a protocol more Zero-Knowledge by running the protocol multiple times with different random choices each time.\n\n2. One can improve the soundness of a protocol: This is also a correct assertion. By using sequential composition, one can enhance the soundness of a protocol by reducing the probability of a cheating prover succeeding in convincing the verifier.\n\n3. One can improve the completeness of a protocol: This assertion is incorrect. Sequential composition does not directly improve the completeness of a protocol. Completeness is typically related to the ability of an honest prover to convince the verifier when the statement is true.\n\n4. One has to repeat a protocol a given number of times using fresh random coins: This is a correct assertion. In sequential composition, the protocol is repeated multiple times with fresh random choices to enhance security properties such as Zero-Knowledge and soundness.\n\nTherefore, the incorrect assertion is: \"one can improve the completeness of a protocol.\" ///Answer: C"}
{"subject": "", "question": "Question:  Consider a Generative Adversarial Network (GAN) which successfully produces images of goats. Which of the following statements is false?\n                    ?\n\nOptions:\nA. The discriminator can be used to classify images as goat vs non-goat.\nB. The generator aims to learn the distribution of goat images.\nC. After the training, the discriminator loss should ideally reach a constant value.\nD. The generator can produce unseen images of goats.\n", "answer": "Explanations: The false statement is: After the training, the discriminator loss should ideally reach a constant value.\n\nExplanation: In a Generative Adversarial Network (GAN), the discriminator and generator are in a constant battle to improve their performance. The discriminator's goal is to distinguish between real and generated images, while the generator's goal is to produce realistic images. The training process involves the discriminator providing feedback to the generator on how to improve its generated images. Ideally, the discriminator loss should not reach a constant value, but rather continue to decrease as the generator improves. Therefore, the statement stating that the discriminator loss should ideally reach a constant value is false. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. A distinguisher \\dots?\n\nOptions:\nA. can be used to break a PRNG.\nB. is an algorithm calling an oracle.\nC. can prove the security of a block cipher.\nD. can show a weakness in a design.\n", "answer": "Explanations: First, let's define what a distinguisher is. In cryptography, a distinguisher is an algorithm that can differentiate between two different distributions of data.\n\na) A distinguisher can be used to break a PRNG (Pseudo-Random Number Generator) by distinguishing between the output of the PRNG and true random numbers. This is a correct assertion.\n\nb) A distinguisher is an algorithm that calls an oracle to help in distinguishing between distributions. This is also a correct assertion.\n\nc) A distinguisher cannot prove the security of a block cipher. While a distinguisher can show a distinguishable difference between distributions, it does not provide a definitive proof of security for a block cipher. This is the incorrect assertion.\n\nd) A distinguisher can show a weakness in a design by distinguishing between the expected behavior and an unintended behavior. This is also a correct assertion.\n\nTherefore, the incorrect assertion is:\n\nc) can prove the security of a block cipher. ///Answer: C"}
{"subject": "", "question": "Question: For this question, one or more assertions can be correct. Tick only the correct assertion(s). There\nwill be a penalty for wrong assertions ticked.Using a 3-gram character model, which of the following expressions are equal to \\( P(\\text{opossum}) \\) ?\n\nOptions:\nA. \\( P(\\text{op}) \\cdot P(\\text{op}|\\text{o}) \\cdot P(\\text{po}|\\text{s}) \\cdot P(\\text{os}|\\text{s}) \\cdot P(\\text{ss}|\\text{u}) \\cdot P(\\text{su}|\\text{m}) \\)\nB. \\( P(\\text{op}) \\cdot P(\\text{o}|\\text{po}) \\cdot P(\\text{s}|\\text{op}) \\cdot P(\\text{s}|\\text{so}) \\cdot P(\\text{u}|\\text{ss}) \\cdot P(\\text{m}|\\text{us}) \\)\nC. \\( P(\\text{op}) \\cdot P(\\text{o}|\\text{op}) \\cdot P(\\text{s}|\\text{po}) \\cdot P(\\text{s}|\\text{os}) \\cdot P(\\text{u}|\\text{ss}) \\cdot P(\\text{m}|\\text{su}) \\)\nD. \\( P(\\text{opo}) \\cdot P(\\text{s}|\\text{po}) \\cdot P(\\text{s}|\\text{os}) \\cdot P(\\text{u}|\\text{ss}) \\cdot P(\\text{m}|\\text{su}) \\)\nE. \\( P(\\text{o}) \\cdot P(\\text{p}|\\text{o}) \\cdot P(\\text{o}|\\text{p}) \\cdot P(\\text{s}|\\text{o}) \\cdot P(\\text{s}|\\text{s}) \\cdot P(\\text{u}|\\text{s}) \\cdot P(\\text{m}|\\text{u}) \\)\nF. \\( P(\\text{opo}) \\cdot P(\\text{pos}) \\cdot P(\\text{oss}) \\cdot P(\\text{ssu}) \\cdot P(\\text{sum}) \\)\nG. \\( P(\\text{opo}) \\cdot P(\\text{ssu}) \\cdot P(\\text{sum}) \\)\nH. \\( \\displaystyle \\frac{ P(\\text{opo}) \\cdot P(\\text{pos}) \\cdot P(\\text{oss}) \\cdot P(\\text{ssu}) \\cdot P(\\text{sum}) }{ P(\\text{po}) \\cdot P(\\text{os}) \\cdot P(\\text{ss}) \\cdot P(\\text{su}) } \\)\n", "answer": "Explanations: To find \\( P(\\text{opossum}) \\) using a 3-gram character model, we need to consider the probability of each character occurring based on the previous two characters.\n\nGiven that the word is \"opossum\":\n- \\( P(\\text{opo}) \\) can be calculated as \\( P(\\text{o} | \\text{po}) \\times P(\\text{po} | \\text{o}) \\)\n- \\( P(\\text{pos}) \\) can be calculated as \\( P(\\text{o} | \\text{po}) \\times P(\\text{s} | \\text{op}) \\)\n- \\( P(\\text{oss}) \\) can be calculated as \\( P(\\text{s} | \\text{po}) \\times P(\\text{s} | \\text{os}) \\)\n- \\( P(\\text{ssu}) \\) can be calculated as \\( P(\\text{s} | \\text{os}) \\times P(\\text{u} | \\text{ss}) \\)\n- \\( P(\\text{sum}) \\) can be calculated as \\( P(\\text{u} | \\text{ss}) \\times P(\\text{m} | \\text{su}) \\)\n\nTherefore, the correct expression for \\( P(\\text{opossum}) \\) is:\n\\( P(\\text{opo}) \\cdot P(\\text{pos}) \\cdot P(\\text{oss}) \\cdot P(\\text{ssu}) \\cdot P(\\text{sum}) \\)\n\nHence, the correct assertion is:\n\\( P(\\text{opo}) \\cdot P(\\text{pos}) \\cdot P(\\text{oss}) \\cdot P(\\text{ssu}) \\cdot P(\\text{sum}) \\) ///Answer: F"}
{"subject": "", "question": "Question: Which of the following is \u200btrue \u200bfor a density based cluster C:?\n\nOptions:\nA. Any two points in C must be density reachable. Each point belongs to one, and only one cluster\nB. Any two points in C must be density reachable. Border points may belong to more than one cluster\nC. Any two points in C must be density connected. Border points may belong to more than one cluster\nD. Any two points in C must be density connected. Each point belongs to one, and only one cluster\n", "answer": "Explanations: The correct option is (Y) Any two points in C must be density connected. Border points may belong to more than one cluster.\n\nThis means that for a density based cluster, all points within the cluster must be connected based on the density of the points. This ensures that all points in the cluster are close enough to each other in terms of density. Border points, which are points on the edge of the cluster, may also belong to more than one cluster if they meet the density criteria for multiple clusters. This allows for more flexibility in how clusters are defined based on the density of the points. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. The security of the DH protocol requires \\dots?\n\nOptions:\nA. that the DL problem is hard.\nB. that the factoring problem is hard.\nC. that we use groups of prime order.\nD. a KDF function.\n", "answer": "Explanations: Option A: \"A large prime number p and a generator g that is a primitive root of p\" - This is a correct assertion as the security of the Diffie-Hellman (DH) protocol does indeed require a large prime number p and a generator g that is a primitive root of p. These values are fundamental to ensure the security of the key exchange process in the DH protocol.\n\nOption B: \"The public keys exchanged between the parties must be kept secret\" - This is an incorrect assertion. In the DH protocol, the security does not rely on the secrecy of the public keys. The public keys are meant to be exchanged openly between the parties to establish a shared secret key, while the private keys must be kept confidential to maintain the security of the communication.\n\nOption C: \"The private keys of the parties must be periodically changed\" - This is an incorrect assertion. In the DH protocol, the private keys are generated once and are not required to be changed periodically. Changing the private keys unnecessarily could introduce unnecessary complexity and potential vulnerabilities in the key exchange process.\n\nOption D: \"The parties must agree on a secret key before starting the communication\" - This is a correct assertion. In the DH protocol, the parties generate a shared secret key through the exchange of public keys and their private keys. Agreeing on this shared secret key is essential for establishing secure communication between the parties. ///Answer: Option C: D"}
{"subject": "", "question": "Question: In the first pass over the database of the FP Growth algorithm?\n\nOptions:\nA. Frequent itemsets are extracted\nB. A tree structure is constructed\nC. The frequency of items is computed\nD. Prefixes among itemsets are determined\n", "answer": "Explanations: 1. Frequent itemsets are extracted - This is correct. In the first pass over the database, frequent itemsets are identified based on a minimum support threshold.\n\n2. A tree structure is constructed - This is incorrect. The tree structure is actually constructed in later passes of the FP Growth algorithm, not in the first pass.\n\n3. The frequency of items is computed - This is partially correct. The frequency of items is indeed computed in the first pass, but it is used to identify frequent itemsets rather than being the main focus of the first pass.\n\n4. Prefixes among itemsets are determined - This is incorrect. Determining prefixes among itemsets is not typically done in the first pass of the FP Growth algorithm. This process is more relevant in later passes when building the conditional FP tree.\n\nOverall, the correct answer is:\n- Frequent itemsets are extracted. ///Answer: A"}
{"subject": "", "question": "Question: Given a message $x$, find a different message $x'$ such that $h(x)=h(x')$ is the definition of ...\n\nOptions:\nA. First preimage attack.\nB. Collision attack.\nC. Second preimage attack.\nD. Known message attack.\n", "answer": "Explanations: {\n    \"explaination\": \"This is the definition of a Collision attack where the goal is to find two different inputs that hash to the same output.\",\n    \"answer\": \"Collision attack.\"\n} ///Answer: B"}
{"subject": "", "question": "Question: Which of the following tasks would typically not be solved by clustering?\n\nOptions:\nA. Community detection in social networks\nB. Discretization of continuous features\nC. Spam detection in an email system\nD. Detection of latent topics in a document collection\n", "answer": "Explanations: The task that would typically not be solved by clustering is 'Discretization of continuous features'. \n\nClustering is a technique used to group similar data points together based on certain criteria, but it does not involve transforming continuous features into discrete values. Discretization is a process of converting continuous features into categorical or ordinal values, which is a different task that typically requires different methods such as binning or decision tree-based algorithms.\n\nThe other tasks listed - 'Community detection in social networks', 'Spam detection in an email system', and 'Detection of latent topics in a document collection' - can all be effectively solved using clustering techniques. Clustering can help identify communities of users with similar behaviors in social networks, classify emails as spam or non-spam based on their features, and group similar documents together to extract latent topics. ///Answer: B"}
{"subject": "", "question": "Question: Let $f:\\R^D\rightarrow\\R$ be an $L$-hidden layer multi-layer perceptron (MLP) such that\n                    \\[\n\t                    f(xv)=\\sigma_{L+1}\big(\\wv^\top\\sigma_L(\\Wm_L\\sigma_{L-1}(\\Wm_{L-1}\\dots\\sigma_1(\\Wm_1xv)))\big),\n                    \\]\n                    with $\\wv\\in\\R^{M}$, $\\Wm_1\\in\\R^{M\times D}$ and $\\Wm_\\ell\\in\\R^{M\times M}$ for $\\ell=2,\\dots, L$, and $\\sigma_i$ for $i=1,\\dots,L+1$ is an entry-wise activation function. For any MLP $f$ and a classification threshold $\tau$ let $C_{f, \tau}$ be a binary classifier that outputs YES for a given input $xv$ if $f(xv) \\leq \tau$ and NO otherwise. \u000bspace{3mm}\n                    Assume $\\sigma_{L+1}$ is the element-wise \textbf{sigmoid} function and $C_{f, \frac{1}{2}}$ is able to obtain a high accuracy on a given binary classification task $T$. Let $g$ be the MLP obtained by multiplying the parameters \textbf{in the last layer} of $f$, i.e. $\\wv$, by 2. Moreover, let $h$ be  the MLP obtained by replacing $\\sigma_{L+1}$ with element-wise \textbf{ReLU}. Finally, let $q$ be the MLP obtained by doing both of these actions. Which of the following is true?\n\t\t            ReLU(x) = max\\{x, 0\\} \\\n\t\t            Sigmoid(x) = \frac{1}{1 + e^{-x}}\n                    ?\n\nOptions:\nA. $C_{g, \frac{1}{2}}$ may have an accuracy significantly lower than $C_{f, \frac{1}{2}}$ on $T$\nB. $C_{h, 0}$ may have an accuracy significantly lower than $C_{f, \frac{1}{2}}$ on $T$\nC. $C_{q, 0}$ may have an accuracy significantly lower than $C_{f, \frac{1}{2}}$ on $T$\nD. $C_{g, \frac{1}{2}}$, $C_{h, 0}$, and $C_{q, 0}$ have the same accuracy as  $C_{f, \frac{1}{2}}$ on $T$\n", "answer": "Explanations: First, let's consider the effect of multiplying the parameters in the last layer of $f$ by 2 to obtain $g$. This action essentially scales the output of the last layer before applying the sigmoid activation function. Since the sigmoid function squashes its input between 0 and 1, doubling the parameters may result in the output being pushed towards the extremes (0 or 1), potentially making it harder for the binary classifier $C_{g, frac{1}{2}}$ to correctly classify the data points. Therefore, option a) is true.\n\nNext, let's consider the effect of replacing the sigmoid activation function with ReLU to obtain $h$. The ReLU activation function is known to be more robust than sigmoid for deep networks due to its ability to mitigate the vanishing gradient problem. However, the choice of activation function can significantly impact the decision boundary learned by the model. In this case, replacing sigmoid with ReLU may alter the decision boundary learned by $h$, potentially leading to a lower accuracy on the binary classification task $T$. Therefore, option b) is also true.\n\nFinally, let's consider the combined effect of both actions on $q$. By both doubling the parameters in the last layer and replacing sigmoid with ReLU, $q$ undergoes significant changes in its architecture. The interaction between these changes may result in a more complex decision boundary that could potentially lead to lower accuracy on $T$ compared to $C_{frac{1}{2}}$. Therefore, option c) is true.\n\nIn conclusion, options a), b), and c) are all true based on the effects of scaling parameters and changing activation functions on the MLPs $g$, $h$, and $q, respectively. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{false} answer. In a group, the operation\\dots?\n\nOptions:\nA. is commutative\nB. is associative.\nC. has a neutral element.\nD. associates an inverse to each value.\n", "answer": "Explanations: The correct option is: is commutative\n\nExplanation:\n- The operation being commutative means that the order of elements does not matter. This property does not necessarily hold in all groups. Some operations in groups are commutative (e.g., addition in the group of integers), but there are groups where the operation is not commutative (e.g., matrix multiplication). So, in general, the operation in a group is not always commutative. ///Answer: A"}
{"subject": "", "question": "Question: Passing on secrets: Alice has posted her RSA credentials as $(m,e)$, with $m$ the modulus and $e$ the encoding exponent. As required by RSA, she keeps her decoding exponent $d$ preciously secret. Bob has a message $t_1$, RSA-encrypts it using $(m,e_1)$ and passes the resulting cryptogram $c_1$ on to Carlos. Carlos has a message $t_2$, RSA-encrypts it using $(m,e_2)$ to obtain the cryptogram $c_2$. Then, Carlos multiplies the two cryptograms, $(c_1\\cdot c_2) \\mod m,$ and passes this to Alice. Alice applies her regular RSA decryption to $(c_1\\cdot c_2) \\mod m.$ Under what condition is the result of this decryption exactly equal to the product $(t_1\\cdot t_2) \\mod m$?\n\nOptions:\nA. If $e_1=e_2=e$.\nB. If $e_1+e_2=e$.\nC. If for some integer $\\ell,$ we have $e_1 e_2 d=\\ell \\phi(m)+1,$ where $\\phi(\\cdot)$ denotes Euler's totient function.\nD. If $d$ is prime and $(e_1+e_2) \\mod m=1$.\n", "answer": "Explanations: We are given that Alice applies her RSA decryption to $(c_1\\cdot c_2) \\mod m$ and we want the result to be exactly equal to the product $(t_1\\cdot t_2) \\mod m$. Let's denote the RSA decryption function as $D(\\cdot)$. \n\nThe decryption of $(c_1\\cdot c_2) \\mod m$ by Alice will give us $(c_1\\cdot c_2)^d \\mod m$. In order for this to be equal to $(t_1 \\cdot t_2) \\mod m$, we must have:\n\n$$(c_1\\cdot c_2)^d \\mod m \\equiv t_1 \\cdot t_2 \\mod m$$\n\nGiven that $c_1 \\equiv t_1^{e_1} \\mod m$ and $c_2 \\equiv t_2^{e_2} \\mod m$, we can rewrite the left side of the equation as:\n\n$$(t_1^{e_1} \\cdot t_2^{e_2})^d \\mod m$$\n\nExpanding this expression, we get:\n\n$$t_1^{e_1 \\cdot d} \\cdot t_2^{e_2 \\cdot d} \\mod m$$\n\nFor this to be equal to $t_1 \\cdot t_2 \\mod m$, we must have:\n\n$$e_1 \\cdot d \\equiv 1 \\mod \\phi(m)$$\n$$e_2 \\cdot d \\equiv 1 \\mod \\phi(m)$$\n\nSo, the condition for the result of Alice's decryption to be exactly equal to the product $(t_1\\cdot t_2) \\mod m$ is that $e_1 \\cdot d \\equiv 1 \\mod \\phi(m)$ and $e_2 \\cdot d \\equiv 1 \\mod \\phi(m)$. ///Answer: C"}
{"subject": "", "question": "Question: The Merkle-D{\\aa}mgard construction is?\n\nOptions:\nA. a method which iterates a hash function to obtain a compression function.\nB. a method which iterates a compression function to obtain a hash function.\nC. a method which constructs a compression function from a block cipher.\nD. a method which constructs a block cipher function from a hash function.\n", "answer": "Explanations: correct answer: a method which iterates a hash function to obtain a compression function.\nexplanation: The Merkle-D\u00e5mgard construction is a method used to construct cryptographic hash functions from a compression function by iteratively applying a hash function to the input data in blocks. This process helps to create a secure hash function by ensuring that any changes to the input data result in a completely different hash value. ///Answer: A"}
{"subject": "", "question": "Question: Why is XML a document model?\n\nOptions:\nA. It supports application-specific markup\nB. It supports domain-specific schemas\nC. It has a serialized representation\nD. It uses HTML tags\n", "answer": "Explanations: XML is considered a document model because it is designed to represent and structure data in the form of documents. It supports application-specific markup, meaning that developers can create their own tags and elements to organize and describe the data within the document. XML also supports domain-specific schemas, allowing for the creation of rules and constraints for the structure and content of the document.\n\nAdditionally, XML has a serialized representation, which means that it can be easily converted into a format that can be read and processed by computers. This makes it a popular choice for exchanging data between different systems and applications. While XML tags may resemble HTML tags, XML is not limited to just representing web content and can be used to represent a wide range of data types and structures. ///Answer: A"}
{"subject": "", "question": "Question: Consider the Diffie-Hellman secret-key-exchange algorithm performed in the cyclic group $(\\mathbb{Z}/11\\mathbb{Z}^\\star, \\cdot)$. Let $g=2$ be the chosen group generator. Suppose that Alice's secret number is $a=5$ and Bob's is $b=3$. Which common key $k$ does the algorithm lead to? Check the correct answer.\n\nOptions:\nA. $10$\nB. $7$\nC. $8$\nD. $9$\n", "answer": "Explanations: Answer: \nA) $10$\n\nExplanation:\n1. Firstly, let's calculate the public keys exchanged by Alice and Bob:\n- Alice computes $A = g^a \\mod 11 = 2^5 \\mod 11 = 32 \\mod 11 = 10$.\n- Bob computes $B = g^b \\mod 11 = 2^3 \\mod 11 = 8$.\n\n2. Alice receives Bob's public key $B=8$, and Bob receives Alice's public key $A=10$.\n\n3. Now, they calculate the shared secret key:\n- Alice computes $k = B^a \\mod 11 = 8^5 \\mod 11 = 10$.\n- Bob computes $k = A^b \\mod 11 = 10^3 \\mod 11 = 10$.\n\n4. Therefore, the common key they arrive at using the Diffie-Hellman key exchange algorithm is $k = 10$.\n\nThus, the correct answer is A) $10$. ///Answer: A"}
{"subject": "", "question": "Question: In JOS and x86, which register stores the system call number when invoking a system call?\n\nOptions:\nA. ecx\nB. eip\nC. eax\nD. esp\nE. No register is required, and the syscall number is followed by int instruction, e.g. int 0x30.\n", "answer": "Explanations: In JOS on the x86 architecture, the system call number is typically stored in a specific register when invoking a system call. Let's determine which register holds the system call number:\n\nIn JOS on x86, the register that commonly stores the system call number when invoking a system call is:\n\n3: **eax**\n\nExplanation:\n- In the x86 architecture, system calls are typically invoked by placing the system call number in the eax register.\n- The eax register is used to pass the system call number to the operating system kernel when making system calls.\n- After placing the system call number in the eax register, the int 0x30 or int instruction (interrupt instruction) is used to trigger the operating system kernel to handle the system call.\n- Upon receiving the interrupt, the kernel retrieves the system call number from the eax register to identify the requested system call operation.\n\nTherefore, in JOS on x86, the system call number is usually stored in the eax register when invoking a system call. ///Answer: C"}
{"subject": "", "question": "Question: Matrix Factorizations: If we compare SGD vs ALS for optimizing a matrix factorization of a $D \\times N$ matrix, for large $D, N$?\n\nOptions:\nA. (a)  Per iteration, SGD has a similar computational cost as ALS\nB. (b)  Per iteration, ALS has an increased computational cost over SGD\nC. (c)  Per iteration, SGD cost is independent of $D, N$\n", "answer": "Explanations: Let's evaluate each statement:\n\n(a) Per iteration, SGD has a similar computational cost as ALS:\n- SGD (Stochastic Gradient Descent) updates one entry of the factor matrices per iteration, which involves computing the gradient for that entry. This computation is independent of the matrix size, so the computational cost per iteration is relatively low compared to ALS.\n- ALS (Alternating Least Squares) updates one entire factor matrix at a time, which involves solving a least squares problem. This cost is higher compared to updating a single entry in SGD.\n- Therefore, statement (a) is incorrect.\n\n(b) Per iteration, ALS has an increased computational cost over SGD:\n- As discussed above, ALS involves updating entire factor matrices, leading to a higher computational cost per iteration compared to updating single entries in SGD.\n- This statement correctly reflects the difference in computational cost between ALS and SGD.\n- Therefore, statement (b) is correct.\n\n(c) Per iteration, SGD cost is independent of $D, N$:\n- In SGD, the cost per iteration is mainly determined by the number of non-zero entries in the matrix and the rank of the factorization. It is not entirely independent of the matrix size $D \\times N$.\n- The computational cost in SGD can increase with the size of the matrix since more entries may need to be updated.\n- Therefore, statement (c) is incorrect.\n\nTherefore, the correct answer is 2) (b) Per iteration, ALS has an increased computational cost over SGD. ///Answer: B"}
{"subject": "", "question": "Question: Consider the composite function $f(x)=g(h(x))$, where all functions are $\\mathbb{R}$ to $\\mathbb{R}$. Which of the following is the weakest condition that guarantees that $f(x)$ is convex?\n\nOptions:\nA. $g(x)$ and $h(x)$ are convex and $g(x)$ and $h(x)$ are increasing\nB. $g(x)$ is convex and $g(x)$ is increasing\nC. $g(x)$ and $h(x)$ are convex and $h(x)$ is increasing\nD. $g(x)$ and $h(x)$ are convex and $g(x)$ is increasing\nE. $g(x)$ is convex and $g(x)$ and $h(x)$ are increasing\nF. $h(x)$ is convex and $g(x)$ and $h(x)$ are increasing\nG. $g(x)$ is convex and $h(x)$ is increasing\n", "answer": "Explanations: Let's reevaluate the condition for convexity in the context of the given options, focusing on the role of $h(x)$ in the composite function $f(x) = g(h(x))$:\n\n1. $g(x)$ and $h(x)$ are convex and $g(x)$ and $h(x)$ are increasing\n2. $g(x)$ is convex and $g(x)$ is increasing\n3. $g(x)$ and $h(x)$ are convex and $h(x)$ is increasing\n4. $g(x)$ and $h(x)$ are convex and $g(x)$ is increasing\n5. $g(x)$ is convex and $g(x)$ and $h(x)$ are increasing\n6. $h(x)$ is convex and $g(x)$ and $h(x)$ are increasing\n7. $g(x)$ is convex and $h(x)$ is increasing\n\nNow, let's consider the conditions for convexity based on the second derivative of $f(x)$:\n\nGiven $f(x) = g(h(x))$,\n$$f'(x) = g'(h(x)) \\cdot h'(x)$$\n$$f''(x) = g''(h(x)) \\cdot (h'(x))^2 + g'(h(x)) \\cdot h''(x)$$\n\nFor $f(x)$ to be convex, we need $f''(x) \\geq 0$.\n\nAnalyzing the options:\n\n1. If both $g(x)$ and $h(x)$ are convex and $h(x)$ is increasing, this is a stronger condition than necessary. $h(x)$ being increasing is not required.\n2. If $g(x)$ is convex and increasing, this is not the weakest condition as convexity of $g(x)$ and $h(x)$ is required.\n3. If both $g(x)$ and $h(x)$ are convex and $h(x)$ is increasing, this is a stronger condition.\n4. If both $g(x)$ and $h(x)$ are convex and $g(x)$ is increasing, this is the weakest condition, and $h(x)$ being convex is required but not necessarily increasing.\n5. If $g(x)$ is convex and both $g(x)$ and $h(x)$ are increasing, this is a stronger condition.\n6. If $h(x)$ is convex and both $g(x)$ and $h(x)$ are increasing, this is a stronger condition.\n7. If $g(x)$ is convex and $h(x)$ is increasing, this is not the weakest condition as convexity of $g(x)$ is also required.\n\nTherefore, the correct option is: $g(x)$ and $h(x)$ are convex and $g(x)$ is increasing. This is the weakest condition that guarantees $f(x)$ is convex. ///Answer: D"}
{"subject": "", "question": "Question: Consider the following loaded dice with $6$ faces: $P(S_1=6)=5/6$ and $P(S_1 = x)=1/30$ for $x\\in\\{1,2,3,4,5\\}$. Suppose we throw it indefinitely. Hence, we have a source $S=S_1 S_2 S_3\\ldots$. Then, $H(S_1,\\ldots,S_n) = \\sum_{i=1}^n H(S_i|S_1\\ldots S_{i-1})$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: True - This statement is true according to the chain rule of entropy, which states that the joint entropy of a sequence of random variables is equal to the sum of the conditional entropies of each individual random variable given the previous ones. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is/are true about fuzzing?\n\nOptions:\nA. Fuzzers may get ``stuck'' and cannot easily detect that they are\n               no longer improving coverage\nB. The efficacy of a fuzzing campaign scales with its speed (executions per second)\nC. Fuzzers generally determine the exploitability of a crash.\nD. There is little to no benefit in running fuzzers in parallel.\n", "answer": "Explanations: The true statements about fuzzing are:\n\n1. \"Fuzzers may get ``stuck'' and cannot easily detect that they are no longer improving coverage\" - This is true because fuzzers can sometimes get stuck in a loop or fail to progress in their testing, which may hinder their ability to improve coverage.\n\n2. \"The efficacy of a fuzzing campaign scales with its speed (executions per second)\" - This is also true because the faster a fuzzer can execute test cases, the more efficiently it can explore different inputs and potentially uncover vulnerabilities.\n\n3. \"Fuzzers generally determine the exploitability of a crash\" - This statement is false. While fuzzers can identify crashes in software, they typically do not assess the exploitability of those crashes. This task is usually left to security researchers or analysts.\n\n4. \"There is little to no benefit in running fuzzers in parallel\" - This statement is false. Running fuzzers in parallel can help increase the coverage and efficiency of the testing process by allowing multiple instances to explore different parts of the target software simultaneously. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements about symbolic execution is/are correct?\n\nOptions:\nA. Symbolic execution can efficiently handle and solve constraints\n            in programs with simple logics but large input space.\nB. State space explosion is a common challenge for symbolic\n            execution.\nC. Symbolic execution can always accurately model a system's\n            environment (e.g., system calls, file I/O, and network I/O).\nD. Symbolic execution requires actually running the target\n            program.\n", "answer": "Explanations: 1. False. Symbolic execution can struggle with programs that have large input spaces due to the exponential growth of possibilities, which can lead to path explosion and make the analysis infeasible.\n\n2. True. State space explosion is a well-known challenge in symbolic execution. As the symbolic execution explores all possible paths through a program, the number of states and paths can grow exponentially, leading to scalability issues and making it difficult to analyze larger programs.\n\n3. False. Symbolic execution may not always accurately model a system's environment, especially when it comes to interactions with external entities like system calls, file I/O, and network I/O. Handling these interactions requires additional techniques or tools to provide realistic behavior in the symbolic execution process.\n\n4. False. Symbolic execution does not require actually running the target program. Instead, it symbolically executes the program by exploring all possible paths and generating constraints to analyze its behavior without the need for concrete execution. ///Answer: B"}
{"subject": "", "question": "Question: Select the \\emph{correct} statement. In DSA signature, i.e., DSS?\n\nOptions:\nA. the public parameter is $N = pq$, for $p$ and $q$ primes, such that $p=aq + 1$.\nB. public parameters are primes $p$ and $q$ such that $p=aq + 1$.\nC. the public parameter is a random number $N$ such that $\\gcd(N, T) = 1$, where $T$ is defined by the DSS standard.\nD. does not require any public parameter.\n", "answer": "Explanations: The most correct statement from the options provided is: public parameters are primes $p$ and $q$ such that $p=aq + 1$.\n\nExplanation:\nIn the Digital Signature Algorithm (DSA), which is also known as the Digital Signature Standard (DSS), the public key parameters consist of two large prime numbers, denoted as $p$ and $q$. These primes are chosen in a specific way to ensure the security of the DSA signature scheme. The condition $p=aq + 1$ is crucial because it ensures that $p$ and $q$ are mathematically related in such a manner that helps prevent certain attacks on the signature scheme.\n\nThe other options provided are not accurate for describing the public parameters of DSA:\n\n1. $N = pq$ for $p$ and $q$ primes: While $N = pq$ is a common notation for the product of two primes in cryptographic algorithms, it does not accurately describe the specific relationship between $p$ and $q$ required in DSA.\n\n2. The public parameter is a random number $N$ such that $\\gcd(N, T) = 1$, where $T$ is defined by the DSS standard: This statement does not accurately capture the nature of the public parameters in DSA. The public key parameters in DSA are not chosen randomly; they follow a specific structure for security reasons.\n\n3. Does not require any public parameter: This statement is incorrect. The DSA signature scheme does indeed require the use of public key parameters, specifically the prime numbers $p$ and $q.\n\nTherefore, the correct statement is that the public parameters in DSA are primes $p$ and $q$ such that $p=aq + 1, where $a$ is an integer. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statement(s) is/are true? To achieve memory safety for C, ...\n\nOptions:\nA. pointer arithmetic must be checked/restricted\nB. memory deallocation must be replaced by garbage collection\nC. all control-flow must be validated against a model\nD. a strict specification of program functionality is required\nE. the standard library must be formally verified\n", "answer": "Explanations: - Pointer arithmetic must be checked/restricted: This statement is true. Checking and restricting pointer arithmetic in C can help prevent memory errors such as buffer overflows and dangling pointers, thereby improving memory safety.\n\n- Memory deallocation must be replaced by garbage collection: This statement is not necessarily true. While garbage collection can aid in memory management and reduce memory leaks, there are other methods, such as manual memory management or smart pointers, that can also ensure memory safety in C.\n\n- All control-flow must be validated against a model: This statement is not necessarily true. While validating control-flow against a model can enhance program correctness and security, it is not a strict requirement for achieving memory safety in C. Other techniques, like input validation and memory access control, can also contribute to memory safety.\n\n- A strict specification of program functionality is required: This statement is not necessarily true. Although having a strict program functionality specification can assist in program design and development, it is not a mandatory requirement for achieving memory safety in C. Following best practices in memory management, such as proper use of pointers and memory deallocation, can also enhance memory safety.\n\n- The standard library must be formally verified: This statement is not necessarily true. While a formally verified standard library can enhance program reliability and security, it is not a strict requirement for achieving memory safety in C. Proper memory allocation and deallocation management, along with adherence to memory management best practices, can also contribute to memory safety. ///Answer: A"}
{"subject": "", "question": "Question: In Machine Learning, we want to learn the \\textbf{parameters W} for the mapping function f: $y=f(x,W) +\\epsilon$ where x is the input, y the output, and $\\epsilon$ the error term.\\\\\n(One or multiple answers)?\n\nOptions:\nA. When f: $R \\rightarrow \\{1,..N\\}$, it is a classification task\nB. When f: $R^M \\rightarrow \\{1,..N\\}$, it is a classification task\nC. When f: $R^M \\rightarrow R$, it is a classification task \nD. When f: $R^M \\rightarrow R$, it is a regression task\n", "answer": "Explanations: Let's think step-by-step:\n\nWhen the mapping function is $f: \\mathbb{R} \\rightarrow \\{1,..N\\}$, it is a classification task.\n\nWhen the mapping function is $f: \\mathbb{R}^M \\rightarrow \\{1,..N\\}$, it is a classification task.\n\nWhen the mapping function is $f: \\mathbb{R}^M \\rightarrow \\mathbb{R}$, it is a regression task.\n\nTherefore, the correct options are:\n- When $f: \\mathbb{R} \\rightarrow \\{1,..N\\}$, it is a classification task.\n- When $f: \\mathbb{R}^M \\rightarrow \\{1,..N\\}$, it is a classification task.\n- When $f: \\mathbb{R}^M \\rightarrow \\mathbb{R}$, it is a regression task. ///Answer: B"}
{"subject": "", "question": "Question: In a Ranked Retrieval result, the result at position k is non-relevant and at k+1 is relevant. Which of the following is always true (P@k and R@k are the precision and recall of the result set consisting of the k top ranked documents)?\n\nOptions:\nA. P@k-1 > P@k+1\nB. P@k-1 = P@k+1\nC. R@k-1 < R@k+1\nD. R@k-1 = R@k+1\n", "answer": "Explanations:  R@k-1 < R@k+1: The recall at position k-1 is lower than at k+1 because adding the non-relevant result at position k decreases the recall. However, once the relevant result is introduced at position k+1, the recall increases. ///Answer: C."}
{"subject": "", "question": "Question: After the join step, the number of k+1-itemsets ?\n\nOptions:\nA. is equal to the number of frequent k-itemsets\nB. can be equal, lower or higher than the number of frequent k-itemsets\nC. is always higher than the number of frequent k-itemsets\nD. is always lower than the number of frequent k-itemsets\n", "answer": "Explanations: After the join step, the number of k+1-itemsets can be equal, lower or higher than the number of frequent k-itemsets. \n\nExplanation:\n- When joining k-itemsets to form k+1 itemsets, it is possible that some of the generated k+1 itemsets may not be frequent. This can result in a lower number of k+1 itemsets compared to the number of frequent k-itemsets.\n- On the other hand, new combinations formed during the join step may result in additional frequent itemsets that were not present in the original set of k-itemsets, leading to a higher number of k+1 itemsets.\n- Therefore, the number of k+1 itemsets after the join step is not fixed and can vary based on the data and support threshold, making it possible for the number to be equal, lower, or higher than the number of frequent k-itemsets. ///Answer: B"}
{"subject": "", "question": "Question: The Time-Memory Tradeoff Attack ...\n\nOptions:\nA. is useful for finding a preimage within complexity $O\\big(\\big({\\frac{2}{3}}\\big)^N\\big).$\nB. is useful for finding a preimage within complexity $O(N^{\\frac{2}{3}}).$\nC. is a dedicated method which works only on SHA1.\nD. can be combined with birthday paradox to find the order of the group in RSA efficiently.\n", "answer": "Explanations: Answer: is useful for finding a preimage within complexity $O(N^{\\frac{2}{3}})$. Rationale: This choice seems the most plausible as the Time-Memory Tradeoff Attack is known for its efficiency in reducing the time complexity to the power of two-thirds, making it more feasible to find a preimage within this complexity. The other options provided do not accurately reflect the general understanding of this attack and its implications. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. In a zero-knowledge interactive proof for $L$, \\ldots?\n\nOptions:\nA. for any ppt verifier, there is a simulator which for any $x \\in L$ produces a conversation indistinguishable from the original conversation.\nB. for any ppt verifier, for some $x \\in L$, any simulated conversation is indistinguishable from the original conversation.\nC. the simulator imitates the verifier.\nD. the simulator is computationaly unbounded.\n", "answer": "Explanations: In a zero-knowledge interactive proof for $L$, the true assertion is: for any ppt verifier, there is a simulator which for any $x \\in L$ produces a conversation indistinguishable from the original conversation.\n\nExplanation:\n- Zero-knowledge interactive proof systems aim to prove the validity of a statement without revealing any additional information beyond the truth of the statement.\n- The correct option states that for any probabilistic polynomial-time (ppt) verifier, there exists a simulator that can produce a conversation that is indistinguishable from the original conversation for any $x$ in the language $L$ being proven.\n- This property ensures that the proof system is zero-knowledge, meaning that the verifier cannot learn any extra information from the interaction beyond the fact that the statement being proven is true. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following statements on Latent Semantic Indexing (LSI) and Word Embeddings (WE) is correct?\n\nOptions:\nA. LSI is deterministic (given the dimension), whereas WE is not\nB. LSI does not take into account the order of words in the document, whereas WE does\nC. The dimensions of LSI can be interpreted as concepts, whereas those of WE cannot\nD. LSI does take into account the frequency of words in the documents, whereas WE does not\n", "answer": "Explanations: The correct statement is: LSI does not take into account the order of words in the document, whereas WE does.\n\nLatent Semantic Indexing (LSI) is a technique used in natural language processing and information retrieval to analyze the relationships between a set of documents and the terms they contain. LSI creates a mathematical representation of the documents and terms in a high-dimensional space, where the dimensions correspond to concepts present in the documents.\n\nLSI does not consider the order of words in the document, instead, it focuses on the relationships between the terms and documents based on their co-occurrence patterns. This means that LSI is not sensitive to the word order within the documents.\n\nOn the other hand, Word Embeddings (WE) are a type of representation learning technique that maps words or phrases into a continuous vector space where the distance between vectors captures semantic relationships between the words. Word Embeddings do take into account the order of words in the document, as the context in which a word appears influences its embedding.\n\nIn summary, LSI does not consider the order of words in the document, while WE does. ///Answer: B"}
{"subject": "", "question": "Question: What is the cardinality of the multiplicative group $\\mathbb{Z}_{77}^*$?\n\nOptions:\nA. 70\nB. 66\nC. 60\nD. 76\n", "answer": "Explanations: To find the cardinality of the multiplicative group \\(\\mathbb{Z}_{77}^*\\), we need to find the number of positive integers less than 77 that are coprime to 77, i.e., the number of elements in the group of units modulo 77 (\\(\\mathbb{Z}_{77}^*\\)).\n\nGiven \\(n = 77\\), we can calculate \\(\\phi(77)\\), where \\(\\phi\\) is Euler's totient function, which gives the number of positive integers less than \\(n\\) that are coprime to \\(n\\).\n\nEuler's totient function is multiplicative, so for a prime power \\(p^k\\), where \\(p\\) is a prime and \\(k\\) is a positive integer, \\(\\phi(p^k) = p^k - p^{k-1}\\).\n\nSince 77 can be factorized as \\(77 = 7 \\times 11\\), we can calculate \\(\\phi(77)\\) as follows:\n\\[\n\\phi(77) = \\phi(7) \\times \\phi(11) = (7-1) \\times (11-1) = 6 \\times 10 = 60\n\\]\n\nTherefore, the cardinality of the multiplicative group \\(\\mathbb{Z}_{77}^*\\) is 60.\n\nSo, the correct option is:\noption 3: 60 ///Answer: C"}
{"subject": "", "question": "Question: The Kerckhoffs principle states that the security of a cryptosystem should rely on the secrecy of\\dots?\n\nOptions:\nA. the key only\nB. the algorithm only\nC. the channel only\nD. the participants' identity only\n", "answer": "Explanations: Answer: A) the key only\n\nExplanation: \nThe Kerckhoffs principle, also known as Kerckhoffs's principle of open design, states that the security of a cryptosystem should rely on the secrecy of the key only, not on the secrecy of the algorithm or system itself. This means that even if the algorithm or system is known to an attacker, the security of the system should not be compromised as long as the key is kept secret. This principle ensures that the security of the cryptosystem is not dependent on keeping the algorithm or system hidden, which is considered a more secure and practical approach in modern cryptography. ///Answer: A"}
{"subject": "", "question": "Question: What is the algorithm to perform optimization with gradient descent? Actions between Start loop and End loop are performed multiple times. (One answer)?\n\nOptions:\nA. 1 Start loop, 2 Initialize weights, 3 Compute gradients, 4 Update weights, 5 End loop\nB. 1 Initialize weights,  2 Compute gradients, 3 Start loop, 4 Update weights, 5 End loop\nC. 1 Initialize weights,  2 Start loop, 3 Update weights, 4 End loop, 5 Compute gradients \nD. 1 Initialize weights, 2 Start loop, 3 Compute gradients, 4 Update weights, 5 End Loop\n", "answer": "Explanations: First, let's break down each possible answer provided:\n\n1. Start loop, 2 Initialize weights, 3 Compute gradients, 4 Update weights, 5 End loop: This sequence seems logical as it starts the loop, initializes weights, computes gradients, updates weights, and then ends the loop. This order is commonly seen in gradient descent algorithms.\n\n2. Initialize weights, 2 Compute gradients, 3 Start loop, 4 Update weights, 5 End loop: This sequence doesn't seem correct as it initializes weights before starting the loop, which is against the usual order of operations in gradient descent.\n\n3. Initialize weights, 2 Start loop, 3 Update weights, 4 End loop, 5 Compute gradients: This sequence is also incorrect as updating weights should come after computing gradients in the optimization process.\n\n4. Initialize weights, 2 Start loop, 3 Compute gradients, 4 Update weights, 5 End Loop: This sequence appears to be the correct order as it initializes weights, starts the loop, computes gradients, updates weights, and ends the loop. \n\nBased on the typical workflow of gradient descent optimization algorithms, the correct answer would be 1. Initialize weights, 2. Start loop, 3. Compute gradients, 4. Update weights, 5. End loop. ///Answer: D"}
{"subject": "", "question": "Question: Let $n$ be an integer. Which of the following is \\emph{not} a group in the general case?\n\nOptions:\nA. $(\\mathbf{R},+)$\nB. $(\\mathbf{Q}\\setminus \\{0\\},\\times)$\nC. $(\\mathbf{Z}_n,+ \\pmod{n})$\nD. $(\\mathbf{Z}_n,\\times \\pmod{n})$\n", "answer": "Explanations: To determine which of the given structures is not a group in the general case, we need to check the four group axioms: closure, associativity, identity element, and inverse element.\n\n1) $(\\mathbf{R},+)$:\n- Closure: The real numbers are closed under addition.\n- Associativity: Addition is associative in real numbers.\n- Identity element: The identity element is 0.\n- Inverse element: Every element has an additive inverse.\nTherefore, $(\\mathbf{R},+)$ forms a group.\n\n2) $(\\mathbf{Q}\\setminus \\{0\\},\\times)$:\n- Closure: The non-zero rational numbers are closed under multiplication.\n- Associativity: Multiplication is associative in rational numbers.\n- Identity element: The identity element is 1.\n- Inverse element: Every non-zero element has a multiplicative inverse.\nTherefore, $(\\mathbf{Q}\\setminus \\{0\\},\\times)$ forms a group.\n\n3) $(\\mathbf{Z}_n,+ \\pmod{n})$:\n- Closure: Addition modulo $n$ is closed in $\\mathbf{Z}_n$.\n- Associativity: Modular addition is associative.\n- Identity element: The identity element is 0.\n- Inverse element: Every element has an additive inverse modulo $n$.\nTherefore, $(\\mathbf{Z}_n,+ \\pmod{n})$ forms a group for any integer $n$.\n\n4) $(\\mathbf{Z}_n,\\times \\pmod{n})$:\n- Closure: Multiplication modulo $n$ is not always closed in $\\mathbf{Z}_n$. For example, if $n$ is composite, there exist elements without multiplicative inverses.\n- Associativity: Multiplication modulo $n$ is associative.\n- Identity element: The identity element is 1.\n- Inverse element: Not every element has a multiplicative inverse modulo $n$, making it not a group in the general case for composite $n$.\nTherefore, $(\\mathbf{Z}_n,\\times \\pmod{n})$ is \\emph{not} a group in the general case when $n$ is composite.\n\nTherefore, the correct answer is 4) $(\\mathbf{Z}_n,\\times \\pmod{n})$. ///Answer: D"}
{"subject": "", "question": "Question: Given that $100000000003$ is prime, what is the cardinality of $\\mathbf{Z}_{200000000006}^*$?\n\nOptions:\nA. $2$\nB. $100000000002$\nC. $100000000003$\nD. $200000000006$\n", "answer": "Explanations: First, let's determine the cardinality of $\\mathbf{Z}_{200000000006}^*$. \n\nSince $100000000003$ is a prime number, we know that $\\mathbf{Z}_{100000000003}^*$ consists of all positive integers less than $100000000003$ that are coprime to $100000000003$. \n\nFor $n$ to be in $\\mathbf{Z}_{200000000006}^*$, it must be coprime to $200000000006$. \n\nThe cardinality of $\\mathbf{Z}_{200000000006}^*$ is therefore the number of positive integers less than $200000000006$ that are coprime to $200000000006$. \n\nBy Euler's totient function, $\\phi(n)$ gives the number of positive integers less than $n$ that are coprime to $n$. \n\nHence, the cardinality of $\\mathbf{Z}_{200000000006}^*$ is $\\phi(200000000006)$. \n\nBy the formula for Euler's totient function, since $200000000006$ can be expressed as $2 \\times 100000000003$, where $2$ and $100000000003$ are coprime, we can simplify $\\phi(200000000006)$ as $\\phi(2) \\times \\phi(100000000003)$.\n\nSince $\\phi(2) = 1$ (as $2$ is prime), the cardinality of $\\mathbf{Z}_{200000000006}^*$ is $\\phi(100000000003)$.\n\nAs $100000000003$ is prime, $\\phi(100000000003) = 100000000003$.\n\nTherefore, the correct answer is option 3: $100000000003$. ///Answer: C"}
{"subject": "", "question": "Question: Let P(x) is \u201cx is an elephant\u201d and F(x) is \u201cx flies\u201d and the domain consists of all animals. Translate the following statement into English: \u2203!x(P(x) \u2227 F(x))?\n\nOptions:\nA. There exists only one elephant that can fly\nB. There is an animal that flies if it\u2019s an elephant\nC. All elephants fly\nD. Some elephants can flyv\nE. None of the other options\n", "answer": "Explanations: The statement \u2203!x(P(x) \u2227 F(x)) translates to \"There exists only one elephant that can fly\" in English. This is because the symbol \u2203! represents \"there exists only one,\" P(x) represents \"x is an elephant,\" and F(x) represents \"x flies.\" \n\nTherefore, the statement is saying that there is at least one animal in the domain that is an elephant and can fly, but there is only one such animal that fits this criteria. This means that there are no other elephants in the domain that can fly, only this one unique elephant. \n\nThis is why the correct option is \"There exists only one elephant that can fly.\" ///Answer: A"}
{"subject": "", "question": "Question: Consider a MAC defined by $\\mathsf{MAC}: \\{0,1\\}^* \\times \\{0,1\\}^k \\mapsto \\{0,1\\}^n$. The complexity of a generic key recovery attacks against $\\mathsf{MAC}$ is \\ldots?\n\nOptions:\nA. $2^{k/2}$\nB. $2^k$\nC. $2^{n/2}$\nD. $2^n$\n", "answer": "Explanations: Answer 1: The complexity of a generic key recovery attack against the MAC defined by $\\mathsf{MAC}: \\{0,1\\}^* \\times \\{0,1\\}^k \\mapsto \\{0,1\\}^n$ is $2^k$. The security of the MAC is typically based on the strength of the secret key $k$, and a brute force key recovery attack would need to search through the entire key space, which has complexity $2^k$, to recover the secret key used by the MAC. ///Answer: B"}
{"subject": "", "question": "Question: Using a salt in a password authentication protocol\\dots?\n\nOptions:\nA. adds more spice in the protocol.\nB. avoids single-target and multi-target bruteforce attacks.\nC. \\textbf{requires} the salt to be encrypted in the database.\nD. avoids multi-target but not single-target bruteforce attacks.\n", "answer": "Explanations: The use of a salt in a password authentication protocol is primarily aimed at (2) avoiding single-target and multi-target brute force attacks. \n\nLet's break down each option:\n\n1. \"Adds more spice in the protocol\": While it's true that adding a salt adds complexity and unpredictability to the hashing process, the primary purpose of the salt is to increase security by mitigating brute force attacks, rather than just adding complexity for the sake of it.\n\n2. \"Avoids single-target and multi-target brute force attacks\": This is a key benefit of using a salt. By adding a unique salt to each password before hashing, it prevents attackers from using precomputed tables (like rainbow tables) to quickly crack passwords.\n\n3. \"Requires the salt to be encrypted in the database\": Salts do not need to be encrypted in the database. In fact, the purpose of a salt is to be stored alongside the hashed password in plaintext. Encrypting the salt would defeat its purpose of adding randomness to each password's hash.\n\n4. \"Avoids multi-target but not single-target brute force attacks\": This is not accurate. The use of a salt is effective in preventing both single-target and multi-target brute force attacks by ensuring that each password's hash is unique, even if two users have the same password.\n\nTherefore, the most accurate option is (2) \"avoids single-target and multi-target brute force attacks.\" ///Answer: B"}
{"subject": "", "question": "Question: A vending machine dispensing books of stamps accepts only $1 bills, $2 bills and $5 bills.\n\nFind a recurrence relation for the number of ways to deposit $$n$$ dollars in the vending machine, where the order in which the coins and bills are deposited matters.\n\nOptions:\nA. $$a_{n} = a_{n-1} + a_{n-2} + a_{n-5}$$\nB. $$a_{n+1} = a_{n-1} + a_{n-2} + a_{n-5}$$\nC. $$a_{n+1} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$\nD. $$a_{n} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$\n", "answer": "Explanations: #### **Answer**: C) $$a_{n+1} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$\n\n#### **Justification and Analysis**:\n- **Reasoning**: \nTo find the recurrence relation for the number of ways to deposit $$n$$ dollars in the vending machine, we consider the possible ways to deposit the last bill. The last bill can either be a $1 bill, $2 bill, or $5 bill. \n    - If the last bill deposited is a $1 bill, then there are $$a_{n-1}$$ ways to deposit the remaining $$(n-1)$$ dollars.\n    - If the last bill deposited is a $2 bill, then there are $$a_{n-2}$$ ways to deposit the remaining $$(n-2)$$ dollars.\n    - If the last bill deposited is a $5 bill, then there are $$a_{n-5}$$ ways to deposit the remaining $$(n-5)$$ dollars.\n\nTherefore, the total number of ways to deposit $$n$$ dollars can be expressed as the sum of these three cases, leading to the recurrence relation $$a_{n+1} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$.\n\n- **Option Analysis**:\n    - Option A) $$a_{n} = a_{n-1} + a_{n-2} + a_{n-5}$$: This option does not account for the different values of bills that can be deposited in the vending machine.\n    - Option B) $$a_{n+1} = a_{n-1} + a_{n-2} + a_{n-5}$$: This option is similar to Option A and does not consider the specific values of bills.\n    - Option D) $$a_{n} = a_{n-1} + 2a_{n-2} + 5a_{n-5}$$: This option does not consider the total number of ways to deposit $$n$$ dollars, as it lacks the understanding of the different cases for each type of bill. ///Answer: C"}
{"subject": "", "question": "Question: In the setting of EM, where $x_{n}$ is the data and $z_{n}$ is the latent variable, what quantity is called the posterior?\n\nOptions:\nA. (a) $\\square p\\left(\\mathbf{x}_{n} \\mid z_{n}, \\boldsymbol{\\theta}\\right)$\nB. (b) $\\square p\\left(\\mathbf{x}_{n}, z_{n} \\mid \\boldsymbol{\\theta}\\right)$\nC. (c) $\\square p\\left(z_{n} \\mid \\mathbf{x}_{n}, \\boldsymbol{\\theta}\\right)$\n", "answer": "Explanations: In the setting of Expectation-Maximization (EM), where \\(x_{n}\\) is the data and \\(z_{n}\\) is the latent variable, the quantity called the posterior is the conditional probability of the latent variable given the observed data and the model parameters. \n\nLet's match this with the given options:\n\n1. \\( p\\left(\\mathbf{x}_{n} \\mid z_{n}, \\boldsymbol{\\theta}\\right) \\)\n2. \\( p\\left(\\mathbf{x}_{n}, z_{n} \\mid \\boldsymbol{\\theta}\\right) \\)\n3. \\( p\\left(z_{n} \\mid \\mathbf{x}_{n}, \\boldsymbol{\\theta}\\right) \\)\n\nThe correct option is:\n\n3. \\( p\\left(z_{n} \\mid \\mathbf{x}_{n}, \\boldsymbol{\\theta}\\right) \\) ///Answer: C"}
{"subject": "", "question": "Question: What is TRUE regarding the Multi-head Self-attention mechanism?\n\nOptions:\nA. Its computation cannot be parallelized compared to LSTMs and other sequential models.\nB. It gives the Transformer the ability to learn different aspects of the meanings of each word.\nC. LSTMs have larger memory than models with self-attention.\nD. Its complexity is linear to the input size.\n", "answer": "Explanations: Let's analyze each of the options regarding the Multi-head Self-attention mechanism:\n\n1. **Its computation cannot be parallelized compared to LSTMs and other sequential models**:\n   - This statement is **incorrect**. The computation in the multi-head self-attention mechanism can be parallelized efficiently, allowing for parallel processing of input sequences. In contrast, sequential models like LSTMs process input sequentially, limiting parallelization.\n\n2. **It gives the Transformer the ability to learn different aspects of the meanings of each word**:\n   - This statement is **correct**. The multi-head self-attention mechanism in Transformers enables the model to capture different relationships and meanings of each word in the context of the input sequence. By utilizing multiple heads, the Transformer can learn various aspects of word meanings and dependencies.\n\n3. **LSTMs have larger memory than models with self-attention**:\n   - This statement is **incorrect**. The memory capacity of a model is not inherently tied to the choice between LSTMs and self-attention mechanisms. Memory capacity can be adjusted based on the model's architecture and parameters rather than the specific mechanism used.\n\n4. **Its complexity is linear to the input size**:\n   - This statement is **incorrect**. The complexity of the multi-head self-attention mechanism in Transformers is quadratic with respect to the input length. This is due to the computation involving each element of the sequence computing attention scores with every other element, resulting in a complexity of O(n^2 * d), where n is the sequence length and d is the model dimensionality.\n\nIn conclusion, the correct statement regarding the Multi-head Self-attention mechanism is:\n- It gives the Transformer the ability to learn different aspects of the meanings of each word. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion concerning WPA2?\n\nOptions:\nA. WPA2 uses RC4.\nB. WPA2 uses AES.\nC. WPA2 uses 64-bit keys.\nD. WPA2 is badly broken.\n", "answer": "Explanations: The correct assertion concerning WPA2 is:\n\n2. WPA2 uses AES.\n\nWPA2 (Wi-Fi Protected Access 2) primarily uses the Advanced Encryption Standard (AES) protocol for securing wireless networks. AES is a strong encryption algorithm and is considered to be secure for protecting data in transit. The other statements are incorrect - WPA2 does not use RC4, does not use 64-bit keys, and is not considered badly broken. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following properties is part of the RDF Schema Language?\n\nOptions:\nA. Type\nB. Predicate\nC. Description\nD. Domain\n", "answer": "Explanations: The correct answer is 'Domain'. \n\n- 'Type' is incorrect because it is not a property in the RDF Schema Language, it is a concept used to define the type of a resource.\n- 'Predicate' is incorrect because it is not a property in the RDF Schema Language, it is used to define relationships between resources.\n- 'Description' is incorrect because it is not a property in the RDF Schema Language, it is used to provide additional information about a resource.\n- 'Domain' is correct because it is a property in the RDF Schema Language used to specify the class that a property belongs to. It defines the domain of a property, i.e., the class of resources to which the property can be applied. ///Answer: D"}
{"subject": "", "question": "Question: I want to send a value to Bob without him knowing which value I sent and such that I cannot change my mind later when I reveal it in clear. I should use \\dots?\n\nOptions:\nA. a stream cipher.\nB. a PRNG.\nC. a commitment scheme.\nD. a digital signature.\n", "answer": "Explanations: The most suitable option for sending a value to Bob without him knowing which value was sent and preventing you from changing your mind later when revealing it is a commitment scheme.\n\nHere's the reasoning behind this choice:\n\n1. **Stream Cipher**: A stream cipher is used for encryption and does not provide a mechanism for commitment. It encrypts data for confidentiality but does not address the requirement of commitment.\n\n2. **PRNG (Pseudo Random Number Generator)**: A PRNG is used to generate random numbers, but it does not provide a way to commit to a specific value without revealing it. It does not ensure that the sender cannot change the value later.\n\n3. **Digital Signature**: A digital signature is used for authentication and integrity, ensuring that the sender is authentic and the message has not been tampered with. However, it does not fulfill the requirement of sending a value without revealing it or preventing the sender from changing the value later.\n\n4. **Commitment Scheme**: A commitment scheme allows a sender to commit to a specific value without revealing it and ensures that the sender cannot change the committed value later. When the sender is ready to reveal the value, they can demonstrate that the revealed value matches the committed value. This property makes a commitment scheme the most appropriate choice for the given scenario. ///Answer: C"}
{"subject": "", "question": "Question: You want to build a convolutional neural network to distinguish between types of cars in images. Your friend Alice, a biologist, has been working on a network to classify wildlife, which she calls WildNet. She spent several weeks training that network, and made it accessible to you. What can you do with it?\n\nOptions:\nA. Nothing, wildlife is not the same as cars.\nB. I can't reuse any of the weights of any layer, but I can take inspiration from the architecture of WildNet.\nC. I can freeze the last few layers, and then remove the early layers and replace it with my own re-learned layers. That way, I can make use of the generic features learned by WildNet.\nD. I can freeze the early layers, and then remove the last layers and replace it with my own re-learned layers. That way, I can make use of the generic features learned by WildNet.\nE. I can use WildNet right away for my task, without modifying anything.\n", "answer": "Explanations: When it comes to reusing a pre-trained neural network, especially in the context of transferring knowledge from one domain to another, there are several strategies one can consider. Let's break down each option:\n\n1) \\textbf{Nothing, wildlife is not the same as cars.}\n- This option is too definitive and dismissive. While it's true that wildlife and cars are different, the features learned by a neural network in one domain can still be useful in another domain, especially when dealing with visual recognition tasks like classifying images.\n\n2) \\textbf{I can't reuse any of the weights of any layer, but I can take inspiration from the architecture of WildNet.}\n- This option acknowledges the potential benefit of studying the architecture of WildNet for inspiration. While directly reusing the weights may not be feasible due to the differences in domains, understanding the overall structure and design choices of WildNet can help in designing a new network tailored for classifying cars.\n\n3) \\textbf{I can freeze the last few layers, and then remove the early layers and replace it with my own re-learned layers. That way, I can make use of the generic features learned by WildNet.}\n- Freezing the last few layers and replacing the early layers with new ones is a common transfer learning technique. By doing this, you can leverage the generic features learned by WildNet in its early layers (such as edge detectors or basic shapes) while adapting the later layers to classify cars specifically.\n\n4) \\textbf{I can freeze the early layers, and then remove the last layers and replace them with my own re-learned layers. That way, I can make use of the generic features learned by WildNet.}\n- This option is incorrect because freezing the early layers (which capture more general features) and replacing the later layers (which capture more specific features) would not be as beneficial for transferring knowledge to classify cars. It's more common to freeze the early layers and replace the later layers for transfer learning.\n\n5) \\textbf{I can use WildNet right away for my task, without modifying anything.}\n- This option is generally not the best approach because while WildNet may have learned useful features for wildlife classification, it might not directly translate well to classifying cars. Fine-tuning or adapting at least some parts of the network for the new task is usually recommended for better performance.\n\nTherefore, based on the given options, the most suitable choice for utilizing WildNet for classifying cars would be: \\textbf{I can freeze the last few layers, and then remove the early layers and replace it with my own re-learned layers. That way, I can make use of the generic features learned by WildNet.} ///Answer: C"}
{"subject": "", "question": "Question: Applying logarithmic scaling is useless if we use Z-Score standardization (also known as normalization) afterwards, as the standardization will undo the effects of the scaling.\n\nOptions:\nA. TRUE\nB. FALSE\n", "answer": "Explanations: First, let's understand the concepts of logarithmic scaling and Z-score standardization.\n\nLogarithmic scaling is a transformation applied to the data by taking the logarithm of each data point. This is often done to compress a wide range of values into a more manageable scale, making patterns in the data more easily visible.\n\nZ-score standardization (normalization) is a statistical method where data points are scaled to have a mean of 0 and a standard deviation of 1. This process allows for comparisons between different datasets by putting them on the same scale.\n\nNow, let's address the question of whether applying logarithmic scaling is useless if we use Z-score standardization afterwards:\n\nThe statement is FALSE.\n\nApplying logarithmic scaling and then Z-score standardization serves different purposes and can be useful in certain situations. Logarithmic scaling can help in handling skewed data distributions or data with a wide range of values by compressing the scale. Z-score standardization, on the other hand, standardizes the data to a common scale for comparison.\n\nWhen you apply logarithmic scaling first, you are transforming the data to a log scale. Subsequently applying Z-score standardization will then standardize these transformed values based on their means and standard deviations. This process can still be meaningful and useful, especially if the initial data required logarithmic scaling to make patterns more visible or to conform to certain assumptions of statistical tests.\n\nIn conclusion, applying logarithmic scaling before Z-score standardization can still have valid use cases and is not inherently useless. It all depends on the specific characteristics of the data and the objectives of the analysis. ///Answer: B"}
{"subject": "", "question": "Question: What adversarial model does not make sense for a message authentication code (MAC)?\n\nOptions:\nA. key recovery.\nB. universal forgery.\nC. existential forgery.\nD. decryption.\n", "answer": "Explanations: 4. Decryption.\n\nExplanation: A message authentication code (MAC) is utilized for verifying the integrity and authenticity of a message, not for encryption or decryption purposes. The adversarial model of decryption is irrelevant to a MAC's function, as its primary goal is to ensure message integrity and authenticity. Key recovery, universal forgery, and existential forgery are more suitable adversarial models for a MAC, as they pertain to potential security threats in message authentication. ///Answer: D"}
{"subject": "", "question": "Question: Which one of the following notions means that ``the information must be protected against any malicious modification''?\n\nOptions:\nA. privacy.\nB. integrity.\nC. confidentiality.\nD. reliability.\n", "answer": "Explanations: The correct answer to the question is \"integrity.\" \n\nTo help the student understand this concept, I would explain that integrity in information security refers to the assurance that data has not been altered or tampered with in an unauthorized way. This means that the information remains intact and trustworthy throughout its lifecycle. \n\nTo solve similar questions in the future, I would advise the student to remember that integrity is all about maintaining the accuracy and consistency of data, ensuring that it has not been maliciously modified. They can also think about real-world examples, such as ensuring that financial records have not been tampered with to understand the importance of integrity in information security. By understanding the core concept and reasoning behind each option, the student can more easily identify the correct answer in similar scenarios. ///Answer: B"}
{"subject": "", "question": "Question: Consider a hash function $H$ with $n$ output bits. Tick the \\emph{incorrect} assertion.\n\nOptions:\nA. Due to birthday paradox, an output collision of $H$ can be found much faster than with running time $2^n$.\nB. It is possible to find an output collision of $H$ with $O(2^{\\frac{n}{2}})$ memory and $O(2^{\\frac{n}{2}})$ running time.\nC. It is possible to find an output collision of $H$ with $O(1)$ memory and $O(2^{\\frac{n}{2}})$ running time.\nD. It is possible to find an output collision of $H$ with $O(2^{\\frac{n}{2}})$ memory and $O(1)$ running time.\n", "answer": "Explanations: Option 4: It is possible to find an output collision of $H$ with $O(2^{\\frac{n}{2}})$ memory and $O(1)$ running time is incorrect.\n\nIn the context of cryptographic hash functions, finding an output collision with $O(2^{\\frac{n}{2}})$ memory and $O(1)$ running time is not feasible or practical. A collision resistance property of a hash function implies that it should be computationally infeasible to find two distinct inputs that produce the same hash output.\n\nOptions 1, 2, and 3 are correct in their assertions:\n- Option 1 correctly states that due to the birthday paradox, an output collision of $H$ can be found much faster than with a running time of $2^n$.\n- Option 2 correctly states that it is possible to find an output collision of $H$ with $O(2^{\\frac{n}{2}})$ memory and $O(2^{\\frac{n}{2}})$ running time.\n- Option 3 correctly states that it is possible to find an output collision of $H$ with $O(1)$ memory and $O(2^{\\frac{n}{2}})$ running time.\n\nTherefore, the incorrect assertion is Option 4. ///Answer: D"}
{"subject": "", "question": "Question: Consider the following joint distribution on $X$ and $Y$, where both random variables take on the values $\\{0,1\\}: p(X=$ $0, Y=0)=0.1, p(X=0, Y=1)=0.2, p(X=1, Y=0)=0.3, p(X=1, Y=1)=0.4$. You receive $X=1$. What is the largest probability of being correct you can achieve when predicting $Y$ in this case?\n\nOptions:\nA. $\\frac{1}{3}$\nB. $\\frac{3}{4}$\nC. $\\frac{1}{7}$\nD. $0$\nE. $1$\nF. $\\frac{2}{3}$\nG. $\\frac{6}{7}$\nH. $\\frac{4}{7}$\nI. $\\frac{3}{7}$\nJ. $\\frac{1}{4}$\nK. $\\frac{2}{4}$\n", "answer": "Explanations: To determine the largest probability of being correct when predicting $Y$ given that $X=1$, we need to find the conditional probability $P(Y|X=1)$. \n\nFrom the joint distribution given, we have:\n- $p(X=1, Y=0) = 0.3$\n- $p(X=1, Y=1) = 0.4$\n\nTo find the conditional probability $P(Y=0|X=1)$, we use the formula for conditional probability:\n\\[P(Y=0|X=1) = \\frac{P(X=1, Y=0)}{P(X=1)}\\]\n\\[P(Y=0|X=1) = \\frac{0.3}{0.3+0.4} = \\frac{0.3}{0.7} = \\frac{3}{7}\\]\n\nSimilarly, the conditional probability $P(Y=1|X=1)$ is:\n\\[P(Y=1|X=1) = \\frac{P(X=1, Y=1)}{P(X=1)}\\]\n\\[P(Y=1|X=1) = \\frac{0.4}{0.3+0.4} = \\frac{0.4}{0.7} = \\frac{4}{7}\\]\n\nTherefore, the largest probability of being correct when predicting $Y$ given that $X=1$ is $\\frac{4}{7}$, which corresponds to Option 7. ///Answer: G"}
{"subject": "", "question": "Question: Given 3 users with ratings\nu1: 1, 3\nu2: 2, 4\nu3: 1, 4?\n\nOptions:\nA. Sim_corr(u1, u2) >  Sim_corr(u1, u3)\nB. Sim_corr(u1, u2) = Sim_corr(u1, u3)\nC. Sim_corr(u1, u2) < Sim_corr(u1, u3)\n", "answer": "Explanations: To determine whether Sim_corr(u1, u2) is greater than, equal to, or less than Sim_corr(u1, u3), we first need to calculate the Pearson correlation coefficient for each pair of users.\n\nThe Pearson correlation coefficient is a measure of the linear correlation between two variables. In this case, we can calculate the Pearson correlation coefficient between the ratings of users u1 and u2, and between the ratings of users u1 and u3.\n\nLet's calculate the Pearson correlation coefficient for each pair of users:\n\nFor users u1 and u2:\n- Ratings for u1: 1, 3\n- Ratings for u2: 2, 4\n\nTo calculate the Pearson correlation coefficient, we first need to calculate the mean rating for each user:\n- Mean rating for u1: (1 + 3) / 2 = 2\n- Mean rating for u2: (2 + 4) / 2 = 3\n\nNext, we calculate the covariance between the ratings of u1 and u2:\n- Cov(u1, u2) = [(1 - 2) * (2 - 3) + (3 - 2) * (4 - 3)] / 2 = (1 * -1 + 1 * 1) / 2 = 0 / 2 = 0\n\nThen, we calculate the standard deviation for each user:\n- StdDev(u1) = sqrt[((1 - 2)^2 + (3 - 2)^2) / 2] = sqrt[(1 + 1) / 2] = sqrt(1) = 1\n- StdDev(u2) = sqrt[((2 - 3)^2 + (4 - 3)^2) / 2] = sqrt[(1 + 1) / 2] = sqrt(1) = 1\n\nFinally, we calculate the Pearson correlation coefficient:\n- Pearson Correlation(u1, u2) = Cov(u1, u2) / (StdDev(u1) * StdDev(u2)) = 0 / (1 * 1) = 0\n\nNow, let's calculate the Pearson correlation coefficient for users u1 and u3:\n- Ratings for u1: 1, 3\n- Ratings for u3: 1, 4\n\nSimilarly, we calculate the mean ratings for u1 and u3:\n- Mean rating for u1: 2\n- Mean rating for u3: 2.5\n\nNext, we calculate the covariance between the ratings of u1 and u3:\n- Cov(u1, u3) = [(1 - 2) * (1 - 2.5) + (3 - 2) * (4 - 2.5)] / 2 = (-1 * -1.5 + 1 * 1.5) / 2 = 2.25 / 2 = 1.125\n\nThen, we calculate the standard deviation for each user:\n- StdDev(u1) = 1\n- StdDev(u3) = sqrt[((1 - 2)^2 + (4 - 2.5)^2) / 2] = sqrt[(1 + 2.25) / 2] = sqrt(1.625) \u2248 1.27\n\nFinally, we calculate the Pearson correlation coefficient:\n- Pearson Correlation(u1, u3) = Cov(u1, u3) / (StdDev(u1) * StdDev(u3)) = 1.125 / (1 * 1.27) \u2248 0.885\n\nTherefore, the Pearson correlation coefficient between users u1 and u2 is 0, and the Pearson correlation coefficient between users u1 and u3 is approximately 0.885.\n\nComparing the two coefficients:\n- Sim_corr(u1, u2) < Sim_corr(u1, u3)\n\nTherefore, the correct response is: Sim_corr(u1, u2) < Sim_corr(u1, u3). ///Answer: C"}
{"subject": "", "question": "Question: Consider the function $f: \\R\to \\R$, $f(x) = \\lvert x - 2023 \rvert$. A subgradient of $f$ at $x = 2023$ exists \textbf{and} is unique.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: To determine whether a subgradient of the function $f(x) = \\lvert x - 2023 \\rvert$ at $x = 2023$ exists and is unique, we need to consider the behavior of the absolute value function at that point.\n\nThe absolute value function $f(x) = \\lvert x - 2023 \\rvert$ can be rewritten as:\n\n$$\nf(x) = \n\\begin{cases}\nx - 2023, & \\text{if } x \\geq 2023 \\\\\n2023 - x, & \\text{if } x < 2023\n\\end{cases}\n$$\n\nAt $x = 2023$, the function $f(x)$ is not differentiable since the left and right derivatives do not match. However, in the subdifferential or subgradient sense, we can consider a subgradient for convex functions.\n\nLet's analyze the choices:\n\nA. True\n\nSince the function $f(x) = \\lvert x - 2023 \\rvert$ is not differentiable at $x = 2023$, a subgradient exists and is not unique. Any value between $[-1, 1]$ can be considered as a subgradient at $x = 2023$.\n\nTherefore, the correct answer is:\n\nB. False\n\nThis is because a subgradient of $f$ at $x = 2023$ exists, but it is not unique due to the non-differentiability of the absolute value function at that point. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements on Latent Semantic Indexing (LSI) and Word Embeddings (WE) is false?\n\nOptions:\nA. LSI is deterministic (given the dimension), whereas WE is not\nB. LSI does not depend on the order of words in the document, whereas WE does\nC. The dimensions of LSI can be interpreted as concepts, whereas those of WE cannot\nD. LSI does take into account the frequency of words in the documents, whereas WE with negative sampling does not\n", "answer": "Explanations: {\n\u00a0 \u00a0 \"explaination\": \"The dimensions of LSI can be interpreted as concepts, whereas those of WE cannot\",\n\u00a0 \u00a0 \"answer\": \"LSI does take into account the frequency of words in the documents, whereas WE with negative sampling does not\"\n} ///Answer: D"}
{"subject": "", "question": "Question: For two $\\ell$-bit numbers, the \\textbf{tightest} complexity to compute their gcd is?\n\nOptions:\nA. $\\mathcal{O}(2^{\\ell})$.\nB. $\\mathcal{O}(\\ell^3)$.\nC. $\\mathcal{O}(\\ell^2)$.\nD. $\\mathcal{O}(\\log{\\ell}^2)$.\n", "answer": "Explanations: The tightest complexity to compute the greatest common divisor (gcd) of two $\\ell$-bit numbers is $\\mathcal{O}(\\ell^2)$. \n\nTo understand this better, let's delve into the two algorithms involved:\n\n1. Euclidean Algorithm:\nThe Euclidean Algorithm computes the gcd of two integers using repeated division. For $\\ell$-bit numbers, each division operation can be done in $\\mathcal{O}(\\ell^2)$ time. However, the crucial point is the number of iterations the algorithm takes, which is proportional to $\\ell$ for $\\ell$-bit numbers. Combining these, the Euclidean algorithm for two $\\ell$-bit numbers runs in $\\mathcal{O}(\\ell^3)$ time in the worst case.\n\n2. Binary GCD Algorithm:\nThe Binary GCD Algorithm improves on the basic Euclidean algorithm by using bitwise operations, which are more efficient with bit-level manipulations. The complexity of the binary GCD algorithm is $\\mathcal{O}(\\ell^2)$ for two $\\ell$-bit numbers. This improvement arises from reducing the need for division operations, relying instead on shifts and subtraction, which are computationally cheaper.\n\nComparing the complexities of the two algorithms, we see that the tightest complexity is $\\mathcal{O}(\\ell^2)$, as it represents the most efficient algorithm for computing the gcd of two $\\ell$-bit numbers.\n\nTherefore, the correct answer is $\\mathcal{O}(\\ell^2)$. ///Answer: C:"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. Assume that $C$ is an arbitrary random permutation.\n\nOptions:\nA. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a}(C)$\nB. $\\mathsf{BestAdv}_n^{n.a.}(C,C^\\ast)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a}(C)$\nC. $E(\\mathsf{LP}^{C}(a,b)) < \\frac{1}{2}$\nD. $\\mathsf{BestAdv}_n^{n.a.}(C,C^\\ast)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty}(C)$\n", "answer": "Explanations: Let's break down each option:\n\n1. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a}(C)$\n\nThis statement is generally true in the context of cryptographic security analysis. Here, $\\mathsf{BestAdv}_n(C,C^\\ast)$ represents the best advantage an adversary can achieve in distinguishing between a random permutation $C$ and a fixed permutation $C^\\ast$. On the other side, $\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a}(C)$ denotes the advantage of a specific adversary $\\mathsf{a}$ in breaking the security of the cryptographic system using the norm $\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a$. Therefore, the equality between these two expressions suggests that the adversary's advantage is equal to the decryption advantage under the specified norm.\n\n2. $\\mathsf{BestAdv}_n^{n.a.}(C,C^\\ast)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a}(C)$\n\nThis statement introduces the additional factor of $\\frac{1}{2}$ in front of the decryption advantage term. While the exact reasoning behind this division factor is not explicitly provided, it could potentially be a normalization factor to scale the decryption advantage to a standard form. However, without further context or explanation, this statement may not hold universally.\n\n3. $E(\\mathsf{LP}^{C}(a,b)) < \\frac{1}{2}$\n\nThe notation $E(\\mathsf{LP}^{C}(a,b))$ typically represents the probability of success in solving a learning parity with noise problem using an oracle $C$ with inputs $a$ and $b$. In this statement, the inequality $E(\\mathsf{LP}^{C}(a,b)) < \\frac{1}{2}$ implies that the success probability of solving the problem is less than $\\frac{1}{2}$. This condition aligns with the difficulty level of the problem and is often desirable in cryptographic scenarios.\n\n4. $\\mathsf{BestAdv}_n^{n.a.}(C,C^\\ast)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty}(C)$\n\nSimilar to the first option, this statement equates the best advantage of an adversary in distinguishing between random permutations to half of the decryption advantage under the infinity norm $\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty$. The infinity norm represents the maximum absolute value of a vector or function. This equality may hold true in specific scenarios where the decryption process is tied to the infinity norm.\n\nIn conclusion, the true assertion among the options provided is:\n\n1. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\mathsf{Dec}^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a}(C)$\n\nThis statement aligns with the general relationship between an adversary's advantage and the decryption process in cryptographic settings. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{\\emph{incorrect}} assertion.\n\nOptions:\nA. $P\\subseteq NP$.\nB. $NP\\subseteq IP$.\nC. $PSPACE\\subseteq IP$.\nD. $NP\\mbox{-hard} \\subset P$.\n", "answer": "Explanations: The incorrect assertion is statement 4, which claims that $NP\\mbox{-hard} \\subset P$. This statement is incorrect because $NP\\mbox{-hard}$ problems are at least as hard as the hardest problems in $NP$, but it does not necessarily mean that they are in the class $P$. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. In a multiplicative cyclic group $G$ of order $m > 1$ with neutral element $e_G$ \\ldots?\n\nOptions:\nA. $\\lambda = m$, where $\\lambda$ is the exponent of $G$.\nB. the order of every element $x \\in G$ is $m$.\nC. there exists $g \\in G$ that generates the whole group.\nD. for any $x \\in G$, we have that $x^m = e_{G}$.\n", "answer": "Explanations: The incorrect assertion is: '$\\lambda = m$, where $\\lambda$ is the exponent of $G$.'\n\nExplanation:\n- The first assertion states that there exists $g \\in G$ that generates the whole group. This is true by definition of a cyclic group.\n- The second assertion states that $\\lambda = m$, where $\\lambda$ is the exponent of $G$. This is incorrect because the exponent of a group is defined as the smallest positive integer $\\lambda$ such that $x^\\lambda = e_G$ for all $x \\in G$. In a cyclic group of order $m$, the exponent is actually equal to the order of the group, not necessarily $m$.\n- The third assertion states that the order of every element $x \\in G$ is $m$. This is true in a cyclic group of order $m$.\n- The fourth assertion states that for any $x \\in G$, we have that $x^m = e_{G}$. This is true in a cyclic group of order $m$.\n\nTherefore, the incorrect assertion is that $\\lambda = m$, where $\\lambda$ is the exponent of $G. ///Answer: A"}
{"subject": "", "question": "Question: Select all valid answers about UNIX-like shell.\n\nOptions:\nA. The shell is a program, that runs in user-space.\nB. The shell is a program, that runs in kernel-space.\nC. The shell is a program, which reads from standard input.\nD. The shell is a function inside kernel.\nE. The shell is the layer, which has to be always used for communicating with kernel.\nF. The shell must run only in a single instance. Multiple running instances cause memory corruption.\nG. The shell is a user interface for UNIX-like systems.\n", "answer": "Explanations: - The shell is a program, that runs in user-space.\n- The shell is a program, which reads from standard input.\n- The shell is a user interface for UNIX-like systems.\n\nThe shell is a program that operates in user-space, separate from the kernel. It reads commands from standard input and interprets them to interact with the operating system. The shell serves as the primary user interface for UNIX-like systems, allowing users to execute commands and manage the system efficiently. ///Answer: A"}
{"subject": "", "question": "Question: Consider a group $G$ with prime order. We can deduce that?\n\nOptions:\nA. all elements are generators.\nB. all elements are generators except the neutral element.\nC. half of the elements are generators.\nD. $G$ is a field.\n", "answer": "Explanations: In a group $G$ with prime order, every non-identity element is a generator of the group. This means that Answer 2 is correct: all elements except the neutral element are generators. This is due to the fact that for a group of prime order $p$, where $p$ is a prime number, all elements other than the identity have no nontrivial subgroups, so they generate the entire group. ///Answer: B"}
{"subject": "", "question": "Question: Consider optimizing a matrix factorization $\\boldsymbol{W} \\boldsymbol{Z}^{\\top}$ in the matrix completion setting, for $\\boldsymbol{W} \\in \\mathbb{R}^{D \\times K}$ and $\\boldsymbol{Z} \\in \\mathbb{R}{ }^{N \\times K}$. We write $\\Omega$ for the set of observed matrix entries. Which of the following statements are correct?\n\nOptions:\nA. Given any $\\Omega$, for $K:=\\min \\{N, D\\}$, there is an exact solution to the problem.\nB. In general, a step of $\\mathrm{SGD}$ will change all entries of the $\\mathbf{W}$ and $\\mathbf{Z}$ matrices.\nC. Adding a Frob-norm regularizer for $\\boldsymbol{W}$ and $\\boldsymbol{Z}$ to the matrix factorization objective function makes the objective convex.\nD. A step of alternating least squares is more costly than an SGD step.\nE. For complete observations $\\Omega=[1 \\ldots D] \\times[1 \\ldots N]$, the problem can be solved by the singular value decomposition.\nF. The cost of an SGD step depends on the number of observed entries.\n", "answer": "Explanations: Answer: The correct statements are:\n\n- Adding a Frob-norm regularizer for $\\boldsymbol{W}$ and $\\boldsymbol{Z}$ to the matrix factorization objective function makes the objective convex. This statement is true because adding a Frobenius norm regularizer makes the objective function convex.\n\n- For complete observations $\\Omega=[1 \\ldots D] \\times[1 \\ldots N]$, the problem can be solved by the singular value decomposition. This statement is true because when all entries are observed, the matrix factorization problem can be solved exactly using singular value decomposition.\n\n- The cost of an SGD step depends on the number of observed entries. This statement is true because in stochastic gradient descent, the cost of each step is influenced by the number of observed entries used in the gradient calculation.\n\nReasoning for the incorrect statements:\n\n- Given any $\\Omega$, for $K:=\\min \\{N, D\\}$, there is an exact solution to the problem. This statement is incorrect because the existence of an exact solution is not guaranteed for any $\\Omega$.\n\n- In general, a step of $\\mathrm{SGD}$ will change all entries of the $\\mathbf{W}$ and $\\mathbf{Z}$ matrices. This statement is incorrect because in stochastic gradient descent, only a subset of entries is updated in each step, not all entries. \n\n- A step of alternating least squares is more costly than an SGD step. This statement is incorrect because alternating least squares can be more computationally efficient than stochastic gradient descent for matrix factorization problems. ///Answer: E"}
{"subject": "", "question": "Question: Take the recurrence relation $$a_n = -3a_{n-1} + 4a_{n-2}$$ with initial conditions $$a_0 = 1$$, $$a_1=3$$ and transform it in the form $$a_n = \u0007lpha_1r_1^n + \u0007lpha_2r_2^n$$. Which statement is true?\n\nOptions:\nA. $$\u0007lpha_1 = \frac{4}{5}$$ and $$\u0007lpha_2 = \frac{1}{5}$$\nB. $$\u0007lpha_1 = -4$$ and $$\u0007lpha_2=1$$\nC. $$\u0007lpha_1 = \frac{7}{5}$$ and $$\u0007lpha_2 = \frac{-2}{5}$$\nD. $$\u0007lpha_1 = \frac{13}{5}$$ and $$\u0007lpha_2 = \frac{-7}{5}$$\n", "answer": "Explanations: Given recurrence relation: $a_n = -3a_{n-1} + 4a_{n-2}$\n\nInitial conditions: $a_0 = 1$, $a_1 = 3$\n\nThe characteristic equation of the given recurrence relation is found by substituting $a_n = r^n$ into the recurrence relation:\n\n$r^n = -3r^{n-1} + 4r^{n-2}$\n\nDividing by $r^{n-2}$, we get:\n\n$r^2 = -3r + 4$\n\nThe characteristic equation is then:\n\n$r^2 + 3r - 4 = 0$\n\nSolving this quadratic equation gives us the roots $r_1 = 1$ and $r_2 = -4$.\n\nTherefore, the general solution of the recurrence relation can be written as:\n\n$a_n = \\alpha_1(1)^n + \\alpha_2(-4)^n$\n\nGiven the initial conditions $a_0 = 1$ and $a_1 = 3$, we substitute these values into the general solution to find the values of $\\alpha_1$ and $\\alpha_2:\n\nFrom $a_0 = 1$:\n\n$1 = \\alpha_1 + \\alpha_2$\n\nFrom $a_1 = 3$:\n\n$3 = \\alpha_1(1) + \\alpha_2(-4)$\n$3 = \\alpha_1 - 4\\alpha_2$\n\nSolving these two equations simultaneously, we find $\\alpha_1 = \\frac{7}{5}$ and $\\alpha_2 = -\\frac{2}{5}$.\n\nTherefore, the correct answer is:\n\nC: $\\alpha_1 = \\frac{7}{5}$ and $\\alpha_2 = -\\frac{2}{5}$.\n\nThis means that statement C is true. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following is the powerset of \\(A= \\{\\{\\emptyset\\}\\}\\):?\n\nOptions:\nA. \\( \\emptyset \\)\nB. \\(  \\{\\emptyset\\} \\)\nC. \\( \\{\\{\\emptyset\\}, \\emptyset\\} \\)\nD. \\( \\{\\{\\{\\emptyset\\}\\}, \\emptyset\\} \\)\n", "answer": "Explanations: Answer A: \n\nOption 1: (False. The powerset of a set includes all possible subsets of that set, including the empty set. Therefore, the empty set will be included in the powerset of any set.)\nOption 2: (True. The powerset of \\(A= \\{\\{\\emptyset\\}\\}\\) will include the set containing the empty set, which is \\(\\{\\emptyset\\}\\).)\nOption 3: (False. The powerset of \\(A= \\{\\{\\emptyset\\}\\}\\) will include both the set \\(\\{\\emptyset\\}\\) and the set \\(\\{\\{\\emptyset\\}\\}\\), but not the set \\(\\{\\{\\{\\emptyset\\}\\}, \\emptyset\\}\\) as it is not a subset of \\(A\\).)\nOption 4: (False. The powerset of \\(A= \\{\\{\\emptyset\\}\\}\\) will not include the set \\(\\{\\{\\{\\emptyset\\}\\}, \\emptyset\\}\\) as it is not a subset of \\(A\\).) ///Answer: B"}
{"subject": "", "question": "Question: Consider a linear regression problem with $N$ samples where the input is in $D$-dimensional space, and all output values are $y_{i} \\in\\{-1,+1\\}$. Which of the following statements is correct?\n\nOptions:\nA. (a)  linear regression cannot \"work\" if $N \\gg D$\nB. (b)  linear regression cannot \"work\" if $N \\ll D$\nC. (c)  linear regression can be made to work perfectly if the data is linearly separable\n", "answer": "Explanations: Answer: (c) linear regression can be made to work perfectly if the data is linearly separable.\n\nRationale: In the given scenario where the output values are binary (-1 or +1), linear regression can still be applied. If the data is linearly separable, meaning there exists a hyperplane that can perfectly separate the two classes, linear regression can indeed work perfectly. In this case, the model will be able to find a linear function that can separate the classes accurately. However, it is important to note that linear regression may not perform well if the data is not linearly separable, regardless of the values of $N$ and $D$. ///Answer: C"}
{"subject": "", "question": "Question: Which one of these Ciphers is perfectly secure?\n\nOptions:\nA. Hieroglyphs\nB. Caesar\nC. Vigen\\`{e}re\nD. Vernam\n", "answer": "Explanations: The only cipher that is perfectly secure is the Vernam cipher. This is because the Vernam cipher uses a one-time pad, where the key is as long as the message and is completely random. This makes it impossible for an attacker to decipher the message without the key, ensuring perfect security ///Answer: D"}
{"subject": "", "question": "Question: Which of AddressSanitizer (ASan), MemorySanitizer (MemSan),\n    UndefinedBehaviorSanitizer (UBSan) or ThreadSanitizer (TSan) can detect\n    bugs (if any) in the following code snippet?\n\n    \\begin{lstlisting}[language=C,style=c]\nint sum_up_to(int x) {}\n    // Return sum of integers up to x\n    int result = x;\n    for (int i = x; i >= 0; i--) {\n        if (INT_MAX - i <= result) {\n            break;\n        }\n        result += i;\n    }\n    return result;\n}\n    \\end{lstlisting}?\n\nOptions:\nA. There is at least one bug in the snippet, but none of the\n            mentioned sanitizers can detect it.\nB. ASan\nC. MemSan\nD. UBSan\nE. TSan\nF. There are no bugs in the snippet.\n", "answer": "Explanations: Let's think step-by-step:\n\nThe bug in the code snippet can be detected by **UndefinedBehaviorSanitizer (UBSan)**. The issue arises from the loop condition potentially causing an overflow when `i` reaches `INT_MIN`.\n\nTherefore, UBSan is the sanitizer that can detect the bug in the provided code snippet. ///Answer: D"}
{"subject": "", "question": "Question: What is the content of the inode?\n\nOptions:\nA. Filename\nB. File mode\nC. Hard links counter\nD. String with the name of the owner\nE. File size\nF. Capacity of the whole file system\nG. Index structure for data blocks\n", "answer": "Explanations:  To determine the content of an inode, we need to understand the structure of an inode in a Unix-like file system. Inodes are data structures that store metadata about files, such as file permissions, ownership, timestamps, and pointers to data blocks where the actual file contents are stored. Let's analyze each option to determine if it is true or false:\n\n0: \"Filename\" False. Inodes do not store the filename. The filename is stored in the directory entry that points to the inode.\n\n1: \"File mode\" True. The file mode, which includes permissions and file type information, is stored in the inode.\n\n2: \"Hard links counter\" True. The hard links counter, which keeps track of the number of hard links pointing to the inode, is stored in the inode.\n\n3: \"String with the name of the owner\" False. The owner's username is associated with the inode, but it is stored as a numerical user ID (UID) rather than a string with the name of the owner.\n\n4: \"File size\" True. The file size, which indicates the size of the file in bytes, is stored in the inode.\n\n5: \"Capacity of the whole file system\" False. The capacity of the entire file system is not stored in the inode. It is a property of the file system itself, not individual inodes.\n\n6: \"Index structure for data blocks\" True. The index structure for data blocks, which includes pointers to the data blocks containing the file's actual content, is stored in the inode.\n\nBased on the analysis above, the correct answer is: 1:\"File mode\" 2:\"Hard links counter\" 4:\"File size\" 6:\"Index structure for data blocks\"\n\nTherefore, the correct answer is the combination of options 1, 2, 4, and 6. ///Answer: B"}
{"subject": "", "question": "Question: Let $(G,+), (H,\\times)$ be two groups and $f:G\\to H$ be an homomorphism. For $x_1,x_2 \\in G$, we have:?\n\nOptions:\nA. $f(x_1) + f(x_2)=f(x_1)\\times f(x_2)$\nB. $f(x_1 + x_2)=f(x_1)\\times f(x_2)$\nC. $f(x_1 + f(x_2))=f(x_1\\times f(x_2))$\nD. $f(x_1 \\times x_2)=f(x_1)+ f(x_2)$\n", "answer": "Explanations: The correct answer is: '$f(x_1 + x_2)=f(x_1)\\\\times f(x_2)$'.\n\nThis is the correct answer because the definition of a homomorphism between groups states that for all $x,y \\in G$, $f(x+y) = f(x) \\times f(y)$. In this case, $x_1$ and $x_2$ are elements of the group $G$, so the correct statement is that $f(x_1 + x_2) = f(x_1) \\times f(x_2)$. The other options do not align with the definition of a homomorphism, hence they are incorrect. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following is well preserved by 2G?\n\nOptions:\nA. Confidentiality\nB. Message Integrity\nC. Challenge freshness\nD. Authentication of Mobile Station\n", "answer": "Explanations: The correct options that are well preserved by 2G are 'Challenge freshness' and 'Authentication of Mobile Station'.\n\n1. Challenge freshness: 2G networks maintain challenge freshness by generating unique challenges for authentication purposes. This helps prevent replay attacks where an attacker captures and reuses authentication data to gain unauthorized access. By using fresh challenges, 2G ensures the security of the authentication process and protects against such attacks.\n\n2. Authentication of Mobile Station: 2G networks implement authentication protocols to verify the identity of mobile stations before granting access to the network. This ensures that only authorized users can connect to the network, enhancing security and preventing unauthorized access or fraudulent activities.\n\nIn summary, 2G effectively preserves challenge freshness by generating unique challenges and maintains the authentication of mobile stations to verify user identities, thus enhancing the overall security of the network. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement. Let $p$ be a prime number, ...\n\nOptions:\nA. $\\forall x \\in \\mathbb{Z}_p$, $x \\neq 0 \\Longrightarrow x$ is a generator of $\\mathbb{Z}_p$.\nB. $\\forall x \\in \\mathbb{Z}_p^*$, $x \\neq 0 \\Longrightarrow x$ is a generator of $\\mathbb{Z}_p^*$.\nC. $\\forall x \\in \\mathbb{Z}_p^*$, $x$ is invertible.\nD. $\\mathbb{Z}_p^*$ is isomorphic to $\\mathbb{Z}_{p-1}$.\n", "answer": "Explanations: The false statement among the options is: \"$\\forall x \\in \\mathbb{Z}_p$, $x \\neq 0 \\Longrightarrow x$ is a generator of $\\mathbb{Z}_p$.\"\n\nJustification:\n- In $\\mathbb{Z}_p$, where $p$ is a prime number, the elements are $\\{0, 1, 2, ..., p-1\\}$.\n- For an element $x$ in $\\mathbb{Z}_p$, it cannot be a generator of $\\mathbb{Z}_p$ if $x$ does not generate all other elements in $\\mathbb{Z}_p$ through repeated addition modulo $p$.\n- Not all elements in $\\mathbb{Z}_p$ are generators. For example, in $\\mathbb{Z}_5$, 1 and 4 are generators, but 2 and 3 are not generators.\n- Therefore, the statement \"$\\forall x \\in \\mathbb{Z}_p$, $x \\neq 0 \\Longrightarrow x$ is a generator of $\\mathbb{Z}_p$\" is false as not all elements in $\\mathbb{Z}_p$ are generators. ///Answer: A"}
{"subject": "", "question": "Question: What is the difference between C++'s \\texttt{static\\_cast} and \\texttt{dynamic\\_cast}?\n\nOptions:\nA. \\texttt{static\\_cast} does not perform any kind of runtime check, while \\texttt{dynamic\\_cast} performs runtime checks on the validity of the cast.\nB. \\texttt{static\\_cast} is faster but less safe than \\texttt{dynamic\\_cast}.\nC. \\texttt{static\\_cast} can only be applied to static classes whereas \\texttt{dynamic\\_cast} works for any class.\nD. \\texttt{static\\_cast} does not work on already-casted objects, while \\texttt{dynamic\\_cast} works always.\n", "answer": "Explanations: - \\texttt{static\\_cast} does not perform any kind of runtime check, while \\texttt{dynamic\\_cast} performs runtime checks on the validity of the cast. This statement is correct as \\texttt{static\\_cast} does not provide any runtime checks.\n- \\texttt{static\\_cast} is faster but less safe than \\texttt{dynamic\\_cast}. This statement is also accurate as \\texttt{static\\_cast} is faster but may lead to undefined behavior if used incorrectly.\n- \\texttt{static\\_cast} can only be applied to static classes whereas \\texttt{dynamic\\_cast} works for any class. This statement is incorrect as both \\texttt{static\\_cast} and \\texttt{dynamic\\_cast} can be used with polymorphic classes.\n- \\texttt{static\\_cast} does not work on already-casted objects, while \\texttt{dynamic\\_cast} works always. This statement is incorrect as \\texttt{static\\_cast} can be used on objects that have already been casted.\n\nBased on the analysis, the correct differences between \\texttt{static\\_cast} and \\texttt{dynamic\\_cast} are:\n1. \\texttt{static\\_cast} does not perform any kind of runtime check, while \\texttt{dynamic\\_cast} performs runtime checks on the validity of the cast.\n2. \\texttt{static\\_cast} is faster but less safe than \\texttt{dynamic\\_cast}.\n\nTherefore, the chosen options are:\n- \\texttt{static\\_cast} does not perform any kind of runtime check, while \\texttt{dynamic\\_cast} performs runtime checks on the validity of the cast.\n- \\texttt{static\\_cast} is faster but less safe than \\texttt{dynamic\\_cast}. ///Answer: A"}
{"subject": "", "question": "Question: Which of AddressSanitizer (ASan), MemorySanitizer (MemSan),\n    UndefinedBehaviorSanitizer (UBSan) or ThreadSanitizer (TSan) can detect\n    bugs (if any) in the following code snippet?\n\n    \\begin{lstlisting}[language=C,style=c]\nint sum_array(int *arr, size_t len) {\n    // Return sum of array elements\n    int result = 0;\n    for (size_t i = 0; i <= len; i++) {\n        result += arr[i];\n    }\n    return result;\n}\n    \\end{lstlisting}?\n\nOptions:\nA. ASan\nB. MemSan\nC. UBSan\nD. TSan\nE. There is at least one bug in the snippet, but none of the\n            mentioned sanitizers can detect it.\nF. There are no bugs in the snippet.\n", "answer": "Explanations: Alright, let's analyze the given code snippet step-by-step to determine if there are any bugs present and which sanitizer can detect them.\n\n1. The function `sum_array` takes an array `arr` and its length `len` as input parameters.\n2. It initializes a variable `result` to store the sum of elements.\n3. It then enters a for loop that iterates from `i=0` to `i<=len`, attempting to add `arr[i]` to the `result`.\n\nNow, let's identify the potential issue in the code:\n- The loop condition `i <= len` is incorrect. It should be `i < len` because array indexing in C starts from 0. Accessing `arr[len]` would go out of bounds, leading to undefined behavior.\n\nGiven this analysis, we can conclude that there is a bug in the code snippet due to the out-of-bounds access in the loop condition.\n\nNow, let's consider which sanitizer can detect this bug:\n- AddressSanitizer (ASan) primarily detects memory errors like buffer overflows and use-after-free, but it may not specifically catch the out-of-bounds access in this case.\n- MemorySanitizer (MemSan) focuses on detecting uninitialized memory reads, so it is not directly relevant here.\n- UndefinedBehaviorSanitizer (UBSan) is designed to catch various types of undefined behavior, including out-of-bounds array accesses. This sanitizer is specifically suited for identifying issues like the one present in the code snippet.\n- ThreadSanitizer (TSan) is used for detecting data races in multithreaded programs and is not applicable in this single-threaded scenario.\n\nTherefore, the correct answer is option 3: UndefinedBehaviorSanitizer (UBSan) can detect the bug of out-of-bounds access in the given code snippet. ///Answer: C"}
{"subject": "", "question": "Question: Select valid answers about file descriptors (FD):?\n\nOptions:\nA. The value of FD is unique for every file in the operating system.\nB. FD is usually used as an argument for read and write.\nC. FD is constructed by hashing the filename.\nD. FDs are preserved after fork() and can be used in the new process pointing to the original files.\n", "answer": "Explanations: Correct options:\nA. The value of FD is unique for every file in the operating system.\nB. FD is usually used as an argument for read and write.\nD. FDs are preserved after fork() and can be used in the new process pointing to the original files.\"\n\nJustification:\nA. The value of a file descriptor (FD) is unique for every file in the operating system. This uniqueness allows the operating system to keep track of different files using their respective file descriptors.\n\nB. File descriptors are commonly used as arguments for read() and write() system calls in UNIX-like operating systems to perform input/output operations on files.\n\nD. File descriptors are preserved after the fork() system call in UNIX-like operating systems. After fork(), the child process inherits the file descriptors of the parent process, which means that the child process can use the same file descriptors to access the same files that the parent process had opened.\n\nOption C is incorrect because file descriptors are not constructed by hashing the filename. File descriptors are integer values that serve as indexes into the file descriptor table maintained by the operating system to manage open files. ///Answer: A"}
{"subject": "", "question": "Question: For which kind of bugs does default LLVM provide sanitizers?\n\nOptions:\nA. Memory leaks\nB. Buffer overflows\nC. Race conditions between threads\nD. Logic bugs\n", "answer": "Explanations: To determine which kinds of bugs default LLVM provides sanitizers for, we need to understand the purpose of sanitizers provided by LLVM. LLVM provides sanitizers to help detect and diagnose various types of bugs in software during development and testing.\n\nLet's break down the options given:\n\n1. Memory leaks: LLVM does provide a sanitizer for detecting memory leaks. This sanitizer is called AddressSanitizer (ASan).\n\n2. Buffer overflows: LLVM also provides a sanitizer for detecting buffer overflows. This sanitizer is called AddressSanitizer (ASan).\n\n3. Race conditions between threads: LLVM provides a sanitizer for detecting race conditions between threads. This sanitizer is called ThreadSanitizer (TSan).\n\n4. Logic bugs: LLVM does not provide specific sanitizers for detecting logic bugs. Logic bugs are usually more complex and may require different approaches for detection.\n\nTherefore, the correct options from the given choices are:\n- Memory leaks\n- Buffer overflows\n- Race conditions between threads\n\nSo, the answer is: Memory leaks, Buffer overflows, Race conditions between threads. ///Answer: A"}
{"subject": "", "question": "Question: Which is NOT a mobile telephony protocol?\n\nOptions:\nA. WPA\nB. DECT\nC. EDGE\nD. CDMA\n", "answer": "Explanations: The option that is NOT a mobile telephony protocol is WPA (Wi-Fi Protected Access). WPA is a security protocol used to secure wireless networks, specifically Wi-Fi networks. It is not directly related to mobile telephony, which primarily involves cellular communication protocols for voice and data transmission over mobile networks. The other options, DECT, EDGE, and CDMA, are all mobile telephony protocols used in various mobile communication technologies. ///Answer: A"}
{"subject": "", "question": "Question: A collision-resistant hash function $h$ is a hash function where it is \\emph{infeasible} to find\\dots?\n\nOptions:\nA. a digest $d$ such that, given a message $m$, $h(m)=d$.\nB. a message $m$ such that, given a digest $d$, $h(m)=d$.\nC. a message $m$ such that, given $m_0$, $h(m)=h(m_0)$.\nD. two different messages $m_1$, $m_2$ such that $h(m_1)=h(m_2)$.\n", "answer": "Explanations: The correct option is: 4. two different messages $m_1$, $m_2$ such that $h(m_1)=h(m_2)$.\n\nA collision-resistant hash function should make it computationally infeasible to find two different inputs that produce the same hash value. This property is important for ensuring the integrity and security of data, as it helps prevent malicious actors from forging messages or manipulating data in a way that results in the same hash value. ///Answer: D"}
{"subject": "", "question": "Question: For $p$ prime and an arbitrary odd $n$, the symbol $\\left( {p \\over n} \\right)$ denotes?\n\nOptions:\nA. a binomial coefficient.\nB. the Legendre symbol.\nC. the Jacobi symbol.\nD. a fraction.\n", "answer": "Explanations: The correct answer is the Legendre symbol.\n\nThe Legendre symbol is defined for a prime number $p$ and an integer $n$ such that $p$ does not divide $n$. It is denoted by $\\left( \\dfrac{p}{n} \\right)$ and is defined as follows:\n\n$\\left( \\dfrac{p}{n} \\right) = \\begin{cases} \n1 & \\text{if } n \\text{ is a quadratic residue modulo } p \\text{ (i.e., there exists an integer } x \\text{ such that } x^2 \\equiv n \\pmod{p}) \\\\\n-1 & \\text{if } n \\text{ is a quadratic non-residue modulo } p \\text{ (i.e., there is no integer } x \\text{ such that } x^2 \\equiv n \\pmod{p}) \\\\\n0 & \\text{if } n \\equiv 0 \\pmod{p}\n\\end{cases}$\n\nThe Legendre symbol plays an important role in number theory and is used in various applications, including determining whether a given integer is a quadratic residue modulo a prime number. ///Answer: B"}
{"subject": "", "question": "Question: Let S(x) be the statement \u201cx has been in a lake\u201d and L(x) be the statement \u201cx lives in Lausanne\u201d and the domain of x consists of all the humans in the world.\n\nThe sentence : \u201cthere exists exactly one human that lives in Lausanne and that has never been in a lake\u201d corresponds to the statement (multiple choices possible):?\n\nOptions:\nA. \\( \\exists! x (S(x) \\wedge L(x)) \\)\nB. \\( \\exists x \\Bigr[( S(x) \\wedge \neg L(x)) \\wedge \forall y \\left[  \neg( S(y) \\wedge \neg L(y)) \\wedge (x=y) \right] \\Bigr] \\)\nC. \\( \\exists x \\Bigr[ (\neg S(x) \\wedge L(x)) \\wedge \forall y \\left[ \neg(\neg S(y) \\wedge L(y)) \u000bee (x=y) \right] \\Bigr] \\)\nD. \\( \\exists! x (\neg S(x) \\wedge L(x)) \\)\n", "answer": "Explanations: To express the sentence \"there exists exactly one human that lives in Lausanne and that has never been in a lake,\" we can break it down as follows:\n\n1. There exists at least one human who lives in Lausanne and has never been in a lake.\n2. There does not exist more than one human who lives in Lausanne and has never been in a lake.\n\nThis can be represented as:\n\n\u2203x (L(x) \u2227 \u00acS(x)) \u2227 \u00ac\u2203y (L(y) \u2227 \u00acS(y) \u2227 y \u2260 x)\n\nExplanation:\n- \u2203x (L(x) \u2227 \u00acS(x)): There exists a human x who lives in Lausanne and has never been in a lake.\n- \u00ac\u2203y (L(y) \u2227 \u00acS(y) \u2227 y \u2260 x): There does not exist another human y who lives in Lausanne and has never been in a lake, different from x.\n\nTherefore, the correct choice is 1 if considering insertion, deletion, and substitution. ///Answer: A"}
{"subject": "", "question": "Question: A constant initialization vector (IV), i.e., same for every plaintext, can be used in the ...\n\nOptions:\nA. CTR mode.\nB. CBC mode.\nC. OFB mode.\nD. ECB mode.\n", "answer": "Explanations: In Cipher Block Chaining (CBC) mode, the use of an Initialization Vector (IV) is crucial to prevent identical plaintext blocks from producing the same ciphertext blocks, which could lead to vulnerability. The IV must be random and unique for each encryption to ensure the security of the encryption process. If a constant IV is used, attackers may exploit patterns in the ciphertext, leading to potential decryption of the data. By utilizing a random and unique IV in CBC mode, the encryption scheme becomes more robust and resistant to cryptanalysis, safeguarding the confidentiality of the transmitted information. ///Answer: B"}
{"subject": "", "question": "Question: Which one of these is not believed to be a hard problem?\n\nOptions:\nA. $\\mathbf{NP}$-hard problems.\nB. Factoring problem.\nC. Discrete logarithm problem.\nD. Computing the CRT.\n", "answer": "Explanations: To determine which of these problems is not believed to be hard, we need to understand the classification of computational problems in terms of complexity. \n\n- $\\mathbf{NP}$-hard problems are a class of problems that are at least as hard as the hardest problems in $\\mathbf{NP}$ (nondeterministic polynomial time) class. These problems are believed to be very difficult to solve efficiently.\n- The factoring problem and the discrete logarithm problem are both examples of problems that are widely believed to be hard. In particular, factoring large numbers is the basis for some cryptographic algorithms like RSA, and the discrete logarithm problem is used in other cryptographic schemes.\n- Computing the Chinese Remainder Theorem (CRT) is generally considered to be an easier problem compared to the others mentioned. The CRT is a mathematical theorem that provides a way to solve a system of congruences efficiently.\n\nTherefore, the correct answer is:\n\nComputing the CRT. ///Answer: D"}
{"subject": "", "question": "Question: Let $E$ and $F$ be two events. Suppose that they satisfy $p(E|F)=p(E) > 0.$ True or false: Then we must have $p(F|E)=p(F).$?\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: Answer A: False\n\nExplanation: The statement $p(E|F)=p(E)$ means that the probability of event $E$ occurring given that event $F$ has already occurred is equal to the probability of event $E$ occurring. This does not necessarily imply that the probability of event $F$ occurring given event $E$ has occurred is equal to the probability of event $F$ occurring. The probabilities of events $E$ and $F$ are independent of each other, and so $p(F|E)$ may not necessarily be equal to $p(F) ///Answer: A:"}
{"subject": "", "question": "Question: How many $x \\in \\mathbb{Z} / 23\\mathbb{Z}$ satisfy the  equation $0 = 1 - x + x^2 - x^3 + .... - x^{21} + x^{22} - x^{23}$, when all operations are with respect to the field $(\\mathbb{Z} / 23 \\mathbb{Z}, + , \\cdot)$? Check the correct answer.\n\nOptions:\nA. 1\nB. 0\nC. 22\nD. 23\nE. 2\n", "answer": "Explanations: The correct answer is 2.\n1. By Fermat's Little Theorem, we have $x^{23} \\equiv x \\pmod{23}$ for all $x \\in \\mathbb{Z}/23\\mathbb{Z}$.\n2. Therefore, $0 = 1 - x + x^2 - x^3 + \\ldots - x^{21} + x^{22} - x^{23}$ simplifies to $0 = 1 - x + x^2 - x^3 + \\ldots - x^{21} + x^{22} - x$.\n3. This equation simplifies further to $1 \\equiv 0 \\pmod{23}$, which is not possible.\n4. Hence, the equation has no solutions in $\\mathbb{Z}/23\\mathbb{Z}$.\n5. The correct answer is 0. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textit{incorrect} assertion.\n\nOptions:\nA. Using elliptic curves, it is possible to obtain a secure 3-party Diffie-Hellman key-exchange.\nB. Cryptographic primitives based on elliptic curves are popular because in general, they require smaller primes than other public-key cryptosystems to be secure.\nC. A non-singular elliptic curve $E_{a,b}(\\mathrm{GF}(p^k))$, with $p>3$ a prime and $k$ a positive integer forms a field.\nD. Standardized random curves over prime fields are believed to be secure.\n", "answer": "Explanations: Assertion 1: Using elliptic curves, it is possible to obtain a secure 3-party Diffie-Hellman key-exchange. - This is a correct assertion. Elliptic curve cryptography can be used for secure key exchange protocols like Diffie-Hellman.\n\nAssertion 2: Cryptographic primitives based on elliptic curves are popular because in general, they require smaller primes than other public-key cryptosystems to be secure. - This is a correct assertion. Elliptic curve cryptography often requires smaller key sizes compared to other public-key cryptosystems for equivalent security levels.\n\nAssertion 3: A non-singular elliptic curve $E_{a,b}(\\text{GF}(p^k))$, with $p>3$ a prime and $k$ a positive integer forms a field. - This assertion is incorrect. An elliptic curve over a finite field does not form a field itself; rather, it is a group structure.\n\nAssertion 4: Standardized random curves over prime fields are believed to be secure. - This is a correct assertion. Standardized elliptic curves over prime fields are believed to be secure and widely used in practice.\n\nTherefore, the incorrect assertion is: \nAssertion 3: A non-singular elliptic curve $E_{a,b}(\\text{GF}(p^k))$, with $p>3$ a prime and $k$ a positive integer forms a field. ///Answer: C"}
{"subject": "", "question": "Question: In x86, what are the possible ways to transfer arguments when invoking a system call? For example, in the following code, string and len are sys_cputs\u2019s arguments.\n\nOptions:\nA. Stack\nB. Registers\nC. Instructions\n", "answer": "Explanations: In x86, there are primarily two ways to transfer arguments when invoking a system call:\n\n1. Stack: The arguments can be pushed onto the stack before invoking the system call. The system call can then access the arguments from the stack.\n\n2. Registers: The arguments can be passed via registers. Different registers can be used to pass different arguments to the system call.\n\nAdditionally, some system calls may require specific instructions to be executed before or after passing the arguments. These instructions can manipulate the registers or stack to ensure that the arguments are passed correctly to the system call. ///Answer: A"}
{"subject": "", "question": "Question: Consider the following lexicon \\(L\\):\nboy    : Adj, N\nboys   : N\nblue   : Adj, N\ndrink  : N, V\ndrinks : N, V\nNice   : Adj, N\n\nWhen using an order-1 HMM model (using \\(L\\)) to tag the word sequence:\"Nice boys drink blue drinks\"does the tag of drink\u00a0depend on the tag of nice?\n\n?\n\nOptions:\nA. yes, because the HMM approach relies on a global maximum.\nB. no, the hypotheses make the two tags independent from each other.\n", "answer": "Explanations: ##Answer:\nNo, the hypotheses make the two tags independent from each other.\n\n##Reasons:\n1. In an order-1 Hidden Markov Model (HMM) for part-of-speech tagging, each word is associated with a set of possible tags based on the lexicon.\n2. The HMM model considers the transition probabilities between tags and emission probabilities of words given the tags.\n3. The tag of a word in an HMM model depends on the probabilities of transitioning from one tag to another and the emission probabilities of the word given each tag, rather than being directly dependent on the tag of a previous word.\n4. In this case, the tag of \"drink\" in the word sequence \"Nice boys drink blue drinks\" would be determined based on the probabilities associated with \"drink\" itself and the transition probabilities to its possible tags, rather than being dependent on the tag of \"Nice\" before it.\n5. Therefore, in the HMM model, the hypotheses make the two tags independent from each other, and the tag of \"drink\" does not directly depend on the tag of \"Nice\" in this context. ///Answer: B"}
{"subject": "", "question": "Question: The Miller-Rabin test \\emph{cannot} output\\dots?\n\nOptions:\nA. ``prime'' for a ``composite'' number.\nB. ``prime'' for a ``prime'' number.\nC. ``composite'' for a ``composite'' number.\nD. ``composite'' for a ``prime'' number.\n", "answer": "Explanations: 1. The Miller-Rabin test can output \"prime\" for a composite number, as it is a probabilistic test and there is a small chance that it may mistakenly classify a composite number as prime. Therefore, this statement is incorrect.\n\n2. The Miller-Rabin test can output \"prime\" for a prime number, as it is designed to correctly identify prime numbers with high probability. Therefore, this statement is incorrect.\n\n3. The Miller-Rabin test can output \"composite\" for a composite number, as it is designed to correctly identify composite numbers with high probability. Therefore, this statement is incorrect.\n\n4. The Miller-Rabin test cannot output \"composite\" for a prime number, as it is designed to correctly identify prime numbers with high probability. Therefore, this statement is correct. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textit{incorrect} assertion.\n\nOptions:\nA. We can establish secure communication with the help of a secure third party.\nB. Unless the manufacturer of our hardware is honest, our communication is not secure.\nC. Clients either have to update their certificate revocation lists regularly or use OCSP to securely validate certificates.\nD. In a network of $n$ users, we need to have $\\approx n^2$ pre-shared symmetric keys to use Kerberos.\n", "answer": "Explanations: The incorrect assertion is: 'In a network of $n$ users, we need to have $\\approx n^2$ pre-shared symmetric keys to use Kerberos.'\n\nExplanation: The assertion that we need $\\approx n^2$ pre-shared symmetric keys in a network of $n$ users to use Kerberos is incorrect. In Kerberos, a trusted third-party key distribution center (KDC) is used to securely distribute session keys between users. The KDC shares a secret key with each user, not with every other user in the network. Therefore, the number of keys needed is proportional to the number of users in the network, not the square of the number of users. This assertion is based on a misunderstanding of how Kerberos works. ///Answer: D"}
{"subject": "", "question": "Question: (Linear Regression) You are given samples $\\mathcal{S}=\\left\\{\\left(\\mathbf{x}_{n}, y_{n}\\right)\\right\\}_{n=1}^{N}$ where $\\mathbf{x}_{n} \\in \\mathbb{R}^{D}$ and $y_{n}$ are scalar values. You are solving linear regression using normal equations. You will always find the optimal weights with 0 training error in case of $N \\leq D$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: Let's consider an example where the number of samples $N$ is less than or equal to the dimensionality $D$ of the feature space, but the optimal weights are found using the normal equations without achieving a training error of 0.\n\nConsider a scenario with $N = 2$ samples and $D = 2$ features. Let the samples be represented as follows:\n\n- $\\mathbf{x}_1 = [1, 0]$\n- $\\mathbf{x}_2 = [0, 1]$\n\nLet the corresponding target values be:\n- $y_1 = 1$\n- $y_2 = 1$\n\nIn this case, the design matrix $\\mathbf{X}$ will be:\n$$\n\\mathbf{X} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n$$\n\nAnd the target vector $\\mathbf{y}$ will be:\n$$\n\\mathbf{y} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n$$\n\nBy using the normal equations to find the optimal weights $\\mathbf{w}$:\n$$\n\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n\nCalculating:\n$$\n\\mathbf{w} = \\left( \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}^T \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\right)^{-1} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}^T \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n$$\n\nThe optimal weights are $\\mathbf{w} = [1, 1]$. However, the training error is not zero since the target values are both 1, and the model predicts $\\hat{y}_1 = 1$ and $\\hat{y}_2 = 1$, resulting in a mean squared error (MSE) of $0.0$ and not $0.0$.\n\nThis example demonstrates that even when the number of samples is less than or equal to the dimensionality of the feature space, it does not guarantee a training error of 0 when solving linear regression using the normal equations. ///Answer: B"}
{"subject": "", "question": "Question: With Fagin\u2019s algorithm, once k documents have been identified that occur in all of the lists?\n\nOptions:\nA. These are the top-k documents\nB. The top-k documents are among the documents seen so far\nC. The search has to continue in round-robin till the top-k documents are identified\nD. Other documents have to be searched to complete the top-k list\n", "answer": "Explanations: \n\nFagin's algorithm is a method used in information retrieval to identify the top-k documents that occur in multiple lists of documents. The algorithm works by scanning through the lists of documents and identifying the documents that appear in all of them.\n\nNow, let's address each of the options provided:\n\n1. \"These are the top-k documents\": This option is incorrect because simply identifying k documents that occur in all lists does not necessarily mean they are the top-k documents overall. It just means they are common across all lists.\n\n2. \"The top-k documents are among the documents seen so far\": This option is more accurate. The algorithm continues scanning through the lists until it has seen enough unique documents to fill the top-k list. So, the top-k documents will indeed be among the documents seen so far.\n\n3. \"The search has to continue in round-robin till the top-k documents are identified\": This option is not entirely accurate. Fagin's algorithm does not necessarily require a round-robin search. It simply looks for common documents across lists until it has enough to fill the top-k list.\n\n4. \"Other documents have to be searched to complete the top-k list\": This option is incorrect because once k documents have been identified that occur in all lists, there is no need to search for additional documents to complete the top-k list. The algorithm stops once it has found enough common documents.\n\nIn conclusion, the correct option is: \"The top-k documents are among the documents seen so far.\" This best reflects how Fagin's algorithm works in identifying the top-k documents from multiple lists. ///Answer: B"}
{"subject": "", "question": "Question: An HMM model would not be an appropriate approach to identify?\n\nOptions:\nA. Named Entities\nB. Part-of-Speech tags\nC. Concepts\nD. Word n-grams\n", "answer": "Explanations: Named Entity Recognition involves identifying specific entities in text, such as names of people, organizations, locations, etc. Hidden Markov Models (HMMs) may not be the most suitable approach for this task because they are limited by their first-order Markov property, which means they model dependencies based only on the previous state. Named Entity Recognition often requires capturing long-range dependencies and context in text, which can be better handled by models like Conditional Random Fields (CRFs) or deep learning models such as Bidirectional LSTMs or Transformers. These models can effectively capture the complex patterns and relationships necessary for accurate Named Entity Recognition. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following operations would switch the user program from user space to kernel space?\n\nOptions:\nA. Dividing integer by 0.\nB. Calling sin() in math library.\nC. Invoking read() syscall.\nD. Jumping to an invalid address.\n", "answer": "Explanations: The correct answer is:\n\n3. Invoking read() syscall.\n\nExplanation: Invoking a system call like read() directly interacts with the operating system kernel, causing a switch from user space to kernel space where the kernel can perform the necessary operations on behalf of the user program. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. The SEI of the distribution $P$ of support $G$ \\ldots?\n\nOptions:\nA. is equal to \\# $G\\cdot\\displaystyle\\sum_{x\\in G}\\left(P(x)-\\frac{1}{\\sharp G}\\right)^2$\nB. is the advantage of the best distinguisher between $P$ and the uniform distribution.\nC. denotes the Squared Euclidean Imbalance.\nD. is positive.\n", "answer": "Explanations: The false assertion is:\n\n- is equal to # $G\\cdot\\displaystyle\\sum_{x\\in G}\\left(P(x)-\\frac{1}{\\sharp G}\\right)^2\n\nExplanation:\n\nThe Structural Imbalance Entropy (SEI) of the distribution P of support G is a measure that quantifies how imbalanced the distribution is. The correct assertions are:\n\n- is the advantage of the best distinguisher between P and the uniform distribution.\n- denotes the Squared Euclidean Imbalance.\n- is positive.\n\nThe false assertion provided in the question seems to be a misinterpretation or a mistake in the formula provided. The SEI is not calculated as the product of the support size and the sum of squared differences between the probabilities and the uniform distribution. The correct formula for SEI involves comparing the distribution P with the uniform distribution and measuring the advantage of the best distinguisher in differentiating between them. ///Answer: A"}
{"subject": "", "question": "Question: The Shannon theorem states that perfect secrecy implies...\n\nOptions:\nA. $H(K)=H(X)$\nB. $H(Y)\\geq H(X)$\nC. $H(K)\\geq H(X)$\nD. $H(Y)\\leq H(X)$\n", "answer": "Explanations: The Shannon theorem states that perfect secrecy implies:\n\n- $H(K)=H(X)$\n\nExplanation: In the context of perfect secrecy, the Shannon theorem indicates that the entropy of the key $H(K)$ must be equal to the entropy of the plaintext $H(X)$ to achieve perfect secrecy. This balance ensures that the ciphertext does not leak any information about the plaintext beyond what is already known. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. In RSA \\ldots?\n\nOptions:\nA. \\ldots decryption is known to be equivalent to factoring.\nB. \\ldots key recovery is provably not equivalent to factoring).\nC. \\ldots decryption is probabilistic.\nD. \\ldots public key transmission needs authenticated and integer channel.\n", "answer": "Explanations: The true assertion in this question is: decryption is known to be equivalent to factoring.\n\nExplanation:\n1. Decryption is known to be equivalent to factoring: In RSA (Rivest-Shamir-Adleman) encryption, the security of the system is based on the difficulty of factoring large composite numbers. The process of decryption in RSA involves using the private key to factorize a large number, which is computationally difficult to reverse without the private key. Therefore, decryption in RSA is known to be equivalent to factoring.\n\n2. Key recovery is provably not equivalent to factoring: Key recovery in RSA involves trying to recover the private key from the public key. While factoring plays a crucial role in RSA security, it is not the same as recovering the private key from the public key. Key recovery is a separate process and is not provably equivalent to factoring.\n\n3. Decryption is probabilistic: Decryption in RSA is not probabilistic. Once the private key is used to decrypt the ciphertext, the process is deterministic and will always result in the original plaintext message. The encryption process in RSA may involve some randomization, but decryption itself is not probabilistic.\n\n4. Public key transmission needs authenticated and integer channel: In RSA, the public key is used for encryption, and the private key is used for decryption. It is important to ensure the authenticity of the public key to prevent attacks such as man-in-the-middle. While an authenticated channel is necessary for securely transmitting the public key, it does not specifically need to be an integer channel. The public key can be transmitted over any secure channel, such as a secure internet connection or through physical means like a USB drive. \n\nIn conclusion, the true assertion in this question is that decryption in RSA is known to be equivalent to factoring. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following scheduler policies are preemptive?\n\nOptions:\nA. FIFO (First In, First Out)\nB. SJF (Shortest Job First)\nC. STCF (Shortest Time to Completion First)\nD. RR (Round Robin)\n", "answer": "Explanations: SJF (Shortest Job First) and RR (Round Robin) are preemptive scheduler policies.\n\nSJF is preemptive because if a new job arrives that has a shorter processing time than the currently running job, the scheduler will preempt the current job and switch to the new job.\n\nRR (Round Robin) is also preemptive because each process is given a small unit of time to execute before being preempted and moved to the back of the queue. This allows for fair allocation of CPU time among all processes. ///Answer: D"}
{"subject": "", "question": "Question: Which one is a stream cipher?\n\nOptions:\nA. RSA\nB. MD5\nC. AES\nD. RC4\n", "answer": "Explanations: Question: Which one is a stream cipher?\n\nOption 0: RSA  \nOption 1: MD5  \nOption 2: AES  \nOption 3: RC4  \n\nAnswer A:\n\nOption 0: (False) RSA is not a stream cipher. RSA (Rivest-Shamir-Adleman) is a public-key cryptosystem used for secure data transmission and digital signatures. It is based on the difficulty of factoring large integers. RSA is not a stream cipher; it is an asymmetric algorithm.\n\nOption 1: (False) MD5 is not a stream cipher. MD5 (Message Digest Algorithm 5) is a cryptographic hash function that produces a 128-bit hash value. It is commonly used for data integrity checks and digital signatures, but it is not a stream cipher.\n\nOption 2: (False) AES (Advanced Encryption Standard) is not a stream cipher. AES is a symmetric encryption algorithm that operates on fixed-size blocks of data. It is widely used for secure communication and data protection, but it is not a stream cipher.\n\nOption 3: (True) RC4 is a stream cipher. RC4 is a symmetric encryption algorithm that generates a stream of pseudorandom bytes, which are combined with the plaintext to produce the ciphertext. It was widely used in various protocols like SSL and WEP, although its security has been weakened due to vulnerabilities.\n\nTherefore, the correct answer is Option 3: RC4. ///Answer: D"}
{"subject": "", "question": "Question: Tick the most accurate answer. Generating an $\\ell$-bit prime number can be done in?\n\nOptions:\nA. $O(\\ell^2)$ time.\nB. $O(\\ell^4)$ time.\nC. $O(\\ell^3)$ time.\nD. $O(\\ell)$ time.\n", "answer": "Explanations: Answer: D\nExplanation: Generating an $\\ell$-bit prime number can be done in $O(\\ell)$ time using algorithms like the Miller-Rabin primality test or the AKS primality test. These algorithms have a time complexity of $O(\\ell)$ for generating a prime number of $\\ell$ bits. Options A, B, and C have higher time complexities and are not accurate for generating $\\ell$-bit prime numbers. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following statements are true?\n\nOptions:\nA. The more training examples, the more accurate the prediction of a $k$-nearest-neighbor classifier.\nB. k-nearest-neighbors cannot be used for regression.\nC. A $k$-nearest-neighbor classifier is sensitive to outliers.\nD. Training a $k$-nearest-neighbor classifier takes more computational time than applying it / using it for prediction.\n", "answer": "Explanations: 1. 'The more training examples, the more accurate the prediction of a $k$-nearest-neighbor classifier.' - This statement is generally true. In $k$-nearest-neighbor classification, having more training examples can help improve the accuracy of predictions as the algorithm has more data to learn from.\n\n2. 'k-nearest-neighbors cannot be used for regression.' - This statement is false. $k$-nearest-neighbors can be used for regression tasks as well, where the algorithm predicts a continuous value rather than a class label.\n\n3. 'A $k$-nearest-neighbor classifier is sensitive to outliers.' - This statement is true. $k$-nearest-neighbors can be sensitive to outliers in the training data, as outliers can significantly affect the distance calculations and thus the classification decisions.\n\n4. 'Training a $k$-nearest-neighbor classifier takes more computational time than applying it / using it for prediction.' - This statement is false. In $k$-nearest-neighbors, the majority of computational time is spent on the prediction phase rather than the training phase. Training typically involves storing the training data, while prediction involves calculating distances to all training examples for each new data point. ///Answer: C"}
{"subject": "", "question": "Question: Information extraction:?\n\nOptions:\nA. Necessarily requires training data.\nB. Can be used to populate ontologies.\nC. Is used to identify characteristic entities in a document.\nD. Is always bootstrapped by using ontologies.\n", "answer": "Explanations: Let's analyze each possible answer step by step:\n\n1. \"Necessarily requires training data\": This statement is generally true. Information extraction often involves the use of machine learning algorithms or models that require training data to learn patterns and extract relevant information accurately.\n\n2. \"Can be used to populate ontologies\": This statement is also accurate. Information extracted from text documents can be structured and organized to populate ontologies with relevant data, enhancing the knowledge base within the ontology.\n\n3. \"Is used to identify characteristic entities in a document\": This statement is true. Information extraction techniques are commonly used to identify specific entities like names, dates, locations, and other key information within documents.\n\n4. \"Is always bootstrapped by using ontologies\": This statement is not universally true. While ontologies can assist in information extraction by providing context and domain knowledge, information extraction processes are not always dependent on ontologies for bootstrapping.\n\nBased on the analysis, the correct options are:\n- Necessarily requires training data.\n- Can be used to populate ontologies.\n- Is used to identify characteristic entities in a document. ///Answer: B"}
{"subject": "", "question": "Question: The Tonelli algorithm is?\n\nOptions:\nA. a factoring algorithm.\nB. a primality testing algorithm.\nC. an algorithm for finding square roots.\nD. an algorithm for testing quadratic residuosity.\n", "answer": "Explanations: D. The Tonelli algorithm is an algorithm for testing quadratic residuosity. This algorithm is specifically designed to determine whether a given number is a quadratic residue modulo another number. It does this by finding the square root of the given number modulo the other number. This algorithm is useful in cryptography and number theory for determining properties of numbers in relation to each other. Therefore, option D is the correct answer. ///Answer: D"}
{"subject": "", "question": "Question: K-means can be equivalently written as the following Matrix Factorization $$ \\begin{aligned} & \\min _{\\mathbf{z}, \\boldsymbol{\\mu}} \\mathcal{L}(\\mathbf{z}, \\boldsymbol{\\mu})=\\left\\|\\mathbf{X}-\\mathbf{M} \\mathbf{Z}^{\\top}\\right\\|_{\\text {Frob }}^{2} \\\\ & \\text { s.t. } \\boldsymbol{\\mu}_{k} \\in \\mathbb{R}^{D}, \\\\ & z_{n k} \\in \\mathbb{R}, \\sum_{k=1}^{K} z_{n k}=1 . \\end{aligned} $$?\n\nOptions:\nA. (a) yes\nB. (b) no\n", "answer": "Explanations: The statement is incorrect. The correct answer is (b) no.\n\nK-means is a clustering algorithm that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean. It does not involve matrix factorization as described in the equation provided. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. A cipher $C$ perfectly decorrelated at order 2 implies\\dots?\n\nOptions:\nA. perfect secrecy when used twice.\nB. security against differential cryptanalysis.\nC. security against linear cryptanalysis.\nD. security against exhaustive search.\n", "answer": "Explanations: A: - security against exhaustive search.\n\nExplanation: A cipher $C$ perfectly decorrelated at order 2 means that the output of the cipher is perfectly random-like when used twice. This does not necessarily guarantee security against exhaustive search. Exhaustive search refers to an attack where all possible keys are tried until the correct one is found. Perfect decorrelation at order 2 does not provide protection against this type of attack. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following models for generating vector representations for text require to precompute the frequency of co-occurrence of words from the vocabulary in the document collection?\n\nOptions:\nA. LSI\nB. CBOW\nC. Fasttext\nD. Glove\n", "answer": "Explanations: LSI, or Latent Semantic Indexing, requires precomputing the frequency of co-occurrence of words in the document collection. LSI creates a vector representation of text by using Singular Value Decomposition (SVD) to identify relationships between words in the document collection based on their co-occurrence frequencies. This method helps to capture the semantic meaning of words in a document collection and is often used for document retrieval and information retrieval tasks. On the other hand, CBOW, Fasttext, and Glove do not require precomputing the frequency of co-occurrence of words in the document collection as they use different techniques such as neural networks and word embeddings to generate vector representations for text. ///Answer: A"}
{"subject": "", "question": "Question: We want to return, from the two posting lists below, the top-2 documents matching a query using Fagin\u2019s algorithm with the aggregation function taken as the sum of the tf-idf weights. How many entries (total of both lists) are accessed in the first phase of the algorithm performing round robin starting at List 1 (i.e., before performing the random access)?List 1List 2documenttf-idfdocumenttf-idfd30.8d10.8d20.6d30.6d10.5d40.5d40.4d20.4?\n\nOptions:\nA. 2\nB. 4\nC. 6\nD. 8\n", "answer": "Explanations: In Fagin's algorithm, the first phase involves accessing entries from both posting lists in a round-robin manner until the top-k results are found. \n\nGiven List 1 and List 2, starting at List 1, we will access entries until the top-2 documents are found based on the sum of the tf-idf weights.\n\nList 1 has 4 entries: d3, d1, d2, and d4.\nList 2 has 4 entries: d1, d3, d4, and d2. \n\nIn the first phase of the algorithm, we need to access all entries in List 1 and List 2 until we find the top-2 documents. This means we need to access a total of 8 entries (4 entries from List 1 and 4 entries from List 2).\n\nTherefore, the correct option is:\n\n8\n\nExplanation: In the first phase of Fagin's algorithm, we need to access entries from both posting lists until the top-k results are found. In this case, we need to access all entries from List 1 and List 2 before performing random access to determine the top-2 documents. ///Answer: D"}
{"subject": "", "question": "Question: In RSA, we use CRT ...\n\nOptions:\nA. to speedup encryption.\nB. to speedup decryption.\nC. since it is necessary operation of the primitive.\nD. to prove security.\n", "answer": "Explanations: In RSA, we use CRT to speed up decryption.\n\nExplanation:\nCRT (Chinese Remainder Theorem) is a technique used in RSA decryption to improve the efficiency of the decryption process. By using CRT, the decryption process can be divided into smaller, independent computations which can be done in parallel. This helps to speed up the decryption process, making it more efficient and faster than traditional RSA decryption methods. ///Answer: B"}
{"subject": "", "question": "Question: Does the following code compile? given Ordering[Int] with \\t def compare(x: Int, y: Int): Int = 0 List(1, 2, 3).sorted?\n\nOptions:\nA. Yes\nB. No\n", "answer": "Explanations: Answer B:\n\nNo, the code will not compile. In order to sort a list of integers using the `sorted` method, an implicit `Ordering[Int]` instance needs to be provided. The code snippet `Ordering[Int] with \\t def compare(x: Int, y: Int): Int = 0` attempts to define a custom ordering for integers, but it is missing the correct implementation and implicit declaration required for the `sorted` method to work. ///Answer: B"}
{"subject": "", "question": "Question: What is the content of the superblock in the JOS file system?\n\nOptions:\nA. List of all directories\nB. List of all files\nC. List of all blocks\nD. List of all inodes\nE. Total number of blocks on disk\nF. Magic number identifying the file system\nG. Node with the root directory ('/')\n", "answer": "Explanations: The content of the superblock in the JOS file system includes:\n- List of all blocks\n- List of all inodes\n- Total number of blocks on disk\n- Magic number identifying the file system\n- Node with the root directory ('/') ///Answer: C"}
{"subject": "", "question": "Question: Which mode of operation is similar to a stream cipher?\n\nOptions:\nA. ECB\nB. OFB\nC. CFB\nD. CBC\n", "answer": "Explanations: The mode of operation that is similar to a stream cipher is Option 2: OFB (Output Feedback). \n\nOFB generates a pseudorandom stream of bits that are XORed with the plaintext to produce the ciphertext, similar to how a stream cipher works. In contrast, ECB (Electronic Codebook) encrypts each block of plaintext independently, making it less similar to a stream cipher. CFB (Cipher Feedback) and CBC (Cipher Block Chaining) also operate on blocks of plaintext rather than a stream of bits. ///Answer: B"}
{"subject": "", "question": "Question: 10 itemsets out of 100 contain item A, of which 5 also contain B. The rule A -> B has:?\n\nOptions:\nA. 5% support and 10% confidence\nB. 10% support and 50% confidence\nC. 5% support and 50% confidence\nD. 10% support and 10% confidence\n", "answer": "Explanations: Support is the proportion of itemsets that contain both A and B out of the total number of itemsets: \\( \\text{Support} = \\frac{5}{100} = 5\\% \\).\n\nConfidence is the proportion of itemsets containing both A and B out of the itemsets containing A: \\( \\text{Confidence} = \\frac{5}{10} = 50\\% \\).\n\nTherefore, the rule A -> B has 5% support and 50% confidence, which matches with Option 3: 5% support and 50% confidence. ///Answer: C"}
{"subject": "", "question": "Question: Mark the \\emph{incorrect} statement. A finite field is?\n\nOptions:\nA. always isomorphic to $\\mathsf{GF}(p^k)$, for some prime $p$ and integer $k$.\nB. always isomorphic to $\\mathbb{Z}_p[x]/Q(x)$, for some prime $p$ and an irreducible polynomial $Q$.\nC. always isomorphic to $\\mathbb{Z}_{p^k}$, for a prime $p$ and integer $k$.\nD. unique up to an isomorphism.\n", "answer": "Explanations: The incorrect statement is option 2: \"always is isomorphic to $\\\\mathbb{Z}_{p^k}$, for a prime $p$ and integer $k.\"\\n\\nExplanation:\\n0. A finite field is always isomorphic to $\\\\mathsf{GF}(p^k)$, for some prime $p$ and integer $k$: \\\\textbf{True.} By definition, a finite field is of order $p^k$ where $p$ is a prime and $k$ is a positive integer. This isomorphism is well-established in the theory of finite fields.\\n\\n1. A finite field is always isomorphic to $\\\\mathbb{Z}_p[x]/Q(x)$, for some prime $p$ and an irreducible polynomial $Q$: \\\\textbf{True.} This statement refers to the construction of finite fields using polynomial rings modulo irreducible polynomials, which is a standard method to define finite fields.\\n\\n2. A finite field is always isomorphic to $\\\\mathbb{Z}_{p^k}$, for a prime $p$ and integer $k$: \\\\textbf{False.} The notation $\\\\mathbb{Z}_{p^k}$ typically refers to the ring of integers modulo $p^k$, which is not a field for $k > 1$. Finite fields have characteristic $p$ for some prime $p$ and are not isomorphic to $\\\\mathbb{Z}_{p^k}$ for $k > 1$.\\n\\n3. A finite field is unique up to an isomorphism: \\\\textbf{True.} This statement is a fundamental result in the theory of finite fields known as the uniqueness of finite fields up to isomorphism. It means that any two finite fields of the same order are isomorphic to each other. ///Answer: B"}
{"subject": "", "question": "Question: Let \\(f(x) = 3 x \\left(\\log_2(x) \right)^2 + x^2 \\). Select the correct statement:?\n\nOptions:\nA. \\(f(x)\\) is \\(O\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\) but not \\(\\Omega\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\)\nB. None of the other statements are correct\nC. \\(f(x)\\) is \\(\\Theta \\left(3 x \\left(\\log_2(x) \right)^2 \right)  \\)\nD. \\(f(x)\\) is \\(\\Omega\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\) but not \\(O\\left(3 x \\left(\\log_2(x) \right)^2  \right)  \\)\n", "answer": "Explanations: The correct statement is: \\(f(x)\\) is \\(\\Theta \\left(3 x \\left(\\log_2(x) \\right)^2 \\right)\\). This means that \\(f(x)\\) is both \\(O\\left(3 x \\left(\\log_2(x) \\right)^2 \\right)\\) and \\(\\Omega\\left(3 x \\left(\\log_2(x) \\right)^2 \\right)\\), indicating that it grows at a similar rate as the function \\(3 x \\left(\\log_2(x) \\right)^2\\). ///Answer: C:"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement regarding the DES round function.\n\nOptions:\nA. There is an expansion operation $E$ from 32 to 48 bits.\nB. A round key is XORed to an internal register.\nC. There are $8$ identical S-boxes (substitution boxes) of size $6 \\times 4$.\nD. There is a permutation $P$ on 32-bits.\n", "answer": "Explanations: {\n    \"explaination\": \"The false statement is: There is a permutation $P$ on 32-bits. In the DES round function, there is a permutation $P$ on 32-bits after the S-box substitution, not before it.\",\n    \"answer\": \"There is a permutation $P$ on 32-bits.\"\n} ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion.\n\nOptions:\nA. Luby-Rackoff's lemma bounds the advantage of distinguisher trying to discern a 3-round Feistel scheme from another Feistel scheme.\nB. Decorrelation can express the best $d$-limited non-adaptive distinguisher between two random functions $F$ and $G$.\nC. Decorrelation uses the $a$-norm to express the advantage gained by the best, limited non-adaptive distinguisher.\nD. Decorrelation is in fact concerned only with expressing the advantage of adaptive distinguishers.\n", "answer": "Explanations: Let's examine each option carefully:\n\nA. Luby-Rackoff's lemma bounds the advantage of a distinguisher trying to discern a 3-round Feistel scheme from another Feistel scheme. \n- This statement is true. Luby-Rackoff's lemma is used to analyze the security of Feistel schemes by bounding the advantage of a distinguisher trying to distinguish a Feistel scheme from a random permutation.\n\nB. Decorrelation can express the best $d$-limited non-adaptive distinguisher between two random functions $F$ and $G.\n- This statement is false. Decorrelation is a technique used to analyze the security of block ciphers, and it focuses on measuring the advantage of a distinguisher in distinguishing the cipher from a random permutation. It is not specifically used for comparing two random functions.\n\nC. Decorrelation uses the $a$-norm to express the advantage gained by the best, limited non-adaptive distinguisher.\n- This statement is false. Decorrelation does not use the $a$-norm to express the advantage gained by a distinguisher. It typically involves analyzing the linear bias between the input and output of a block cipher.\n\nD. Decorrelation is in fact concerned only with expressing the advantage of adaptive distinguishers.\n- This statement is false. Decorrelation is used to analyze the security of block ciphers against both adaptive and non-adaptive attacks. It focuses on measuring the advantage of a distinguisher in distinguishing the cipher from a random permutation.\n\nTherefore, the true assertion is A. Luby-Rackoff's lemma bounds the advantage of a distinguisher trying to discern a 3-round Feistel scheme from another Feistel scheme. ///Answer: A"}
{"subject": "", "question": "Question: Choose the \\emph{correct} statement.\n\nOptions:\nA. $\\mathbb{Z}_n$ is a field $\\Leftrightarrow$ $n$ is a composite number\nB. $\\mathbb{Z}_n$ is a field $\\Leftrightarrow$ $\\mathbb{Z}_n^* = \\mathbb{Z}_n$\nC. $\\mathbb{Z}_n$ is a field $\\Leftrightarrow$ $n$ is a prime\nD. $\\mathbb{Z}_n$ is a field $\\Leftrightarrow$ $\\mathbb{Z}_n^* = \\emptyset$\n", "answer": "Explanations: Option A: The scientific method is a linear process that always follows the same steps in the same order.\n\nExplanation: This statement is incorrect. While the scientific method generally involves steps such as making observations, forming a hypothesis, conducting experiments, analyzing data, and drawing conclusions, it is not always a linear process. Scientists may need to go back and revise their hypothesis, repeat experiments, or consider new variables. The scientific method is more of a cyclical and iterative process rather than a strict linear one.\n\nOption B: Scientific theories are never subject to change or revision.\n\nExplanation: This statement is incorrect. Scientific theories are based on extensive evidence and have withstood rigorous testing, but they are always subject to change or revision in the face of new evidence or better explanations. The process of science involves constantly questioning and refining our understanding of the natural world, and scientific theories can be updated or even replaced as our knowledge advances.\n\nOption C: Hypotheses are educated guesses that can be proven true or false through experimentation.\n\nExplanation: This statement is correct. A hypothesis is a proposed explanation for a phenomenon that can be tested through experimentation. Hypotheses are based on existing knowledge and observations, and they can be supported or refuted by evidence gathered through experiments. The scientific method involves formulating hypotheses and testing them to determine their validity.\n\nOption D: Scientific laws are the same as scientific theories.\n\nExplanation: This statement is incorrect. Scientific laws and theories are distinct concepts in science. Scientific laws describe a consistent pattern or relationship observed in nature, such as the laws of thermodynamics or the law of gravity. Scientific theories, on the other hand, are comprehensive explanations that integrate a wide range of observations and experimental evidence to describe a phenomenon. Theories are more complex and explanatory than laws, and they can be revised or replaced as our understanding of a topic evolves. ///Answer: B"}
{"subject": "", "question": "Question: Finding collisions on a set of N elements ...\n\nOptions:\nA. requires the storage of size $ \\Omega(N).$\nB. requires time $ O({N}^{\\frac{1}{3}}).$\nC. can be done with the storage of size $O(1).$\nD. is doable for $N=2^{256}$.\n", "answer": "Explanations: 1. **Finding Collisions**: In cryptography, a collision occurs when two different inputs produce the same output in a hash function. Finding collisions means finding two different inputs that result in the same hash value.\n\n2. **Set of N Elements**: Refers to a collection of N distinct elements.\n\n3. **Options**:\n    a. **Requires the Storage of Size $ \\Omega(N)$**: This option indicates that the algorithm requires a storage space proportional to or greater than N.\n    \n    b. **Requires Time $ O({N}^{\\frac{1}{3}})$**: This option specifies the time complexity of the algorithm, indicating that the algorithm's time requirement grows at most as the cube root of N.\n    \n    c. **Can be Done with the Storage of Size $O(1)$**: This option implies that the algorithm requires constant storage space, regardless of the size of N.\n    \n    d. **Is Doable for $N=2^{256}$**: This option states that the task of finding collisions is feasible for a set of 2^256 elements, which is a large number often used in cryptography due to its relation to a 256-bit hash function.\n\nTo answer the question, finding collisions on a set of N elements:\n- Option 1 suggests that it requires storage of size at least N, indicating a significant storage requirement.\n- Option 2 implies that the time complexity is proportional to the cube root of N, which is a moderate time requirement.\n- Option 3 states that it can be achieved with constant storage space, which is efficient.\n- Option 4 affirms that finding collisions is feasible for N = 2^256.\n\nIn conclusion, finding collisions on a set of N elements can be efficiently done with constant storage size and is possible for a set of 2^256 elements. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion. Linear cryptanalysis \\ldots?\n\nOptions:\nA. was invented long before the Caesar cipher.\nB. is a chosen plaintext key recovery attack.\nC. requires $\\frac{1}{DP}$ pairs of plaintext-ciphertext.\nD. breaks DES with $2^{43}$ known plaintexts.\n", "answer": "Explanations: The correct assertion is:\n\nB: is a chosen plaintext key recovery attack.\n\nLinear cryptanalysis is a type of cryptanalysis that attempts to exploit a linear approximation of the encryption function in order to recover the secret key. It is a chosen plaintext attack, meaning the attacker can choose plaintexts to encrypt and observe the corresponding ciphertexts.\n\nOption A is incorrect because the Caesar cipher is an ancient encryption method that predates the invention of linear cryptanalysis.\n\nOption C is incorrect because the number of plaintext-ciphertext pairs required for linear cryptanalysis is typically related to the number of bits in the key, not the number of bits in the plaintext or ciphertext.\n\nOption D is incorrect because breaking DES using linear cryptanalysis requires far more known plaintexts than $2^{43}$. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{incorrect} assertion regarding plain Rabin, i.e., Rabin without any redundancy.\n\nOptions:\nA. The Rabin Key Recovery Problem relies on the discrete logarithm problem.\nB. Plain Rabin suffers from a chosen ciphertext key recovery attack.\nC. The decryption of plain Rabin is ambiguous.\nD. The Rabin Decryption Problem is equivalent to the factoring problem.\n", "answer": "Explanations: The incorrect assertion regarding plain Rabin, i.e., Rabin without any redundancy, is: \\textbf{The Rabin Key Recovery Problem relies on the discrete logarithm problem.} The correct statement is that the Rabin Key Recovery Problem does not depend on the discrete logarithm problem but rather on the factorization problem. In plain Rabin encryption, without any redundancy like in the original Rabin cryptosystem, the decryption is ambiguous due to the existence of four possible roots for each ciphertext. This ambiguity makes it vulnerable to a chosen ciphertext key recovery attack, where an attacker can recover the private key by obtaining ciphertexts of chosen plaintexts. The security of plain Rabin relies on the hardness of the Rabin Decryption Problem, which is equivalent to the factorization problem, not the discrete logarithm problem. Understanding these key principles is crucial for evaluating and implementing secure cryptographic systems. ///Answer: A"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. Problems in Bluetooth includes:?\n\nOptions:\nA. integrity protection is void.\nB. messages can be erased in radio channel.\nC. privacy protection is weak.\nD. the key of E0 cipher can be recovered within a few seconds.\n", "answer": "Explanations: The correct answer is:\n- the key of E0 cipher can be recovered within a few seconds.\n\nThis statement is incorrect because the E0 cipher is known for its security vulnerabilities, and the key recovery process for E0 cipher is not something that can be achieved within a few seconds. It requires significant computational resources and time to recover the key used in the E0 cipher. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following is TRUE when comparing Vector Space Model (VSM) and Probabilistic Language Model (PLM)? (Slide 73 Week 2)?\n\nOptions:\nA. Both VSM and PLM require parameter tuning\nB. Both VSM and PLM use collection frequency in the model\nC. Both VSM and PLM take into account multiple term occurrences\nD. Both VSM and PLM are based on a generative language model\n", "answer": "Explanations: Let's break down each option and determine which one is true:\n\n1) Both VSM and PLM require parameter tuning:\n- This statement is not entirely accurate. While both VSM and PLM may involve some parameter tuning, it is not a requirement for either model. Parameter tuning can help optimize the performance of the models, but it is not a strict requirement.\n\n2) Both VSM and PLM use collection frequency in the model:\n- This statement is generally true. Both VSM and PLM use collection frequency information to build their models. Collection frequency refers to the frequency of terms in the entire document collection.\n\n3) Both VSM and PLM take into account multiple term occurrences:\n- This statement is also true. Both VSM and PLM consider the occurrences of terms in documents. In VSM, the term frequency is important, while in PLM, the probability of terms occurring in documents is considered.\n\n4) Both VSM and PLM are based on a generative language model:\n- This statement is not true. VSM is not based on a generative language model. VSM is a geometric model that represents documents as vectors in a high-dimensional space, while PLM is based on a generative language model that estimates the probability of generating a document given a query.\n\nTherefore, the correct option among the given choices is:\n- (2) Both VSM and PLM use collection frequency in the model\n\nI hope this clarifies the comparison between Vector Space Model (VSM) and Probabilistic Language Model (PLM) for you. Let me know if you need further explanation or have any more questions. ///Answer: B"}
{"subject": "", "question": "Question: Let $S_{0},S_{1},S_{2},\\dots$ be an infinite sequence produced by a source $\\mathcal{S}$. All $S_{n}$ take values in $\\{0,1\\}$, and $S_{n+1}$ depends only on $S_n$, that is,   $p_{S_{n+1} | S_0, \\dots, S_n}(s_{n+1} | s_0, \\dots, s_n) = p_{S_{n+1} | S_n}(s_{n+1} | s_n)$.   The probability $p_{S_{n+1}|S_{n}}$ is schematically represented in the graph below:     \begin{center}    \tikzset{      state/.style = {        draw,        circle,        minimum size = 20pt,        font = \fontsize{12}{12}\\selectfont,      }    }    \begin{tikzpicture}[> = latex]      \node[state] (a) {$0$};      \node[state, right of = a] (b) {$1$};      \\path[->] (a) edge[bend left, above] node {$1/2$} (b);      \\path[->] (a) edge[loop left] node {$\frac{1}{2}$} ();      \\path[->] (b) edge[bend left, below] node {$1/4$} (a);      \\path[->] (b) edge[loop right] node {$\frac{3}{4}$} ();    \\end{tikzpicture}  \\end{center}  For instance, the edge from $0$ to $1$ means that $p_{S_{n+1}|S_{n}}(1|0) = \frac{1}{2}$. We also have that $p_{S_0}(0)=1$. True or false: The source is regular.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: To determine if the source is regular, we need to check if it satisfies the conditions of a regular source. \n\nA source is considered regular if all states have the same probability of being visited infinitely often. In this case, since the source is defined by an infinite sequence where $S_{n+1}$ only depends on $S_n$, it is important to analyze the transition probabilities between states.\n\nLooking at the transition probabilities given in the graph:\n- From state $0$ to state $1$: $p_{S_{n+1}|S_{n}}(1|0) = \\frac{1}{2}$\n- From state $1$ to state $0$: $p_{S_{n+1}|S_{n}}(0|1) = \\frac{1}{4}$\n\nGiven that the transition probabilities are not equal in both directions, the source does not satisfy the condition of a regular source where all states have the same probability of being visited infinitely often.\n\nTherefore, the correct answer is:\nB: False ///Answer: B"}
{"subject": "", "question": "Question: Recall that we say that a kernel $K: \\R \times \\R \rightarrow \\R $ is\n\t\tvalid if there exists $k \\in \\mathbb{N}$ and $\\Phi: \\R \rightarrow \\R^k$\n\t\tsuch that for all $(x, x') \\in \\R \times \\R $, $K(x, x') =  \\Phi(x)^\top \\Phi(x')$. The kernel $K(x, x') = \\cos(x + x')$ is a valid kernel.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: To determine if $K(x, x') = \\cos(x + x')$ is a valid kernel, we need to check if there exists $k \\in \\mathbb{N}$ and a feature map $\\Phi: \\R \\rightarrow \\R^k$ such that for all $(x, x') \\in \\R \\times \\R$, $K(x, x') = \\Phi(x)^T \\Phi(x')$.\n\nLet's consider the function $K(x, x') = \\cos(x + x')$. We can rewrite this as:\n$K(x, x') = \\cos(x)\\cos(x') - \\sin(x)\\sin(x')$.\n\nThis doesn't look like a simple inner product of two feature vectors in a finite-dimensional space. Therefore, it is not clear how we can find a feature map $\\Phi$ that would satisfy $K(x, x') = \\Phi(x)^T \\Phi(x')$.\n\nTherefore, the statement that $K(x, x') = \\cos(x + x')$ is a valid kernel is **False**. ///Answer: B"}
{"subject": "", "question": "Question: (Convex I) Unions of convex sets are convex.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False. \n\nWhen taking the union of two convex sets, the resulting set may not be convex. For example, consider two convex sets: a circle and a line segment. The union of these sets would be the circle together with the line segment. This union is not convex as the line segment connecting two points on the circumference of the circle may pass outside the circle. \n\nTherefore, the statement that unions of convex sets are always convex is incorrect. ///Answer: B"}
{"subject": "", "question": "Question: Pick the \\textit{correct} assertion. Kasiski test can be used to \\dots?\n\nOptions:\nA. distinguish good students in cryptography classes.\nB. break a cipher that provides perfect secrecy.\nC. break the ROT13 cipher.\nD. break the Vigen\\`ere cipher.\n", "answer": "Explanations: Alright, let's break this down step by step. The Kasiski test is a technique used in cryptanalysis to break ciphers, specifically the Vigen\u00e8re cipher. This test involves analyzing repeated patterns in the ciphertext to determine the key length used in the Vigen\u00e8re cipher.\n\nSo, out of the options provided, the correct assertion is: \\textbf{break the Vigen\\`ere cipher.}\n\nThe Kasiski test is not used to distinguish good students in cryptography classes or to break ciphers that provide perfect secrecy. While ROT13 is a simple letter substitution cipher, the Kasiski test is not typically used to break it as it is easily solvable through basic frequency analysis.\n\nTherefore, the most appropriate application of the Kasiski test is to break the Vigen\u00e8re cipher by identifying the key length through repeated patterns in the ciphertext. ///Answer: D"}
{"subject": "", "question": "Question: Regarding communities, which of the following is true?\n\nOptions:\nA. Modularity is a measure of how communities are connected together\nB. Agglomerative algorithms recursively decompose communities into sub-communities\nC. Divisive algorithms are based on modularity\nD. Girvan-Newman works by removing edges with the highest betweenness measure\n", "answer": "Explanations: The correct answer to the question is: 'Girvan-Newman works by removing edges with the highest betweenness measure'.\n\nExplanation:\n\n1. Understanding community detection algorithms: In network science, community detection algorithms are used to identify groups of nodes in a network that are more densely connected to each other than to the rest of the network.\n\n2. Modularity: Modularity is a measure of how well a network is divided into communities. It quantifies the strength of the division of a network into communities compared to a random network.\n\n3. Agglomerative algorithms: Agglomerative algorithms start with each node as a separate community and iteratively merge communities based on some criterion until a stopping condition is met.\n\n4. Divisive algorithms: Divisive algorithms start with the whole network as one community and recursively split it into sub-communities based on some criterion.\n\n5. Girvan-Newman algorithm: The Girvan-Newman algorithm works by iteratively removing edges with the highest betweenness measure. Betweenness measure is a metric that quantifies the number of shortest paths between pairs of nodes that pass through a particular edge. By removing edges with high betweenness, the algorithm effectively breaks the network into communities.\n\nTherefore, the statement 'Girvan-Newman works by removing edges with the highest betweenness measure' is true in the context of community detection algorithms. ///Answer: D"}
{"subject": "", "question": "Question: Consider the following loaded dice with $6$ faces: $P(S_1=6)=5/6$ and $P(S_1 = x)=1/30$ for $x\\in\\{1,2,3,4,5\\}$. Suppose we throw it indefinitely. Hence, we have a source $S=S_1 S_2 S_3\\ldots$. Then, $H(S_n) = H(S_{n-1})$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: Step 1: Calculate the entropy of the loaded dice:\nH(S) = -\u03a3 P(x) * log2(P(x))\n\nH(S) = - [(5/6) * log2(5/6) + (1/30) * log2(1/30) * 5]\n\nH(S) \u2248 1.17 bits\n\nStep 2: Calculate the conditional entropy of the loaded dice:\nH(Sn|Sn-1) = H(Sn) - H(Sn-1)\n\nGiven that H(Sn) = H(Sn-1), the conditional entropy is 0.\n\nStep 3: Determine if the statement is true or false:\nSince the conditional entropy is 0, it means that the outcome of Sn is completely predictable given Sn-1. Therefore, the statement \"H(Sn) = H(Sn-1)\" is true. \n\nFinal Answer: True ///Answer: A"}
{"subject": "", "question": "Question: We consider a month of 30 days. I have \\(n\\) chocolates and each day, I can either: not eat chocolate or eat exactly one chocolate. All chocolates needs to be eaten by the end of the month. What is the smallest number of chocolates needed to guarantee that I will eat chocolate 5 days in a row during the month?\n\nOptions:\nA. 27\nB. 25\nC. 24\nD. 26\n", "answer": "Explanations: To determine the smallest number of chocolates needed to guarantee that you will eat chocolate for 5 days in a row during the month, we need to consider the worst-case scenario where there is no possibility of eating chocolate 5 days in a row. This helps us find the minimum number of chocolates required to ensure that such a sequence cannot occur.\n\nLet's analyze the situation. If we are unable to have a 5-day sequence with chocolate eaten continuously, it means we can have at most 4 days in a row where chocolate is eaten. We then look for a scenario where 4 days in a row are covered, and any chocolate placed on the 5th day would form a 5-day sequence.\n\nTo find this configuration:\n- Suppose we have a sequence of 4 days where chocolate is eaten. And then, on the 5th day, we eat another chocolate to ensure we have chocolate eaten for 5 consecutive days. This means that in every 5-day block, we need to account for at least one chocolate eaten. This approach ensures that even at the month's end, we do not have a 5-day block with no chocolate consumed.\n\nFollowing this logic, the smallest number of chocolates needed to guarantee that you will have chocolate eaten for 5 days in a row during the month is:\n- 4 days in a row where chocolate is eaten, followed by another chocolate on the 5th day to ensure the 5-day sequence: \\(4 + 1 = 5\\) chocolates.\n- Given that we can repeat this pattern throughout the month:\n  - For a 30-day month, this pattern can occur 6 times (6 sets of 5 days), which totals to \\(6 \\times 5 = 30\\) chocolates.\n\nTherefore, 30 chocolates are required to guarantee that you will eat chocolate for 5 consecutive days during the month.\n\nSince the options provided do not include 30 as a choice, there may be an underlying assumption or interpretation that needs to be considered. Based on the standard scenario described above, it seems that the option that best aligns with this perspective is Option 4: 26, which is the lowest value greater than 24. ///Answer: D"}
{"subject": "", "question": "Question: Generative Adversarial Networks use the generator and discriminator models during training but only the discriminator for data synthesis.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False\n\nExplanation: Generative Adversarial Networks (GANs) utilize both the generator and discriminator models during training to generate synthetic data. The generator model generates new data samples, while the discriminator model evaluates the samples to determine if they are real or fake. This feedback loop helps both models improve over time through competition and collaboration. The generator alone cannot synthesize data without the feedback from the discriminator. ///Answer: B"}
{"subject": "", "question": "Question: In the GSM infrastructure, which of the following entity contains Ki?\n\nOptions:\nA. MS (Mobile Station)\nB. VLR (Visitor Location Register)\nC. SIM (Subscriber Identity Module)\nD. TGS (Ticket Granting Server)\n", "answer": "Explanations: In the GSM infrastructure, the entity that contains Ki is the SIM (Subscriber Identity Module). Ki, also known as the Authentication Key, is stored securely in the SIM card and is used for authenticating the subscriber's identity and ensuring secure communication between the mobile device and the network. ///Answer: C"}
{"subject": "", "question": "Question: In x86, select all synchronous exceptions?\n\nOptions:\nA. Divide error\nB. Timer\nC. Page Fault\nD. Keyboard\n", "answer": "Explanations: a) Divide error\\nc) Page Fault\\n\\nSynchronous exceptions are exceptions that occur as a direct result of executing an instruction. \\n\\na) Divide error occurs when an attempt is made to divide a number by zero.\\nc) Page Fault occurs when a program tries to access a memory location that is not currently in physical memory. \\n\\nb) Timer and d) Keyboard are examples of asynchronous exceptions, which occur independently of the current instruction being executed. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} statement. The following statements have been proven:?\n\nOptions:\nA. $\\textrm{SAT} \\in \\textrm{PSPACE} \\cap \\textrm{P}$\nB. $\\textrm{P} \\subseteq \\textrm{NP} \\subseteq \\textrm{IP}$\nC. $\\textrm{P} \\subseteq \\textrm{IP} \\cap \\textrm{NP}$\nD. $\\textrm{co-NP} \\subseteq \\textrm{PSPACE}$\n", "answer": "Explanations: The incorrect statement is: $\\textrm{P} \\subseteq \\textrm{IP} \\cap \\textrm{NP}$.\n\nThe reasoning behind this is as follows:\n- $\\textrm{P} \\subseteq \\textrm{NP}$ is a well-known fact, as all problems in $\\textrm{P}$ are also in $\\textrm{NP}$.\n- $\\textrm{NP} \\subseteq \\textrm{IP}$ is true because all problems in $\\textrm{NP}$ have interactive proofs.\n- $\\textrm{co-NP} \\subseteq \\textrm{PSPACE}$ is true because the complement of a problem in $\\textrm{NP}$ is in $\\textrm{PSPACE}$.\n\nTherefore, the incorrect statement is $\\textrm{P} \\subseteq \\textrm{IP} \\cap \\textrm{NP}$. ///Answer: C:"}
{"subject": "", "question": "Question: Let $E_{a,b}(\\mathrm{GF}(p))$ be non-singular elliptic curve with prime order, with $p$ a 256-bit prime and let $P,Q,R \\in E_{a,b}(\\mathrm{GF}(p)) \\backslash \\{\\mathcal{O}\\}$ such that $R=aP$ for an integer $a > 0$. Tick the \\textit{correct} assertion.\n\nOptions:\nA. It is hard to subtract $P$ from $Q$.\nB. The point at infinity $\\mathcal{O}$ does not have any inverse point.\nC. Given $P$ and $R$, it is hard to recover $a$.\nD. To compute the point $P+Q$, we just have to compute $(x_P+x_Q \\bmod{p}, y_P+y_Q \\bmod{p})$.\n", "answer": "Explanations: Let's break down the options given step by step:\n\n1. \"It is hard to subtract $P$ from $Q\": In elliptic curve cryptography, subtracting one point from another is essentially the same operation as adding the inverse of the second point to the first point. This operation is computationally feasible and not considered hard.\n\n2. \"The point at infinity $\\mathcal{O}$ does not have any inverse point\": In elliptic curve cryptography, the point at infinity $\\mathcal{O}$ is neutral with respect to addition, meaning that adding it to any point yields that same point. So, in this context, the point at infinity does not have an inverse point.\n\n3. \"Given $P$ and $R$, it is hard to recover $a$: Given an elliptic curve point $P$ and its multiple $R = aP$, the discrete logarithm problem arises when trying to determine the scalar $a$ from $P$ and $R$. This problem is computationally hard and forms the basis of elliptic curve cryptography security.\n\n4. \"To compute the point $P+Q$, we just have to compute $(x_P+x_Q \\bmod{p}, y_P+y_Q \\bmod{p})$: In elliptic curve cryptography, the addition of two points $P$ and $Q$ involves geometric operations and modulo arithmetic based on the curve's equation. The formula given here is not the correct way to compute the sum of two points on an elliptic curve.\n\nTherefore, the correct assertion is: \"Given $P$ and $R$, it is hard to recover $a$.\" ///Answer: C"}
{"subject": "", "question": "Question: Select what is true about the Baum-Welch algorithm.A penalty will be applied for any incorrect answers.\n\nOptions:\nA. It uses expectation maximization to estimate parameters of the Hidden Markov Model.\nB. The algorithm always converges towards a global maximum of the parameters estimated.\nC. The resulting parameters from the algorithm do not depend on the values the parameters are initialized with.\nD. If we have few word sequences annotated with PoS tags, we can use these data to set, in a supervised manner, the initial parameters of the Baum-Welch algorithm.\n\nE. You cannot randomly initialize the parameters of the algorithm.\n", "answer": "Explanations: The correct options are:\n\n1. It uses expectation maximization to estimate parameters of the Hidden Markov Model.\n2. If we have few word sequences annotated with PoS tags, we can use these data to set, in a supervised manner, the initial parameters of the Baum-Welch algorithm.\n\nExplanation:\n1. The Baum-Welch algorithm indeed uses the expectation maximization (EM) algorithm to estimate the parameters of the Hidden Markov Model.\n2. The algorithm may not always converge towards a global maximum, as it can get stuck in local optima. Therefore, this statement is incorrect.\n3. The resulting parameters from the Baum-Welch algorithm can depend on the initial values of the parameters. Therefore, this statement is incorrect.\n4. If we have some annotated data available, we can use it to initialize the parameters of the algorithm in a supervised manner, which can help improve the convergence and performance of the algorithm. ///Answer: A"}
{"subject": "", "question": "Question: Which one of these is \\emph{not} a stream cipher?\n\nOptions:\nA. IDEA\nB. RC4\nC. A5/1\nD. E0\n", "answer": "Explanations: 1. IDEA: IDEA is a block cipher, not a stream cipher. It operates on fixed-size blocks of data, encrypting each block individually.\n\n2. RC4: RC4 is a stream cipher. It generates a pseudorandom key stream to encrypt plaintext data bit by bit or byte by byte.\n\n3. A5/1: A5/1 is also a stream cipher. It is used in GSM mobile networks for encryption of voice and data traffic.\n\n4. E0: E0 is a stream cipher used in Bluetooth technology for encryption and authentication. It is not a block cipher.\n\nTherefore, the answer is IDEA, as it is the only option that is not a stream cipher. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion regarding the Diffie-Hellman key exchange over a group $G$.\n\nOptions:\nA. It is never secure against a passive adversary.\nB. It is not secure if you apply a key derivation function to the output of the exchange.\nC. It is not secure when discrete logarithms in $G$ can be computed efficiently.\nD. It is always secure if the order of $G$ is a prime number.\n", "answer": "Explanations: The correct assertion is: It is not secure when discrete logarithms in $G$ can be computed efficiently.\n\nThe security of the Diffie-Hellman key exchange relies on the assumption that the discrete logarithm problem (DLP) in the group $G$ is computationally hard. If an adversary can efficiently compute discrete logarithms in $G, they can break the security of the Diffie-Hellman key exchange. This is why it is important to choose a group $G$ in which the discrete logarithm problem is believed to be hard, such as using large prime order groups or elliptic curve groups with suitable parameters. \n\nThe other options are incorrect:\n- It is not secure against a passive adversary: Diffie-Hellman key exchange is secure against passive adversaries as long as the discrete logarithm problem is hard in the chosen group.\n- It is not secure if you apply a key derivation function to the output of the exchange: Applying a key derivation function to the output of the Diffie-Hellman key exchange can actually enhance the security of the derived key.\n- It is always secure if the order of $G$ is a prime number: While using a prime order group can be a good choice, it is not a guarantee of security. The security of the Diffie-Hellman key exchange depends on the hardness of the discrete logarithm problem in the chosen group. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement. The Shannon Encryption Model ...\n\nOptions:\nA. requires a black-box encryption model.\nB. assumes a known input distribution.\nC. assumes the key is independent from the message.\nD. requires the correctness property $\\Pr[C_K^{-1}(C_K(X))=X]=1$.\n", "answer": "Explanations: 1. requires a black-box encryption model. - \\textbf{True}\n2. assumes a known input distribution. - \\textbf{False}\n3. assumes the key is independent from the message. - \\textbf{False}\n4. requires the correctness property $\\Pr[C_K^{-1}(C_K(X))=X]=1$. - \\textbf{True} \n\nTherefore, the false statements are:\n2. assumes a known input distribution.\n3. assumes the key is independent from the message. ///Answer: B:"}
{"subject": "", "question": "Question: Which of the following algorithms is \\emph{not} a hash function?\n\nOptions:\nA. SHA-1\nB. MD5\nC. RC4\nD. MD4\n", "answer": "Explanations: To determine which of the following algorithms is not a hash function, we need to understand what a hash function is. \n\nA hash function is a mathematical function that takes an input (or \"message\") and returns a fixed-size string of bytes. This string is typically a hexadecimal number that represents a unique fingerprint of the input data. The key properties of a hash function are determinism (the same input will always produce the same output), quick computation, and resistance to collisions (different inputs will not produce the same output).\n\nNow, let's analyze each of the algorithms given:\n\nA: SHA-1 - SHA-1 is a cryptographic hash function that produces a 160-bit (20-byte) hash value known as a message digest. It is widely used for secure communication and data integrity verification. SHA-1 is a hash function.\n\nB: MD5 - MD5 is a widely used cryptographic hash function that produces a 128-bit (16-byte) hash value. While MD5 was once widely used, it is now considered to be broken and insecure due to vulnerabilities that allow for collision attacks. MD5 is a hash function.\n\nC: RC4 - RC4 is a symmetric stream cipher algorithm that is used for encryption, not hashing. It is not a hash function.\n\nD: MD4 - MD4 is a cryptographic hash function that produces a 128-bit (16-byte) hash value. Similar to MD5, MD4 is considered to be broken and insecure due to vulnerabilities that allow for collision attacks. MD4 is a hash function.\n\nTherefore, the algorithm that is not a hash function is C: RC4. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following is true for Recommender Systems (RS)?\n\nOptions:\nA. Matrix Factorization can predict a score for any user-item combination in the dataset.\nB. The complexity of the Content-based RS depends on the number of users\nC.  Item-based RS need not only the ratings but also the item features\nD. Matrix Factorization is typically robust to the cold-start problem.\n", "answer": "Explanations: To determine which statement is true for Recommender Systems (RS), let's analyze each option:\n\n1. Matrix Factorization can predict a score for any user-item combination in the dataset:\n   - True. Matrix Factorization is a common technique used in collaborative filtering-based recommender systems to predict ratings for user-item pairs based on the matrix of existing ratings.\n\n2. The complexity of the Content-based RS depends on the number of users:\n   - False. The complexity of a content-based recommender system typically depends on the number of items or features in the dataset, not the number of users.\n\n3. Item-based RS need not only the ratings but also the item features:\n   - True. Item-based collaborative filtering methods use item-item similarities and do not solely depend on user-item ratings but also on item features.\n\n4. Matrix Factorization is typically robust to the cold-start problem:\n   - False. Matrix Factorization suffers from the cold-start problem, which occurs when new users or items have limited or no ratings in the system. Matrix Factorization requires existing ratings to make predictions.\n\nTherefore, the true statement for Recommender Systems (RS) among the given options is:\n- Matrix Factorization can predict a score for any user-item combination in the dataset. ///Answer: A"}
{"subject": "", "question": "Question: How many generators are there in $\\mathbb{Z}_n$?\n\nOptions:\nA. $1$\nB. $n-1$\nC. $n$\nD. $\\varphi (n)$\n", "answer": "Explanations: The number of generators in $\\mathbb{Z}_n$ can be determined by finding the number of elements in $\\mathbb{Z}_n$ that are coprime to $n$. \n\n1. $1$: The element $1$ is always a generator in $\\mathbb{Z}_n$. This is because $1$ raised to any power will cycle through all the elements in $\\mathbb{Z}_n$, as $1^n \\equiv 1 \\mod n$ for all $n$.\n\n2. $n-1$: The number of generators in $\\mathbb{Z}_n$ is equal to the number of elements in $\\mathbb{Z}_n$ that are coprime to $n$. This is also known as Euler's totient function, denoted by $\\varphi(n)$. When $n$ is a prime number, all elements in $\\mathbb{Z}_n$ are coprime to $n$ except for $0$. Therefore, the number of generators is $n-1$.\n\n3. $n$: If $n$ is a prime number, all elements in $\\mathbb{Z}_n$ are coprime to $n$ except for $0$. In this case, all elements in $\\mathbb{Z}_n$ are generators, so the number of generators is equal to $n$.\n\n4. $\\varphi(n)$: As mentioned earlier, the number of generators in $\\mathbb{Z}_n$ is equal to the number of elements in $\\mathbb{Z}_n$ that are coprime to $n$, which is given by Euler's totient function $\\varphi(n)$.\n\nTherefore, the number of generators in $\\mathbb{Z}_n$ can be $1$, $n-1$, $n$, or $\\varphi(n)$, depending on the value of $n$. ///Answer: D"}
{"subject": "", "question": "Question: The difference between an interactive proof of knowledge and an interactive proof system relies in \\ldots?\n\nOptions:\nA. \\ldots the completeness property.\nB. \\ldots the soundness property.\nC. \\ldots the termination property.\nD. \\ldots the names of the participants.\n", "answer": "Explanations: The difference between an interactive proof of knowledge and an interactive proof system lies in the completeness and soundness properties. \n\nIn an interactive proof of knowledge, the prover not only proves a statement but also convinces the verifier that they possess knowledge related to the statement being proven. This additional aspect of proving knowledge is not required in a general interactive proof system, where the focus is on the completeness and soundness properties of the proof without specifically proving knowledge. ///Answer: A"}
{"subject": "", "question": "Question: A proof system is perfect-black-box zero-knowledge if \\dots?\n\nOptions:\nA. for any PPT verifier $V$, there exists a PPT simulator $S$, such that $S$ produces an output which is hard to distinguish from the view of the verifier.\nB. for any PPT simulator $S$ and for any PPT verifier $V$, $S^{V}$ produces an output which has the same distribution as the view of the verifier.\nC. there exists a PPT simulator $S$ such that for any PPT verifier $V$, $S^{V}$ produces an output which has the same distribution as the view of the verifier.\nD. there exists a PPT verifier $V$ such that for any PPT simulator $S$, $S$ produces an output which has the same distribution as the view of the verifier.\n", "answer": "Explanations: To identify the correct option for a perfect-black-box zero-knowledge proof system, let's break down the concept:\n\n1. In a zero-knowledge proof system, a prover can convince a verifier of a statement's truth without revealing any additional information beyond the validity of the statement. This means that the verifier learns nothing from the interaction that they couldn't have learned by themselves.\n\n2. Black-box zero-knowledge extends this concept to consider the verifier as a polynomial-time Turing machine with access to an oracle. This oracle provides an abstract representation of the prover's computation, allowing the verifier to make queries and receive responses without seeing the internal details of the computation.\n\nNow, let's analyze the options:\n\n- **Option 1**: for any PPT verifier V, there exists a PPT simulator S, such that S produces an output which is hard to distinguish from the view of the verifier.\n    - This option describes a scenario where the simulator can mimic the view of the verifier convincingly. However, this alone does not ensure that the simulator's output matches the verifier's view in terms of the distribution of outputs.\n\n- **Option 2**: for any PPT simulator S and for any PPT verifier V, S^V produces an output which has the same distribution as the view of the verifier.\n    - This option correctly states that the simulator's output should have the same distribution as the verifier's view. It ensures that the verifier cannot distinguish between the real interaction and the simulation.\n\n- **Option 3**: there exists a PPT simulator S such that for any PPT verifier V, S^V produces an output which has the same distribution as the view of the verifier.\n    - This option is essentially the same as Option 2 but in a slightly different wording, stating that there exists a simulator that can match the verifier's view distribution for any verifier.\n\n- **Option 4**: there exists a PPT verifier V such that for any PPT simulator S, S produces an output which has the same distribution as the view of the verifier.\n    - This option does not align with the concept of zero-knowledge proof systems. The verifier should not dictate the simulation; instead, the simulator should be able to produce outputs indistinguishable from the verifier's view for any verifier.\n\nTherefore, the correct option for a perfect-black-box zero-knowledge proof system is: for any PPT simulator S and for any PPT verifier V, S^V produces an output which has the same distribution as the view of the verifier. ///Answer: B"}
{"subject": "", "question": "Question: The multiplication of two quadratic residues ...\n\nOptions:\nA. is not a quadratic residue.\nB. is always a quadratic residue.\nC. can be either a quadratic residue or not.\nD. is non-invertible.\n", "answer": "Explanations: In number theory, a quadratic residue modulo \\( n \\) is an integer \\( a \\) such that there exists an integer \\( x \\) satisfying \\( x^2 \\equiv a \\pmod n \\). \n\nLet's analyze the given options regarding the multiplication of two quadratic residues:\n\n1: The multiplication of two quadratic residues is not a quadratic residue:\n- This statement is false. The product of two quadratic residues modulo a prime \\( p \\) is always a quadratic residue. This property is a consequence of the fact that the product of two quadratic residues \\( a \\) and \\( b \\) modulo a prime \\( p \\) results in another quadratic residue modulo \\( p \\).\n\n2: The multiplication of two quadratic residues is always a quadratic residue:\n- This statement is generally true. When two quadratic residues are multiplied modulo a prime, the result is indeed a quadratic residue.\n\n3: The multiplication of two quadratic residues can be either a quadratic residue or not:\n- This statement is incorrect based on the definition and properties of quadratic residues.\n\n4: The multiplication of two quadratic residues is non-invertible:\n- This statement is not relevant to the concept of quadratic residues. In modular arithmetic, the product of two numbers may or may not have a multiplicative inverse based on the modulus, but it does not affect the property of being a quadratic residue.\n\nTherefore, the correct option is:\n2: The multiplication of two quadratic residues is always a quadratic residue. ///Answer: B"}
{"subject": "", "question": "Question: Given the 2-itemsets {1, 2}, {1, 3}, {1, 5}, {2, 3}, {2, 5}, when generating the 3-itemset we will:?\n\nOptions:\nA. Have 4 3-itemsets after the join and 4 3-itemsets after the prune\nB. Have 4 3-itemsets after the join and 2 3-itemsets after the prune\nC. Have 3 3-itemsets after the join and 3 3-itemsets after the prune\nD. Have 2 3-itemsets after the join and 2 3-itemsets after the prune\n", "answer": "Explanations: To generate the 3-itemsets from the given 2-itemsets {1, 2}, {1, 3}, {1, 5}, {2, 3}, {2, 5}, we need to perform a join and a prune step.\n\n1. Join Step: We will join the 2-itemsets based on the (k-1)-itemsets that they share. In this case, we are looking for 2-itemsets that share the same (k-1)-1 itemsets, which would be the itemsets {1, 2}, {1, 3}, {1, 5}, {2, 3}, and {2, 5}. The possible 3-itemsets that can be formed from this join are {1, 2, 3}, {1, 2, 5}, {1, 3, 5}, and {2, 3, 5}.\n\n2. Prune Step: In the prune step, we need to check if all the (k-1)-subsets of the candidate 3-itemsets are frequent. In this case, we need to check if {1, 2}, {1, 3}, {1, 5}, {2, 3}, and {2, 5} are frequent 2-itemsets. Since all of these 2-itemsets are present in the given set, all 4 of the candidate 3-itemsets {1, 2, 3}, {1, 2, 5}, {1, 3, 5}, and {2, 3, 5} are valid.\n\nTherefore, the correct answer is:\n\n- Have 4 3-itemsets after the join and 4 3-itemsets after the prune ///Answer: A"}
{"subject": "", "question": "Question: Suppose a file system used only for reading immutable files in random fashion. What is the best block allocation strategy?\n\nOptions:\nA. Linked-list allocation\nB. Continuous allocation\nC. Index allocation with B-tree\nD. Index allocation with Hash-table\n", "answer": "Explanations: First, since the files are immutable and only for reading, there is no need to worry about frequent updates or fragmentation of the files. This means that we do not need to consider strategies like linked-list allocation or continuous allocation, which are more suitable for systems with frequent updates.\n\nIn this case, the best block allocation strategy would be Index allocation with a B-tree or Hash-table. Both of these strategies provide efficient random access to blocks of data, which is important for reading files in random fashion. \n\nHowever, between the two, Index allocation with a B-tree is typically more efficient for larger file systems as it provides balanced access to blocks and supports efficient range queries. On the other hand, Index allocation with a Hash-table may be more suitable for smaller file systems with simpler access patterns.\n\nTherefore, the best block allocation strategy for a file system used only for reading immutable files in random fashion would be Index allocation with a B-tree. ///Answer: C"}
{"subject": "", "question": "Question: Suppose we have the binary plaintext $0011$ and we encrypt it with the Vernam cipher and the binary key $0101$. The ciphertext is\\dots?\n\nOptions:\nA. $0110$.\nB. $1000$.\nC. $0111$.\nD. $0001$.\n", "answer": "Explanations: Reasoning:\n1. The Vernam cipher, also known as a one-time pad, performs bitwise XOR operation between the plaintext and the key to generate the ciphertext.\n2. Given plaintext $0011$ and key $0101$, we perform XOR operation bit by bit:\n   - Plaintext: $0011$\n   - Key: $0101$\n   - Ciphertext: $0011 \\oplus 0101 = 0110$\n $0110$. ///Answer: A"}
{"subject": "", "question": "Question: A colleague challenges you to create a $(n-1,k,d_{min})$ code $\\mathcal C'$ from a $(n,k,d_{min})$ code $\\mathcal C$ as follows: given a generator matrix $G$ that generates $\\mathcal C$, drop one column from $G$. Then, generate the new code with this truncated $k \times (n-1)$ generator matrix. The catch is that your colleague only gives you a set $\\mathcal S=\\{\\vec s_1,\\vec s_2, \\vec s_3\\}$ of $3$ columns of $G$ that you are allowed to drop, where $\\vec s_1$ is the all-zeros vector, $\\vec s_2$ is the all-ones vector, and $\\vec s_3$ is a canonical basis vector. From the length of the columns $s_i$ you can infer $k$. You do not know $n$, neither do you know anything about the $n-3$ columns of $G$ that are not in $\\mathcal S$. However, your colleague tells you that $G$ is in systematic form, i.e., $G=[I ~~ P]$ for some unknown $P$, and that all of the elements in $\\mathcal S$ are columns of $P$. Which of the following options in $\\mathcal S$ would you choose as the column of $G$ to drop?\n\nOptions:\nA. $\\vec s_1$ (the all-zeros vector).\nB. $\\vec s_2$ (the all-ones vector)\nC. $\\vec s_3$ (one of the canonical basis vectors).\nD. It is impossible to guarantee that dropping a column from $\\mathcal S$ will not decrease the minimum distance.\n", "answer": "Explanations: To solve this problem, we need to consider the implications of dropping each column from the set $\\mathcal S$ on the code parameters of the resulting $(n-1,k,d_{min})$ code $\\mathcal C'$.\n\nThe correct answer is: '$\\vec s_3$ (one of the canonical basis vectors).'\n\nHere is the step-by-step explanation:\n\n1. Given that the generator matrix $G$ is in systematic form $G=[I ~~ P]$, and all elements in $\\mathcal S$ are columns of $P$, dropping the column corresponding to $\\vec s_3$ (canonical basis vector) from $P$ will not affect the systematic part $I$ of $G$.\n\n2. Dropping the column corresponding to $\\vec s_3$ from $P$ will result in a new $k \\times (n-1)$ generator matrix $G'$ where $G'=[I ~~ P']$.\n\n3. By dropping the canonical basis vector, the resulting code $\\mathcal C'$ will still have the same systematic form $[I ~~ P']$ as the original code $\\mathcal C$.\n\n4. Dropping the column corresponding to $\\vec s_3$ will not affect the minimum distance $d_{min}$ of the code because the canonical basis vectors are linearly independent, and dropping one of them does not reduce the minimum distance.\n\n5. Therefore, dropping the column corresponding to $\\vec s_3$ is the correct choice as it ensures that the minimum distance $d_{min}$ of the new code $\\mathcal C'$ remains the same as the original code $\\mathcal C$.\n\nIn summary, the optimal choice is to drop the column corresponding to $\\vec s_3$ (one of the canonical basis vectors) to create the $(n-1,k,d_{min})$ code $\\mathcal C'$. ///Answer: C"}
{"subject": "", "question": "Question: Let $b$ be the maximum number of linearly independent columns of a parity check matrix $H$ of a linear code. True or false: Then, the minimum distance of the code is $b+1$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: This statement is false. The minimum distance of a linear code is not necessarily equal to $b+1$, where $b$ is the maximum number of linearly independent columns of a parity check matrix $H$. The minimum distance of a code depends on various factors such as the specific code construction and properties.\n\nTherefore, the correct option is:\nOption 1: False ///Answer: B"}
{"subject": "", "question": "Question: For a blockcipher $B:\\{0,1\\}^k\\times \\{0,1\\}^n \\rightarrow \\{0,1\\}^n$ that has decorrelation $Dec^q_{\\| \\cdot \\|_{\\infty}}(B,C^*)=d$ (from a perfect cipher $C^*$), the best advantage of \\textit{any} distinguisher that makes $q$ queries is \\ldots?\n\nOptions:\nA. bounded by $d/2$.\nB. not related to $d$; we have to use the $a$-norm to get a more general result.\nC. bounded by $d$.\nD. bounded by $d-\\frac{1}{2}$.\n", "answer": "Explanations: In cryptography, when we talk about the advantage of a distinguisher in distinguishing a block cipher from a perfect cipher, we are essentially referring to how much better the distinguisher is at distinguishing the two compared to random chance.\n\nGiven that the decorrelation $Dec^q_{\\| \\cdot \\|_{\\infty}}(B,C^*)=d$, where $d$ represents the statistical distance between the distributions produced by the block cipher $B$ and the perfect cipher $C^*$, the best advantage of any distinguisher that makes $q$ queries is bounded by $d/2$. This is because the statistical distance provides a measure of how \"far\" the block cipher distribution is from the perfect cipher distribution, and the advantage is directly related to this distance.\n\nTherefore, the correct answer is: $\\boxed{\\text{1: bounded by }d/2}$. ///Answer: A"}
{"subject": "", "question": "Question: For an interactive proof system, the difference between perfect, statistical and computational zero-knowledge is based on \\ldots?\n\nOptions:\nA. \\ldots the distinguishability between some distributions.\nB. \\ldots the percentage of recoverable information from a transcript with a honest verifier.\nC. \\ldots the number of times the protocol is run between the prover and the verifier.\nD. \\ldots whether the inputs are taken in $\\mathcal{P}$, $\\mathcal{NP}$ or $\\mathcal{IP}$.\n", "answer": "Explanations: Option 1 is the correct choice. The difference between perfect, statistical, and computational zero-knowledge in an interactive proof system is based on the distinguishability between some distributions. \n\n- Perfect zero-knowledge: In a perfect zero-knowledge proof system, the verifier cannot distinguish between the interactions with a real prover and a simulated prover, even with unbounded computational power.\n\n- Statistical zero-knowledge: In a statistical zero-knowledge proof system, the verifier cannot distinguish between the interactions with a real prover and a simulated prover, except with a negligible probability.\n\n- Computational zero-knowledge: In a computational zero-knowledge proof system, the verifier cannot efficiently distinguish between the interactions with a real prover and a simulated prover, i.e., the advantage of the verifier in distinguishing between the two is negligible. \n\nThese definitions are based on the distinguishability between the distributions of the interactions with a real prover and a simulated prover. ///Answer: A"}
{"subject": "", "question": "Question: Suppose traditional inode pointer structure in ext3, i.e. 12 direct pointers, 1 singly, 1 doubly and 1 triply indirect pointer. Further suppose block size of 1kB and a 64-bit pointer. What is the maximal possible size for a single file?\n\nOptions:\nA. 512kB\nB. 2MB\nC. 4MB\nD. 10MB\nE. 1GB\nF. 4GB\n", "answer": "Explanations: Let's calculate the maximal possible size for a single file step by step:\n\n1. Direct pointers:\n12 direct pointers * block size = 12 * 1kB = 12kB\n\n2. Singly indirect pointer:\n1 singly indirect pointer * block size * number of pointers in the block = 1 * 1kB * (1kB / 8 bytes) = 128kB\n\n3. Doubly indirect pointer:\n1 doubly indirect pointer * block size * number of pointers in the singly indirect block * number of pointers in the doubly indirect block = 1 * 1kB * (1kB / 8 bytes) * (1kB / 8 bytes) = 16MB\n\n4. Triply indirect pointer:\n1 triply indirect pointer * block size * number of pointers in the singly indirect block * number of pointers in the doubly indirect block * number of pointers in the triply indirect block = 1 * 1kB * (1kB / 8 bytes) * (1kB / 8 bytes) * (1kB / 8 bytes) = 2GB\n\nAdding all the sizes together:\n12kB (direct) + 128kB (singly indirect) + 16MB (doubly indirect) + 2GB (triply indirect) = 2,153,612kB = 2.15GB\n\nTherefore, the maximal possible size for a single file is 2.15GB, which is closest to the option 5) 1GB. ///Answer: E"}
{"subject": "", "question": "Question: Consider the program below. Tick the correct answer. def fun(x: List[Int]) = if x.isEmpty then None else Some(x) val lists = List(List(1, 2, 3), List(), List(4, 5, 6)) for \\t l <- lists \\t v1 <- fun(l) \\t v2 <- fun(v1) yield v2?\n\nOptions:\nA. This program does not compile.\nB. This program compiles and the last statement has type List[Int].\nC. This program compiles and the last statement has type List[List[Int]].\nD. This program compiles and the last statement has type List[Option[List[Int]]].\nE. This program compiles and the last statement has type List[List[Option[Int]]].\nF. This program compiles and the last statement has type List[Option[Int]].\nG. This program compiles and the last statement has type List[Some[Int]].\nH. This program compiles and the last statement has type Some[List[Int]].\nI. This program compiles and the last statement has type Option[List[Int]].\n", "answer": "Explanations: I: This program compiles and the last statement has type Option[List[Int]].\n\nExplanation:\n- The program defines a function fun that takes a List of Integers and returns an Option of List[Int].\n- It then creates a list of lists (lists) with elements List(1, 2, 3), List(), List(4, 5, 6).\n- The for-comprehension iterates over each list in lists (l), applies fun to l (v1), and then applies fun again to v1 (v2).\n- Since fun returns an Option, the last statement in the for-comprehension will have type Option[List[Int]].\n- Therefore, the correct answer is I: This program compiles and the last statement has type Option[List[Int]]. ///Answer: I"}
{"subject": "", "question": "Question: Which assumption(s) is (are) required during the exchange of a key which will be used by a block cipher?\n\nOptions:\nA. Nothing.\nB. Authentication.\nC. Authentication and integrity.\nD. Confidentiality.\n", "answer": "Explanations: The assumption of authentication is required during the exchange of a key which will be used by a block cipher. This is because the parties involved in the exchange need to verify each other's identities to ensure that the key is being securely shared between the intended recipients. Without authentication, there is a risk of the key being intercepted or manipulated by a malicious third party. \n\nThe assumption of integrity may also be required, as ensuring the integrity of the key during the exchange process is essential to prevent any unauthorized modifications or tampering. This can help guarantee that the key remains valid and secure throughout the encryption process. \n\nConfidentiality is not necessarily a required assumption during the exchange of a key for a block cipher, as the focus is on securely sharing the key rather than keeping the key itself confidential. However, confidentiality may be important in other aspects of the encryption process to protect the encrypted data. ///Answer: B"}
{"subject": "", "question": "Question: Select all statements that are true.A penalty will be applied for any wrong answers.\n\nOptions:\nA. Phrase-structure grammars are relatively better suited for fixed-order languages than free-order languages.\nB. Dependency grammars describe functional dependencies between words in a sequence.\nC. Phrase-structure grammars better describe selectional constraints.\nD. The expressive power of context-free grammars are higher than that of context-dependent grammars.\nE. Any context-free grammar can be transformed into Chomsky-Normal form.\nF. Dependency grammars\u00a0better describe\u00a0positional constraints.\n", "answer": "Explanations: The correct statements from the given options are:\n1. 'The expressive power of context-free grammars are higher than that of context-dependent grammars.'\n2. 'Dependency grammars describe functional dependencies between words in a sequence.'\n3. 'Any context-free grammar can be transformed into Chomsky-Normal form.'\n\nLet's analyze each option one by one:\n\n1. 'Phrase-structure grammars are relatively better suited for fixed-order languages than free-order languages.'\n- This statement is not necessarily true. Phrase-structure grammars can be used for both fixed-order and free-order languages. Thus, this statement is not correct.\n\n2. 'The expressive power of context-free grammars are higher than that of context-dependent grammars.'\n- This statement is correct. Context-free grammars have higher expressive power compared to context-dependent grammars in terms of language generation capabilities.\n\n3. 'Dependency grammars better describe positional constraints.'\n- This statement is not entirely accurate. Dependency grammars primarily focus on functional dependencies between words rather than positional constraints.\n\n4. 'Phrase-structure grammars better describe selectional constraints.'\n- This statement is not correct. Phrase-structure grammars focus on the hierarchical structure of sentences rather than selectional constraints.\n\n5. 'Dependency grammars describe functional dependencies between words in a sequence.'\n- This statement is correct. Dependency grammars are specifically designed to describe the relationships and dependencies between words in a sequence.\n\n6. 'Any context-free grammar can be transformed into Chomsky-Normal form.'\n- This statement is true. Chomsky-Normal form is a standard form for context-free grammars, and any context-free grammar can be transformed into this form.\n\nTherefore, the correct statements are options 2, 5, and 6. These choices align with the key points of understanding the differences between grammar formalisms and the transformations that can be applied to context-free grammars. ///Answer: B"}
{"subject": "", "question": "Question: A Carmichael number is?\n\nOptions:\nA. a prime number which cannot pass the Rabin-Miller test.\nB. a composite number which often passes the Rabin-Miller test.\nC. a prime number which cannot pass the Fermat test.\nD. a composite number which often passes the Fermat test.\n", "answer": "Explanations: First, let's define what a Carmichael number is. A Carmichael number is a composite number that satisfies the modular arithmetic congruence condition for being a pseudoprime to every base coprime to it. In simpler terms, a Carmichael number is a composite number that behaves like a prime number in certain primality tests, leading to potential false positives in these tests.\n\nNow, let's evaluate each option based on this definition:\n\nOption 1: a prime number which cannot pass the Rabin-Miller test.\n- This option is incorrect because Carmichael numbers are composite numbers, not prime numbers. Therefore, a Carmichael number cannot be a prime number.\n\nOption 2: a composite number which often passes the Rabin-Miller test.\n- This option is incorrect because Carmichael numbers do not often pass the Rabin-Miller test. While they can pass the test for some bases, they eventually fail for all bases coprime to the Carmichael number.\n\nOption 3: a prime number which cannot pass the Fermat test.\n- This option is incorrect because Carmichael numbers are not prime numbers. They are composite numbers that pass the Fermat test for some bases, leading to the false impression of being prime.\n\nOption 4: a composite number which often passes the Fermat test.\n- This option is correct. Carmichael numbers are composite numbers that can pass the Fermat test for some bases, which contributes to their pseudoprime behavior.\n\nTherefore, the correct option is:\nOption 4: a composite number which often passes the Fermat test.\n\nThe other options are incorrect because they incorrectly describe the characteristics of Carmichael numbers. ///Answer: D"}
{"subject": "", "question": "Question: The Generic Collision Search Algorithm is based on ...\n\nOptions:\nA. the Dictionary Attack.\nB. Birthday Paradox.\nC. the simulation of quantum state on a standard machine.\nD. Murphy's Law.\n", "answer": "Explanations: The correct answer is \"Birthday Paradox.\"\n\nExplanation:\n- The Dictionary Attack is not the basis of the Generic Collision Search Algorithm. A Dictionary Attack is a different method used to crack passwords or encryption.\n- The simulation of quantum state on a standard machine is not the basis of the Generic Collision Search Algorithm. While quantum algorithms may have implications for cryptography, the Generic Collision Search Algorithm itself does not rely on simulating quantum states.\n- Murphy's Law is not the basis of the Generic Collision Search Algorithm. Murphy's Law is a general adage about things going wrong.\n\nThe Birthday Paradox, on the other hand, is directly related to the Generic Collision Search Algorithm. The Birthday Paradox demonstrates the probability of collisions in hash functions, which is a key concept in understanding and implementing collision search algorithms in cryptography. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion.\n\nOptions:\nA. The information one can obtain from two ciphertexts from the Vernam cipher using the same key is irrelevant.\nB. Given any plaintext space, the Vernam cipher is perfectly secure for any key distribution.\nC. Given any plaintext space, the Caesar cipher is perfectly secure for some key distributions.\nD. The Vernam cipher is perfectly secure for any plaintext distribution.\n", "answer": "Explanations: First, let's analyze each potential answer:\n1) The information from two ciphertexts from the Vernam cipher using the same key is not irrelevant, as the key can be easily recovered by XORing the two ciphertexts together.\n2) The Vernam cipher is perfectly secure if the key is truly random and at least as long as the plaintext. However, if the key is not truly random or is shorter than the plaintext, it is not perfectly secure.\n3) The Caesar cipher is not perfectly secure for any key distribution, as it only has 25 possible keys which can be easily brute-forced.\n4) The Vernam cipher is perfectly secure for any plaintext distribution if the key is truly random and at least as long as the plaintext.\n\nTherefore, the correct assertion is:\nAnswer 4) The Vernam cipher is perfectly secure for any plaintext distribution. ///Answer: D"}
{"subject": "", "question": "Question: What is $(\\frac{9}{43})$?\n\nOptions:\nA. 1\nB. -1\nC. 0\nD. .209\n", "answer": "Explanations: To find the value of $\\\\left(\\\\frac{9}{43}\\\\right)$, we need to divide 9 by 43.\\n\\nPerforming the division, we get:\\n\\\\[ \\\\frac{9}{43} = 0.20930232558... \\\\]\\n\\nRounded to three decimal places, $\\\\left(\\\\frac{9}{43}\\\\right) \\\\approx 0.209$.\\n\\nTherefore, the correct answer choice is:\\n3: \".209\"\\n\\nThe value is a positive fraction between 0 and 1. ///Answer: D"}
{"subject": "", "question": "Question: The Fermat test \\emph{cannot} output\\dots?\n\nOptions:\nA. ``prime'' for a ``composite'' number.\nB. ``prime'' for a ``prime'' number.\nC. ``composite'' for a ``composite'' number.\nD. ``composite'' for a ``prime'' number.\n", "answer": "Explanations: The Fermat test is a probabilistic primality test that checks if a given number is likely to be prime based on Fermat's Little Theorem. When the Fermat test outputs \"composite\" for a number, it means that the number is composite, which indicates that it is not a prime number. \n\nTherefore, the Fermat test cannot output \"prime\" for a composite number. The correct selection from the given options is: \"prime\" for a \"composite\" number. ///Answer: A"}
{"subject": "", "question": "Question: LFSRs are used within \\ldots?\n\nOptions:\nA. IDEA\nB. A5/1\nC. AES\nD. SAFER\n", "answer": "Explanations: A: LFSRs are used within A5/1. This algorithm is used in the encryption of GSM (Global System for Mobile Communications) communication. Therefore, the correct answer is A5/1. ///Answer: B"}
{"subject": "", "question": "Question: Consider $GF(8)$ defined as $\\mathbb{Z}_2[X]/(P(X))$ with $P(x) = X^3 + X + 1$. Compute $X^2 \\times (X + 1)$ in $\\mathbb{Z}_2[X]/(P(X))$?\n\nOptions:\nA. $X^2+X+1$.\nB. $X^2 + 1$.\nC. $X^2$.\nD. $X+1$.\n", "answer": "Explanations: To compute $X^2 \\times (X + 1)$ in $\\mathbb{Z}_2[X]/(P(X))$, we need to perform polynomial multiplication and reduction modulo $P(X) = X^3 + X + 1$.\n\n1. Start by multiplying $X^2$ by $X + 1$:\n\\[ X^2 \\times (X + 1) = X^3 + X^2 \\]\n\n2. Next, reduce the result modulo $P(X) = X^3 + X + 1$:\n\\[ X^3 + X^2 \\mod (X^3 + X + 1) \\]\n\nUsing polynomial long division or polynomial reduction, we can divide $X^3 + X^2$ by $X^3 + X + 1$:\n\\[ X^3 + X^2 = (X + 1)(X^3 + X + 1) + (X + 1) \\]\n\nThis means that $X^3 + X^2$ is equivalent to $(X + 1)$ in $\\mathbb{Z}_2[X]/(P(X))$. \n\nTherefore, $X^2 \\times (X + 1)$ in $\\mathbb{Z}_2[X]/(P(X))$ simplifies to $X + 1$. \n\nThe correct option is: $X + 1$. ///Answer: D"}
{"subject": "", "question": "Question: Church booleans are a representation of booleans in the lambda calculus. The Church encoding of true and false are functions of two parameters: Church encoding of tru: t => f => t Church encoding of fls: t => f => f Which of the following functions implements not b?\n\nOptions:\nA. b => b tru fls\nB. fls\nC. b => b fls tru\nD. b => fls => tru\nE. b => b fls fls\n", "answer": "Explanations: To implement the `not` function for a Church boolean `b`, we need to return the opposite Church boolean value.\n\nIf `b` is `tru`, then `not tru` should return `fls`.\nIf `b` is `fls`, then `not fls` should return `tru`.\n\nTherefore, the implementation of the `not` function would be:\n\n```plaintext\nnot = b => b(fls)(tru)\n```\n\nThis function takes a Church boolean `b` and returns the opposite Church boolean value. ///Answer: D"}
{"subject": "", "question": "Question: To which expression is the following for-loop translated? for x <- xs if x > 5; y <- ys yield x + y?\n\nOptions:\nA. xs.flatMap(x => ys.map(y => x + y)).withFilter(x => x > 5)\nB. xs.withFilter(x => x > 5).map(x => ys.flatMap(y => x + y))\nC. xs.withFilter(x => x > 5).flatMap(x => ys.map(y => x + y))\nD. xs.map(x => ys.flatMap(y => x + y)).withFilter(x => x > 5)\n", "answer": "Explanations: {\n\u00a0 \"explaination\": \"The correct translation of the given for-loop is xs.withFilter(x => x > 5).flatMap(x => ys.map(y => x + y)).\",\n\u00a0 \"answer\": \"xs.withFilter(x => x > 5).flatMap(x => ys.map(y => x + y))\"\n} ///Answer: C"}
{"subject": "", "question": "Question: If we keep the same graphical description for the transducer, but change the \"padding convention\" by complementing strings by empty characters only on the left when necessary, would the new transducer recognize the same string associations?\n\nOptions:\nA. Yes\nB. No\n", "answer": "Explanations: When we change the padding convention by complementing strings with empty characters only on the left, it affects how the strings are processed by the transducer. Let's analyze the implications of this change step by step:\n\n1. **Original Transducer**: In the original transducer, the padding convention adds empty characters to the right of the input strings. This means that the input strings are effectively shifted to the left, maintaining the relative positions of the characters in the strings.\n\n2. **Effects of Changing Padding Convention**: When we change the padding convention to add empty characters only on the left, the input strings are shifted to the right. This results in a different alignment of characters within the strings compared to the original transducer.\n\n3. **Recognition of String Associations**: The recognition of string associations by a transducer depends on the alignment of characters in the input strings. Changing the padding convention alters this alignment, potentially leading to different string associations being recognized by the transducer.\n\n4. **Conclusion**: Based on the above analysis, the new transducer with the changed padding convention would not recognize the same string associations as the original transducer. The shift in alignment caused by padding the strings on the left would result in different mappings and associations being identified by the transducer.\n\nTherefore, the correct answer to the question is:\n\nB) No\n\nThis answer is supported by a logical and comprehensive explanation of how the change in padding convention affects the recognition of string associations by the transducer. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following is true regarding the random forest classification algorithm?\n\nOptions:\nA. It is not suitable for parallelization.\nB. It uses only a subset of features for learning in each weak learner.\nC. We compute a prediction by randomly selecting the decision of one weak learner.\nD. It produces a human interpretable model.\n", "answer": "Explanations: Answer: B) It uses only a subset of features for learning in each weak learner.\n\nExplanation: \nA) It is not suitable for parallelization - This statement is false. Random forest classification algorithm is suitable for parallelization as the individual decision trees can be trained independently of each other.\n\nB) It uses only a subset of features for learning in each weak learner - This statement is true. In random forest algorithm, each decision tree is trained on a random subset of features, which helps in reducing overfitting and improving the model's generalization.\n\nC) We compute a prediction by randomly selecting the decision of one weak learner - This statement is false. In random forest algorithm, the final prediction is made by aggregating the predictions of all the decision trees in the forest through voting or averaging.\n\nD) It produces a human interpretable model - This statement is false. Random forest classification algorithm typically does not produce a human interpretable model due to the ensemble nature of multiple decision trees. ///Answer: B"}
{"subject": "", "question": "Question: For logistic regression, what is the best encoding for the feature ``span'' which is the length of the bridge in meters? (One answer!!!!!!)?\n\nOptions:\nA. round to full meters\nB. find appropriate bins and use one-hot encoding\nC. find appropriate bins and use polynomial encoding\nD. logistic regression does not require an encoding\n", "answer": "Explanations: The best encoding for the feature \"span,\" which represents the length of the bridge in meters, in logistic regression is:\n\n'find appropriate bins and use polynomial encoding'\n\nThis approach involves discretizing the continuous feature \"span\" into suitable bins or intervals and then applying polynomial encoding to capture potential non-linear relationships between the feature and the target variable. This method allows logistic regression to model more complex patterns in the data that may not be captured by a simple linear relationship. ///Answer: C"}
{"subject": "", "question": "Question: Let $X$ and $K$ be two independent random variables in a group $G$ with $n$ elements and where $K$ is uniformly distributed over $G$. Let $Y = X+K$. Then, for any $y\\in G$, $\\Pr[Y=y]=$\\dots?\n\nOptions:\nA. $1/n$.\nB. $\\Pr[X=y]$.\nC. $1/n^2$.\nD. $1/(n-1)$.\n", "answer": "Explanations: The correct answer is '$1/n$'.\n\nExplanation:\nGiven that $X$ and $K$ are independent random variables in a group $G$ with $n$ elements, and that $K$ is uniformly distributed over $G, we have $Y = X+K$.\n\nTo find $\\Pr[Y=y]$, we need to consider all possible values of $X$ and $K$ that sum up to $y$.\n\nSince $K$ is uniformly distributed over $G$ with $n$ elements, the probability of $K=k$ for any $k\\in G$ is $1/n$.\n\nTherefore, the probability of $Y=y$ can be expressed as the sum of probabilities of all pairs $(X,K)$ that satisfy $X+K=y$.\n\nSince $X$ and $K$ are independent, the probability of $X=x$ and $K=k$ occurring simultaneously is the product of their individual probabilities.\n\nHence, $\\Pr[Y=y] = \\sum_{k\\in G}\\Pr[X=y-k]\\Pr[K=k] = \\sum_{k\\in G}\\Pr[X=y-k]\\times \\frac{1}{n}$\n\nThis simplifies to $\\Pr[Y=y] = \\frac{1}{n}$, as the sum of probabilities of all possible $X$ values that satisfy $X+K=y$ is equal to $\\frac{1}{n}$ since there are $n$ elements in $G$.\n\nTherefore, the correct answer is '$1/n$'. ///Answer: A"}
{"subject": "", "question": "Question: The test loss of the 1-nearest neighbor classifier is always zero.\n\nOptions:\nA. TRUE\nB. FALSE\n", "answer": "Explanations: 1: \"FALSE\"\\n\\nExplanation:\\nThe test loss of the 1-nearest neighbor classifier is not always zero. While the 1-nearest neighbor classifier is known for its simplicity and ability to memorize the training data, it may not always perform perfectly on unseen test data. The classifier\\'s performance can be affected by noise in the data, outliers, or instances where multiple points have equal distance to the test point, leading to misclassifications and non-zero test loss. ///Answer: B"}
{"subject": "", "question": "Question: Consider two fully connected networks, A and B, with a constant width for all layers, inputs and outputs. Network A has depth $3L$ and width $H$, network B has depth $L$ and width $2H$. Everything else is identical for the two networks and both $L$ and $H$ are large. In this case, performing a single iteration of backpropagation requires fewer scalar multiplications for network A than for network B.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The correct answer is b) False.\n\nTo understand why this statement is false, we need to consider the total number of scalar multiplications required for a single iteration of backpropagation in each network. \n\nIn network A, with a depth of $3L$ and width of $H$, the total number of scalar multiplications required can be calculated as follows:\nTotal scalar multiplications for network A = $3L \\times H \\times H$\n\nIn network B, with a depth of $L$ and width of $2H$, the total number of scalar multiplications required can be calculated as follows:\nTotal scalar multiplications for network B = $L \\times 2H \\times 2H$\n\nSimplifying the above equations, we get:\nTotal scalar multiplications for network A = $3LH^2$\nTotal scalar multiplications for network B = $4LH^2$\n\nComparing the total scalar multiplications for both networks, we can see that network B actually requires fewer scalar multiplications for a single iteration of backpropagation compared to network A. Therefore, the statement that performing a single iteration of backpropagation requires fewer scalar multiplications for network A than for network B is false. ///Answer: B."}
{"subject": "", "question": "Question: Select \\emph{incorrect} statement. Brithday paradox?\n\nOptions:\nA. is a brute force technique.\nB. can be implemented with constant memory using Rho ($\\rho$) method.\nC. is used to recover the secret key of AES in $2^{64}$ computations.\nD. can be implemented using a table of size $\\Theta\\sqrt{N}$\n", "answer": "Explanations: The incorrect statement is: 'is used to recover the secret key of AES in $2^{64}$ computations.'\n\nJustification: The birthday paradox is not a technique used to recover the secret key of AES in $2^{64}$ computations. The birthday paradox is a probability theory concept that deals with the likelihood of shared birthdays in a group, and it is not directly related to AES key recovery. ///Answer: C"}
{"subject": "", "question": "Question: Given this program snippet which is part of a large (> 10000 LoC) codebase,\n    which of these statements are true, given that the contents of string \"s\"\n    are attacker controlled, the attacker can run the function f only once, the\n    attacker has access to the binary and the binary is compiled for x86\\_64 on\n    a modern Linux system?\n\n\\begin{lstlisting}[language=C,style=c]\n#include <string.h>\nvoid f(char* s) {\n    char b[100] = {0};\n    memcpy(b, s, strlen(s));\n    printf(\"\\%s\", b);\n}\n\\end{lstlisting}?\n\nOptions:\nA. If this program is compiled with no mitigations, an attacker can\n            gain remote code execution.\nB. If this program is compiled with DEP (Data-Execution Prevention)\n            and no other mitigation, an attacker can gain remote code execution.\nC. If this program is compiled with stack canaries and no other\n            mitigation, an attacker can leak the canary.\nD. If this program is compiled with stack canaries and no other\n            mitigation, an attacker can reliably gain remote code execution.\n", "answer": "Explanations: - If this program is compiled with no mitigations, an attacker can gain remote code execution.\n\nThis statement is true because the function f is vulnerable to a buffer overflow due to the use of memcpy without proper bounds checking. Since the contents of the string \"s\" are attacker controlled, the attacker can provide a string that is longer than the allocated buffer \"b\" (100 bytes) and overwrite the return address on the stack. By carefully crafting the input string, the attacker can redirect the control flow of the program to execute arbitrary code.\n\n- If this program is compiled with DEP (Data-Execution Prevention) and no other mitigation, an attacker can gain remote code execution.\n\nThis statement is true because DEP only prevents code from being executed on the stack or heap, but it does not prevent the attacker from redirecting the control flow to existing code in the binary (such as the printf function) or from using Return-Oriented Programming (ROP) techniques to chain together existing code snippets to execute arbitrary code.\n\n- If this program is compiled with stack canaries and no other mitigation, an attacker can leak the canary.\n\nThis statement is true because stack canaries are used to detect buffer overflows by placing a random value on the stack before the return address. If the canary value is modified, an error will be detected before the function returns. However, since the attacker can run the function f only once, they can leak the canary value by causing a buffer overflow and observing the program behavior (e.g., by causing a segmentation fault).\n\n- If this program is compiled with stack canaries and no other mitigation, an attacker can reliably gain remote code execution.\n\nThis statement is false because while the attacker can leak the canary value, they cannot reliably gain remote code execution without knowing the exact value of the canary. The canary value is randomized at runtime, making it difficult for the attacker to bypass this protection and successfully execute arbitrary code. ///Answer: A"}
{"subject": "", "question": "Question: Consider a public key cryptosystem. The channel used to transmit the public key has to be\\dots?\n\nOptions:\nA. \\dots encrypted.\nB. \\dots authenticated.\nC. \\dots confidential.\nD. \\dots authenticated and confidential.\n", "answer": "Explanations: Answer: authenticated and confidential.\nRationale: When transmitting the public key in a public key cryptosystem, it is crucial that the channel used ensures both authentication and confidentiality. Authentication ensures that the public key being received is indeed from the intended sender and has not been tampered with during transmission. Confidentiality ensures that the public key remains secure and cannot be intercepted or accessed by unauthorized parties, preserving the integrity of the encryption process. By using a channel that is both authenticated and confidential, the security and trustworthiness of the public key exchange are maintained, making it a fundamental requirement for secure communication in a public key cryptosystem. ///Answer: D"}
{"subject": "", "question": "Question: Let $\\mathcal{C}$ be a binary $(6,3)$ linear code containing the codewords $\\mathbf{x}_1 = 011011$, $\\mathbf{x}_2 = 101101$ and $\\mathbf{x}_3 = 111000$. True or false: The minimum distance of the code is $d_{\\min} = 3$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False\n\nThe minimum distance of a code is defined as the minimum number of positions in which any two distinct codewords differ. To find the minimum distance of the code, we need to find the minimum Hamming distance between all pairs of codewords.\n\nLet's calculate the Hamming distances between the given codewords:\n1. $\\mathbf{x}_1$ and $\\mathbf{x}_2$: $d(\\mathbf{x}_1, \\mathbf{x}_2) = 4$ (011011 vs 101101)\n2. $\\mathbf{x}_1$ and $\\mathbf{x}_3$: $d(\\mathbf{x}_1, \\mathbf{x}_3) = 4$ (011011 vs 111000)\n3. $\\mathbf{x}_2$ and $\\mathbf{x}_3$: $d(\\mathbf{x}_2, \\mathbf{x}_3) = 5$ (101101 vs 111000)\n\nThe minimum distance of the code is the smallest of these distances, which is $d_{\\min} = 4$. Therefore, the correct answer is False, the minimum distance of the code is $d_{\\min} = 4$. ///Answer: B"}
{"subject": "", "question": "Question: Suppose that in a given FP Tree, an item in a leaf node N exists in every path. Which of the following is true?\n\nOptions:\nA. N co-occurs with its prefixes in every transaction\nB. {N}\u2019s minimum possible support is equal to the number of paths\nC. For every node P that is a parent of N in the FP tree, confidence(P->N) = 1\nD. The item N exists in every candidate set\n", "answer": "Explanations: The correct answer is: {N}\u2019s minimum possible support is equal to the number of paths.\n\nExplanation:\n- If an item N exists in every path in an FP tree, it means that the frequency of N is equal to the number of paths in the FP tree.\n- The support of an itemset is the number of transactions containing that itemset. In this case, since N exists in every path, the minimum support of {N} is equal to the number of paths containing N in the FP tree.\n- This is because the support of an itemset is the number of transactions that contain that itemset, and in this case, the paths in the FP tree represent transactions. ///Answer: B"}
{"subject": "", "question": "Question: You are given the task to select the size of the subgroup order for a discrete logarithm based scheme. Which value would you recommend taking in account both security and efficiency?\n\nOptions:\nA. 80 bits\nB. 160 bits\nC. 1024 bits\nD. 2048 bits\n", "answer": "Explanations: When selecting the size of the subgroup order for a discrete logarithm based scheme, it is important to consider both security and efficiency. \n\n1. **Security**: The size of the subgroup order directly impacts the security of the scheme. A larger subgroup order provides higher security against attacks such as brute force or discrete logarithm calculations.\n\n2. **Efficiency**: A larger subgroup order typically requires more computational resources and time for cryptographic operations. Therefore, choosing a balance between security and efficiency is crucial.\n\nGiven the options provided:\n\n- **80 bits**: This size is considered too small for modern cryptographic standards and may not provide sufficient security against current attacks.\n  \n- **160 bits**: While 160 bits used to be considered secure for certain applications, it is now generally recommended to use larger key sizes for better security.\n\n- **1024 bits**: This size is commonly used for RSA encryption, but for discrete logarithm based schemes, it is considered to be on the lower end in terms of security for modern applications.\n\n- **2048 bits**: This size is currently considered a standard for many cryptographic applications, providing a good balance between security and efficiency. It offers a high level of security against current attacks while still being efficient enough for practical use.\n\nTherefore, considering both security and efficiency, the recommended value for the subgroup order size would be **2048 bits**. ///Answer: D"}
{"subject": "", "question": "Question: Given the following lemmas, holding for all types A, x: A, b1: Bool, b2: Bool, p: A => Bool, xs: List[A] and ys: List[A]: (ForallNil) nil.forall(p) === True (ForallCons) (x :: xs).forall(p) === p(x) && xs.forall(p) (ExistsNil) nil.exists(p) === False (ExistsCons) (x :: xs).exists(p) === p(x) || xs.exists(p) (NegFalse) !False === True (NegOr) !(b1 || b2) === !b1 && !b2 (NegAnd) !(b1 && b2) === !b1 || !b2 (NegInvolutive) !!b1 === b1 Let us prove the following lemma for all l: List[A] and all p: A => Bool: (ListNegExists) !l.exists(x => !p(x)) === l.forall(p) We prove it by induction on l. Base case: l is Nil. Therefore, we need to prove: !Nil.exists(x => !p(x)) === Nil.forall(p) Starting from the left hand-side (!Nil.exists(x => !p(x))), what exact sequence of lemmas should we apply to get the right hand-side (Nil.forall(p))?\n\nOptions:\nA. NegInvolutive, ForallNil, ExistsNil\nB. ForallNil, NegFalse, ExistsNil\nC. NegFalse, ExistsNil, ForallNil\nD. NegFalse, ForallNil, ExistsNil\nE. ExistsNil, NegInvolutive, ForallNil\nF. ExistsNil, NegFalse, ForallNil\nG. ForallNil, NegInvolutive, ExistsNil\nH. NegInvolutive, ExistsNil, ForallNil\n", "answer": "Explanations: To prove !Nil.exists(x => !p(x)) === Nil.forall(p) starting from the left-hand side, you would apply the lemmas in the following sequence: NegFalse, ExistsNil, ForallNil. This sequence allows you to use the NegFalse lemma to simplify the expression, then apply ExistsNil to the resulting expression, and finally use ForallNil to arrive at Nil.forall(p) on the right-hand side. ///Answer: C."}
{"subject": "", "question": "Question: For $K$ a field, $a,b\\in K$ with $4a^3+27b^2 \\neq 0$, $E_{a,b}(K)$ is?\n\nOptions:\nA. a field.\nB. a group.\nC. a ring.\nD. a ciphertext.\n", "answer": "Explanations: The notation $E_{a,b}(K)$ typically refers to an elliptic curve defined over the field $K$ with parameters $a$ and $b$. In this case, $K$ is a field and $a,b \\in K$ with $4a^3 + 27b^2 \\neq 0$ ensures that the elliptic curve equation is non-singular.\n\nGiven that an elliptic curve over a field forms a group under a geometrically defined addition operation, the best option among the given choices is: a group. ///Answer: B"}
{"subject": "", "question": "Question: Which one of these is \\emph{not} a skeleton used to design block ciphers?\n\nOptions:\nA. a Feistel scheme\nB. a Subsitution-permutation network\nC. a Merkle-Damg{\\aa}rd construction\nD. a Lay-Massey Scheme\n", "answer": "Explanations: The Lay-Massey Scheme is \\emph{not} a skeleton used to design block ciphers. \n\n1. \\textbf{Feistel scheme}: A Feistel network is a structure used in the construction of block ciphers. It divides the block into two halves and processes one half through a round function that depends on the other half. This scheme is used in many famous block ciphers like DES, Triple DES, and Blowfish.\n\n2. \\textbf{Substitution-permutation network (SPN)}: An SPN is another common structure for designing block ciphers. It consists of multiple rounds where a substitution layer (S-box) is applied followed by a permutation layer (P-box). This structure is used in ciphers like AES.\n\n3. \\textbf{Merkle-Damg{\\aa}rd construction}: The Merkle-Damg{\\aa}rd construction is a method used for constructing collision-resistant cryptographic hash functions. It is not a skeleton used in block cipher design but rather in the design of hash functions like SHA-1 and SHA-256.\n\n4. \\textbf{Lay-Massey Scheme}: The Lay-Massey Scheme is not a commonly known or used skeleton in the design of block ciphers. It is not a standard term in the context of block cipher design. ///Answer: D"}
{"subject": "", "question": "Question: Let $n \\geq 2$ be a positive integer, and $M$ a uniformly distributed binary message of length $2n$. Let $P_K(M)$ denote the one-time pad encryption of $M$ with key $K$. Let $K_1$ be a uniformly distributed binary key length $n$. Let $K_2$ be the complement of $K_1$. Let $K_3$ be the reverse of $K_1$. Let $K_i||K_j$ denote the concatenation of the two keys. True or false: Encryption with the key $K_6 = ( K_1|| K_3 ) $, $P_{K_6}(M)$ provides perfect secrecy.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: To reason through this question, we need to understand what perfect secrecy means in the context of encryption. Perfect secrecy means that even with unlimited computational power, an attacker cannot determine any information about the message based on the encrypted message.\n\n1. First, let's understand how the one-time pad encryption works. In the one-time pad encryption, the message is XORed with the key to produce the encrypted message. The decryption is done by XORing the encrypted message with the same key.\n\n2. Now, let's analyze the key $K_6 = (K_1 || K_3)$. $K_1$ is a uniformly distributed binary key of length $n$, and $K_3$ is the reverse of $K_1$. When concatenated, $K_6$ becomes a key of length $2n$.\n\n3. To determine if the encryption with the key $K_6$ provides perfect secrecy, we need to consider the properties of the key. \n\n4. Let's consider the case where the key $K_6$ is used to encrypt a message $M$. Since $K_3$ is the reverse of $K_1$, the XOR operation with $K_6$ would effectively cancel out each other's effects. This means that the encryption with $K_6$ essentially becomes the same as using no key at all.\n\n5. Therefore, since using no key at all does not provide perfect secrecy, the encryption with the key $K_6 = (K_1 || K_3)$ would also not provide perfect secrecy.\n\n6. Based on this reasoning, the correct answer is Option A: False. Encryption with the key $K_6 = (K_1 || K_3)$ does not provide perfect secrecy. ///Answer: A"}
{"subject": "", "question": "Question: A retrieval model attempts to capture?\n\nOptions:\nA. the interface by which a user is accessing information\nB. the importance a user gives to a piece of information for a query\nC. the formal correctness of a query formulation by user\nD. the structure by which a document is organised\n", "answer": "Explanations: \n\nFirst, let's understand what a retrieval model is in the context of Information Retrieval. A retrieval model is a framework for presenting information to users based on their information needs. It aims to capture the relevance of a document to a user query.\n\nNow, let's analyze each option:\n\n1. the interface by which a user is accessing information: This option refers to the way a user interacts with the system to access information. While the interface is important for user experience, it is not the main focus of a retrieval model.\n\n2. the importance a user gives to a piece of information for a query: This option aligns with the purpose of a retrieval model. The model aims to capture the relevance and importance of a document to a user query, so this is a relevant choice.\n\n3. the formal correctness of a query formulation by user: This option focuses on the formal structure and correctness of a user's query. While query formulation is important in information retrieval, it is not the main goal of a retrieval model.\n\n4. the structure by which a document is organized: This option refers to the organization of documents, which is more related to document management than retrieval models.\n\nBased on the analysis, the correct answer is:\n- the importance a user gives to a piece of information for a query\n\nThis option aligns with the main goal of a retrieval model, which is to capture the relevance and importance of a document to a user query. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements about mitigations are correct?\n\nOptions:\nA. Control-Flow Integrity can efficiently protect the forward edge\n            but, when using target sets, is limited on the backward edge\nB. Code-Pointer Integrity (specifically the implementation\n            described in the slides) uses a separate stack to protect code\n            pointers.\nC. Shadow stacks can be implemented in software with zero\n            overhead.\nD. Safe stacks protect against corruption of all data on the\n            stack.\n", "answer": "Explanations: Control-Flow Integrity can efficiently protect the forward edge but may have limitations on the backward edge when using target sets. This mitigation technique focuses on maintaining the integrity of the control flow graph to prevent attackers from diverting the program's execution path. On the other hand, Code-Pointer Integrity, as described in the slides, utilizes a separate stack to safeguard code pointers. This approach adds an extra layer of protection against memory corruption attacks targeting code pointers. Shadow stacks, although effective in enhancing security, typically incur some overhead when implemented in software. These stacks help in detecting stack-based buffer overflows and preventing attackers from tampering with return addresses. Safe stacks, on the other hand, primarily protect against corruption of return addresses and other sensitive data on the stack, providing an additional layer of defense against stack-based attacks. ///Answer: A"}
{"subject": "", "question": "Question: What is the gradient of $\\boldsymbol{x}^{\\top} \\boldsymbol{W}^{\\top} \\boldsymbol{W} \\boldsymbol{x}$ with respect to $\\boldsymbol{x}$ (written as a vector)?\n\nOptions:\nA. $2 \\boldsymbol{W}^{\\top} \\boldsymbol{x}$\nB. $2 \\boldsymbol{W}^{\\top} \\boldsymbol{W} \\boldsymbol{x}$\nC. $2 \\boldsymbol{W} \\boldsymbol{W}^{\\top} \\boldsymbol{x}$\nD. $2 \\boldsymbol{W}$\nE. $2 \\boldsymbol{W} \\boldsymbol{x}$\n", "answer": "Explanations: First, we can expand the expression $\\boldsymbol{x}^{\\top} \\boldsymbol{W}^{\\top} \\boldsymbol{W} \\boldsymbol{x}$ using matrix multiplication rules. \n\n$\\boldsymbol{x}^{\\top} \\boldsymbol{W}^{\\top} \\boldsymbol{W} \\boldsymbol{x} = \\boldsymbol{x}^{\\top} (\\boldsymbol{W}^{\\top} \\boldsymbol{W}) \\boldsymbol{x}$\n\nNext, we can differentiate the expression with respect to $\\boldsymbol{x}$ to find the gradient. \n\nDifferentiating $\\boldsymbol{x}^{\\top} (\\boldsymbol{W}^{\\top} \\boldsymbol{W}) \\boldsymbol{x}$ with respect to $\\boldsymbol{x}$ gives us:\n\n$2(\\boldsymbol{W}^{\\top} \\boldsymbol{W}) \\boldsymbol{x}$\n\nTherefore, the correct answer is the second option: $2 \\boldsymbol{W}^{\\top} \\boldsymbol{W} \\boldsymbol{x}$. \n\nThis result comes from the application of the chain rule in matrix calculus and the properties of matrix transposition. ///Answer: B"}
{"subject": "", "question": "Question: Let $X$ be a random variable distributed over the alphabet $\\mathcal{X}=\\{0,1,\\ldots,n\\}$. Assume also that there exist $x_1,x_2\\in\\mathcal{X}$ such that $p_X(x_1)\neq p_X(x_2)$ (\textit{i.e.}, $X$ is not uniformly distributed over $\\mathcal{X}$). Let $Y=2^X$ and $Z=\\lfloor X/2 \rfloor$.  True or false: $H(Y|X)=H(Z|X)$?\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The conditional entropy $H(Y|X)$ is defined as $H(Y|X) = - \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)}$, where the sum is taken over all possible values of $x$ and $y$.\n\nFor $Y=2^X$, since $Y$ is a deterministic function of $X$, knowing the value of $X$ fully determines the value of $Y$. Therefore, $H(Y|X) = 0$.\n\nFor $Z = \\lfloor X/2 \\rfloor$, there are multiple possible values of $X$ that map to the same value of $Z$ due to the floor function. This means that knowing the value of $X$ only partially determines the value of $Z$, leading to a non-zero conditional entropy $H(Z|X)$.\n\nSince $H(Y|X) = 0$ and $H(Z|X) > 0$, the statement that $H(Y|X) = H(Z|X)$ is false.\n\nThus, the correct answer is:\n\n2. False ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements is correct?\n\nOptions:\nA. (a) A neural net with one hidden layer and an arbitrary number of hidden nodes with sigmoid activation functions can approximate any \"suffiently smooth\" function.\nB. (b) A neural net with one hidden layer and an arbitrary number of hidden nodes with sigmoid activation functions can approximate any \"suffiently smooth\" function on a bounded domain.\nC. (c) On a bounded domain, neural nets can approximate any \"sufficiently smooth\" function \"in average\" but not \"pointwise\".\n", "answer": "Explanations: First, let's break down each statement:\n\n(a) A neural net with one hidden layer and an arbitrary number of hidden nodes with sigmoid activation functions can approximate any \"sufficiently smooth\" function.\nThis statement is known as the Universal Approximation Theorem, which states that a neural network with one hidden layer and non-linear activation functions can approximate any continuous function to any desired degree of accuracy. The key here is that the function must be \"sufficiently smooth\".\n\n(b) A neural net with one hidden layer and an arbitrary number of hidden nodes with sigmoid activation functions can approximate any \"sufficiently smooth\" function on a bounded domain.\nThis statement is a slight modification of the Universal Approximation Theorem, but with the added condition that the function must be defined on a bounded domain. This restriction is important because it limits the complexity of the function being approximated.\n\n(c) On a bounded domain, neural nets can approximate any \"sufficiently smooth\" function \"in average\" but not \"pointwise\".\nThis statement suggests that while neural networks can approximate \"sufficiently smooth\" functions on a bounded domain in an average sense, they may not be able to do so at every point in the domain. This highlights a potential limitation of neural networks in terms of pointwise approximation.\n\nIn conclusion, based on the three statements provided, option (b) is the correct one. This is because it captures the essence of the Universal Approximation Theorem while also acknowledging the importance of the bounded domain constraint. ///Answer: B"}
{"subject": "", "question": "Question: For his awesome research, Tugrulcan is going to use the Pagerank with teleportation and HITS algorithm, not on a network of webpages but on the retweet network of Twitter! The retweet network is a directed graph, where nodes are users and an edge going out from a user A and to a user B means that \"User A retweeted User B\". Which one is FALSE about a Twitter bot that retweeted other users frequently but got never retweeted by other users or by itself?\n\nOptions:\nA. It will have a non-zero hub value.\nB. It will have an authority value of zero.\nC. It will have a pagerank of zero.\nD. Its authority value will be equal to the hub value of a user who never retweets other users.\n", "answer": "Explanations: The HITS algorithm calculates two values for each node in a directed graph: the hub value and the authority value. \n\nIn the context of the retweet network of Twitter, the hub value of a node represents how well-connected the user is to other users who are frequently retweeted, while the authority value represents how frequently a user is retweeted by other users.\n\nIf a Twitter bot frequently retweets other users but never gets retweeted by other users or by itself, it means that it has a high hub value because it is connected to other users who are frequently retweeted. However, it will have an authority value of zero because it is not being retweeted by other users.\n\nTherefore, statements 1, 2, and 3 are true, but statement 4 is false. The authority value of the Twitter bot will not be equal to the hub value of a user who never retweets other users because the Twitter bot is not being retweeted by other users, while the user who never retweets other users may still be retweeted by others. ///Answer: D"}
{"subject": "", "question": "Question: In which case is the DDH problem believed to be hard?\n\nOptions:\nA. In $\\mathbb{Z}_p^*$.\nB. In $\\mathbb{Z}_n$, where $n = pq$ for two large primes $p$ and $q$.\nC. Given an oracle that returns the least significant bit of $g^{xy}$.\nD. In a large subgroup of prime order of a regular elliptic curve.\n", "answer": "Explanations: First, let's review what the Decisional Diffie-Hellman (DDH) problem is. In the DDH problem, given a group $G$, a generator $g$ of $G$, and elements $g^a$, $g^b$, and $g^c$ in $G$, the task is to determine whether $c = ab$ or not.\n\nNow let's analyze each option:\n\n1. In $\\mathbb{Z}_p^*$:\nThe DDH problem is not hard in $\\mathbb{Z}_p^*$ because it is equivalent to the Discrete Logarithm Problem, which is efficiently solvable using algorithms like the baby-step giant-step algorithm or Pollard's rho algorithm.\n\n2. In $\\mathbb{Z}_n$, where $n = pq$ for two large primes $p$ and $q$:\nThis option is related to the RSA cryptosystem, where the DDH problem is believed to be hard due to the difficulty of factoring large composite numbers into their prime factors. Therefore, the DDH problem is believed to be hard in this case.\n\n3. Given an oracle that returns the least significant bit of $g^{xy}$:\nThis option does not directly relate to the DDH problem. It seems to be a different problem involving querying an oracle for the least significant bit of a specific value, which does not align with the definition of the DDH problem.\n\n4. In a large subgroup of prime order of a regular elliptic curve:\nThe DDH problem is believed to be hard in elliptic curve groups, including large subgroups of prime order. This belief is based on the Elliptic Curve Discrete Logarithm Problem, which is difficult to solve efficiently due to the elliptic curve structure.\n\nTherefore, the case where the DDH problem is believed to be hard is option 2: In $\\mathbb{Z}_n$, where $n = pq$ for two large primes $p$ and $q$. ///Answer: B"}
{"subject": "", "question": "Question: Consider a linear model $\\hat{y} = xv ^\top \\wv$ with the squared loss under an $\\ell_\\infty$-bounded adversarial perturbation. For a single point $(xv, y)$, it corresponds to the following objective:\n                    \begin{align}\n                        \\max_{\tilde{xv}:\\ \\|xv-\tilde{xv}\\|_\\infty\\leq \\epsilon} \\left(y - \tilde{xv} ^\top \\wv \right)^{2},\n                        \tag{OP}\\AMClabel{eq:opt_adv_regression}\n                    \\end{align}\n                    where $\\|xv-\tilde{xv}\\|_\\infty\\leq \\epsilon$ denotes the $\\ell_\\infty$-norm, i.e.  $|x_i - \tilde{x}_i| \\leq \u000barepsilon$ for every $i$.\n                    \\\n                    Assume that $\\wv = (3, -2)^\top$, $xv = (-1, 2)^\top$, $y=2$. What is the optimal $\tilde{xv}^\\star$ that maximizes the objective in Eq.~(\\AMCref{eq:opt_adv_regression})?\n\t                ?\n\nOptions:\nA. $(-1-\u000barepsilon, 2-\u000barepsilon)^\top$\nB. $(-1-\u000barepsilon, 2)^\top$\nC. $(-1+\u000barepsilon, 2)^\top$\nD. $(-1+\u000barepsilon, 2+\u000barepsilon)^\top$\nE. Other\n", "answer": "Explanations: To find the optimal $\tilde{xv}^\\star$ that maximizes the objective in Eq.~(\\ref{eq:opt_adv_regression}), we need to consider the squared loss under an $\\ell_\\infty$-bounded adversarial perturbation. Let's go step by step to solve this problem:\n\nGiven:\n- $\\wv = (3, -2)^T$\n- $xv = (-1, 2)^T$\n- $y = 2$\n- We want to find $\tilde{xv}^\\star = (x_1, x_2)^T$ that maximizes the objective in Eq.~(\\ref{eq:opt_adv_regression}) with the constraint $\\|xv-\tilde{xv}\\|_\\infty \\leq \\epsilon$\n\nSubstitute the given values into the objective function:\n\\begin{align}\n\\max_{\tilde{xv}:\\ \\|xv-\tilde{xv}\\|_\\infty\\leq \\epsilon} (2 - (x_1 \\cdot 3 + x_2 \\cdot (-2)))^{2}\n\\end{align}\n\nSince we are looking for the maximum value, we want to minimize the term inside the square. This means we want to find $\tilde{xv}$ such that $x_1 \\cdot 3 + x_2 \\cdot (-2)$ is maximized. \n\nGiven that $xv = (-1, 2)^T$, the original value of $x_1 \\cdot 3 + x_2 \\cdot (-2)$ is $-3 + 4 = 1$. We want to move this value as far as possible in the negative direction within the $\\ell_\\infty$-norm constraint.\n\nThe constraint $\\|xv-\tilde{xv}\\|_\\infty \\leq \\epsilon$ implies that $|x_1 - \tilde{x}_1| \\leq \\epsilon$ and $|x_2 - \tilde{x}_2| \\leq \\epsilon$. \n\nTo maximize $x_1 \\cdot 3 + x_2 \\cdot (-2)$, we want to decrease $x_1$ and increase $x_2$ as much as possible. This leads to the option $(-1-\u000barepsilon, 2+\u000barepsilon)^T$ as it moves the original values in the direction that maximizes the objective while staying within the $\\ell_\\infty$-norm constraint.\n\nTherefore, the optimal $\tilde{xv}^\\star$ that maximizes the objective in Eq.~(\\ref{eq:opt_adv_regression}) is $(-1-\u000barepsilon, 2+\u000barepsilon)^T$. The correct option is \"Other\". ///Answer: E"}
{"subject": "", "question": "Question: Which of these primitives can be surely considered insecure today?\n\nOptions:\nA. Symmetric key encryption with an $82$ bit key.\nB. RSA encryption scheme with a $1613$ bit modulus.\nC. A signature scheme based on a random elliptic curve over a finite field $\\mathrm{GF}(q)$ of cardinality $q > 2^{200}$.\nD. Diffie-Hellman key exchange over a prime-order subgroup of $\\mathbb{Z}_p^*$ that has order $\\approx 2^{200}$ with a prime $p$ of $512$ bits.\n", "answer": "Explanations: The primitive that can be considered insecure today is 'Symmetric key encryption with an 82 bit key.' \n\nSymmetric key encryption with an 82 bit key is considered insecure today because of advancements in technology and computing power. An 82-bit key length is no longer considered sufficient to protect against brute force attacks, where an attacker tries all possible combinations of the key to decrypt the data. With current computing power, it is feasible to crack an 82-bit key in a reasonable amount of time.\n\nOn the other hand, the other options listed - RSA encryption scheme with a 1613 bit modulus, a signature scheme based on a random elliptic curve over a finite field GF(q) of cardinality q > 2^200, and Diffie-Hellman key exchange over a prime-order subgroup of Z_p* that has order \u2248 2^200 with a prime p of 512 bits - all have key lengths that are considered secure against current attacks. These primitives use key lengths that offer sufficient security against brute force attacks with current technology. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is/are true about testing?\n\nOptions:\nA. Adequate code coverage is crucial for dynamic testing.\nB. False positives matter in static analyses.\nC. Symbolic execution is a technique of whitebox dynamic testing.\nD. Tests are sufficient to prove that a program is bug-free.\n", "answer": "Explanations: Let's analyze each statement one by one:\n\n1 - Adequate code coverage is crucial for dynamic testing.\n- This statement is true. Code coverage measures the percentage of code that is executed by the tests. Adequate code coverage is necessary to ensure that the tests are thorough and can detect potential issues in the code.\n\n2 - False positives matter in static analyses.\n- This statement is true. False positives in static analysis tools can lead to wasted time and effort in investigating non-issues. It is important to minimize false positives to make the static analysis process more efficient.\n\n3 - Symbolic execution is a technique of whitebox dynamic testing.\n- This statement is incorrect. Symbolic execution is a technique used in whitebox testing, but it is not considered dynamic testing. Whitebox testing involves analyzing the internal structure of the software, while dynamic testing involves executing the software.\n\n4 - Tests are sufficient to prove that a program is bug-free.\n- This statement is false. While tests are a crucial part of the software testing process, they cannot guarantee that a program is completely bug-free. Testing can help identify and reduce the number of bugs in a program, but it is not possible to prove the absence of all bugs through testing alone.\n\nIn conclusion, statements 1 and 2 are true, while statements 3 and 4 are false. ///Answer: A"}
{"subject": "", "question": "Question: Principle Component Analysis (PCA) is a technique for...\n\nOptions:\nA. dimensionality reduction\nB. data augmentation\nC. feature extraction\nD. variance normalization\n", "answer": "Explanations: PCA is a technique used for dimensionality reduction. \n\nExplanation for each option:\n1) Dimensionality reduction: Correct. PCA helps in reducing the number of dimensions in a dataset while preserving the most important information.\n2) Data augmentation: Incorrect. Data augmentation involves creating new training examples through various techniques like rotation, flipping, etc. PCA does not augment the data.\n3) Feature extraction: Partially correct. PCA can be used for feature extraction by transforming the data into a new set of features that are linear combinations of the original features.\n4) Variance normalization: Incorrect. PCA does involve normalizing the data, but it is not specifically for variance normalization. It aims to decorrelate the features and find the directions (principal components) along which the data varies the most.\n\nTherefore, the correct option is 1) dimensionality reduction. ///Answer: A"}
{"subject": "", "question": "Question: If we run the Apriori algorithm on the following transaction database with minimal support count of 2, which of the itemsets will have a support count of 3?Transaction IDItemsT1{1,3,4}T2{2,3,5}T3{1,2,3,5}T4{2,5}T5{1,2,3,5}?\n\nOptions:\nA. {1,5}\nB. {2,3}\nC. {2,5}\nD. {1,2}\n", "answer": "Explanations: To find the itemsets with a support count of 3, we first need to identify the frequent itemsets in the database based on the minimum support count of 2. \n\n1. First iteration:\n   - Count the support for each individual item:\n     - {1}: 3\n     - {2}: 4\n     - {3}: 4\n     - {4}: 1\n     - {5}: 4\n   - Prune items with support less than 2: {4}\n   \n2. Second iteration:\n   - Generate candidate pairs from frequent items in the previous step: {1,2}, {1,3}, {1,5}, {2,3}, {2,5}, {3,5}\n   - Count the support for each candidate pair:\n     - {1,2}: 2\n     - {1,3}: 2\n     - {1,5}: 2\n     - {2,3}: 3\n     - {2,5}: 3\n     - {3,5}: 3\n   - Prune pairs with support less than 2: {1,2}, {1,3}, {1,5}\n   \n3. Third iteration:\n   - Generate candidate triples from frequent pairs in the previous step: {1,2,3}, {1,2,5}, {1,3,5}, {2,3,5}\n   - Count the support for each candidate triple:\n     - {1,2,5}: 2\n     - Prune triples with support less than 2: None\n   \nTherefore, the correct answer among the options provided is:\n- {1,2} ///Answer: D"}
{"subject": "", "question": "Question: Fill the missing line of code: (one answer)\\\\\n \\hspace*{.5cm} \\#code missing\\\\\n \\hspace*{.5cm} np.mean(np.random.randn(1000))\\\\?\n\nOptions:\nA. import np\nB. import numpy\nC. import numpy as np\nD. import np.mean\\\\\n\t\timport np.random\n", "answer": "Explanations: The question is asking to provide the correct line of code that should be inserted to import the necessary library before calling the `np.mean(np.random.randn(1000))` function. \n\nAmong the given options:\n1. import np - This is not a valid import statement.\n2. import numpy - This imports the numpy library but does not create an alias.\n3. import numpy as np - This is the correct way to import numpy and create an alias 'np' for it.\n4. import np.mean - This is not a valid import statement.\n5. import np.random - This imports only the random module from numpy, but does not alias numpy as np.\n\nTherefore, the correct option is: 3. import numpy as np The correct answer is: 3. import numpy as np ///Answer: C"}
{"subject": "", "question": "Question: With negative sampling a set of negative samples is created for\n?\n\nOptions:\nA. For each word of the vocabulary\nB. For each word-context pair\nC. For each occurrence of a word in the text\nD. For each occurrence of a word-context pair in the text\nE. \n", "answer": "Explanations: Correct answer: For each word-context pair\n\nExplanation: With negative sampling, a set of negative samples is created for each word-context pair. This means that for every word and its context pair in the text, a number of negative samples (words that are not the actual context) are generated to train the model on distinguishing between the actual context and randomly sampled noise. This technique helps improve training efficiency and performance in word embedding models. ///Answer: D"}
{"subject": "", "question": "Question: KEM \\dots?\n\nOptions:\nA. stands for Keyless Encryption Mechanism.\nB. is a Korean encryption mechanism.\nC. is a symmetric-key algorithm.\nD. is a public-key algorithm.\n", "answer": "Explanations: KEM stands for Keyless Encryption Mechanism. This term refers to a type of cryptographic algorithm used for public-key encryption. Unlike traditional public-key encryption, where a key pair is generated and one key is kept secret while the other is shared, KEM does not require a specific key for encryption. Instead, it generates a random value to serve as a shared secret key, which is used to encrypt the data. This approach adds an extra layer of security by avoiding the need to transmit or store sensitive keys. ///Answer: A"}
{"subject": "", "question": "Question: We consider a classification problem on linearly separable data. Our dataset had an outlier---a point that is very far from the other datapoints in distance (and also far from margins in SVM but still correctly classified by the SVM classifier).\n                    We trained the SVM, logistic regression and 1-nearest-neighbour models on this dataset.\n                    We tested trained models on a test set that comes from the same distribution as training set, but doesn't have any outlier points.\n                    For any vector $\u000bv \\in \\R^D$ let $\\|\u000bv\\|_2 := \\sqrt{v_1^2 + \\dots + v_D^2}$ denote the Euclidean norm. The hard-margin SVM problem for linearly separable points in $\\R^D$ is to minimize  the Euclidean norm $\\| \\wv \\|_2$ under some constraints.\n\t                What are the additional constraints for this optimization problem? ?\n\nOptions:\nA. $y_n \\ww^\top x_n \\geq 1 ~ \forall n \\in \\{1,\\cdots,N\\}$\nB. $\\ww^\top x_n \\geq 1 ~ \forall n \\in\\{1,\\cdots,N\\}$\nC. $y_n + \\ww^\top x_n \\geq 1 ~ \forall n \\in \\{1,\\cdots,N\\}$\nD. $\frac{y_n}{\\ww^\top x_n }\\geq 1 ~\forall n \\in \\{1,\\cdots,N\\}$\n", "answer": "Explanations: To determine the additional constraints for the optimization problem of the hard-margin SVM for linearly separable points in $\\mathbb{R}^D$, we need to consider the nature of the support vectors in the SVM.\n\nSupport vectors are the data points that lie on the margin or are misclassified. In the case of a linearly separable dataset, all data points should lie outside the margin with a margin of at least 1. This is to ensure that the decision boundary can be placed in such a way that there is a clear separation between the two classes.\n\nGiven this information, the correct additional constraint for the optimization problem is Option 1: $\\mathbf{w}^{T} \\mathbf{x}_n \\geq 1$ for all $n \\in \\{1,\\cdots,N\\}$. This constraint ensures that the decision boundary can separate the classes with a margin of at least 1.\n\nTherefore, the correct option is:\nOption 1: $\\mathbf{w}^{T} \\mathbf{x}_n \\geq 1$ for all $n \\in \\{1,\\cdots,N\\}$ ///Answer: A"}
{"subject": "", "question": "Question: The edit distance between \u201cpiece\u201d and \u201cpeace\u201d is(Penalty for wrong ticks)?\n\nOptions:\nA. 5\nB. 3\nC. 1, if considering insertion and deletion only\nD. 2, if considering insertion and deletion only\nE. 3, if considering insertion and deletion only\nF. 1, if considering insertion, deletion and substitution\nG. 2, if considering insertion, deletion and substitution\nH. 3, if considering insertion, deletion and substitution\nI. 1, if considering insertion, deletion, transposition and substitution\nJ. 2, if considering insertion, deletion, transposition and substitution\nK. 3, if considering insertion, deletion, transposition and substitution\n", "answer": "Explanations: \"p e a c e\" to \"p i e c e\":\nSubstitute \"a\" with \"i\" -> \"p e i c e\"\nTranspose \"e\" and \"i\" -> \"p i e c e\"\nThe distance is 2 if considering insertion, deletion, transposition, and substitution. ///Answer: G"}
{"subject": "", "question": "Question: Tick the \\textbf{incorrect} assumption.\n\nOptions:\nA. In GSM, the network chooses which encryption algorithm should be use.\nB. In GSM, the subscriber integrity key $K_i$ is securely stored in the SIM card.\nC. In GSM, the mobile phone has to authenticate to the network.\nD. In GSM, the network has to authenticate to the phone.\n", "answer": "Explanations: Certainly, let's approach this question step by step to identify the incorrect assumption:\n\n- In GSM, the network chooses which encryption algorithm should be used. (This is a valid statement as the network does determine the encryption algorithm.)\n- In GSM, the subscriber integrity key $K_i$ is securely stored in the SIM card. (This is also a valid statement as the subscriber integrity key is indeed stored securely in the SIM card.)\n- In GSM, the mobile phone has to authenticate to the network. (This is a valid statement as the mobile phone needs to authenticate to the network for secure communication.)\n- In GSM, the network has to authenticate to the phone. (This statement is incorrect as in GSM, the network does not authenticate to the phone; instead, the phone authenticates to the network.)\n\nTherefore, the incorrect assumption is: - In GSM, the network has to authenticate to the phone. ///Answer: D"}
{"subject": "", "question": "Question: Increasing the depth of a decision tree cannot increase its training error.\n\nOptions:\nA. TRUE\nB. FALSE\n", "answer": "Explanations: The statement \"Increasing the depth of a decision tree cannot increase its training error\" is **False**.\n\nHere's a detailed explanation:\n\n1. **Decision Trees**:\n   - Decision trees are a popular type of model used in machine learning for classification and regression tasks.\n   - They partition the feature space into regions and make predictions based on the majority class or average target value of the training instances in each region.\n\n2. **Training Error**:\n   - The training error of a machine learning model is the error or loss calculated on the training dataset.\n   - It measures how well the model fits the training data.\n\n3. **Effect of Increasing Depth**:\n   - Increasing the depth of a decision tree can lead to more complex and intricate decision boundaries.\n   - Deeper trees can capture more intricate patterns in the training data, potentially reducing bias and improving the model's ability to fit the training data.\n\n4. **Overfitting**:\n   - However, increasing the depth of a decision tree beyond a certain point can lead to overfitting.\n   - Overfitting occurs when a model learns the noise and random fluctuations in the training data, leading to poor generalization to unseen data.\n   - Deeper trees are more likely to overfit the training data by capturing noise and outliers, resulting in high variance and poor performance on new data.\n\n5. **Impact on Training Error**:\n   - As the depth of a decision tree increases, it is possible for the model to memorize the training data, resulting in a lower training error.\n   - However, this reduction in training error does not necessarily indicate better generalization performance.\n   - The model may perform well on the training data but poorly on unseen data due to overfitting.\n\n6. **Conclusion**:\n   - In summary, increasing the depth of a decision tree can lead to lower training error by capturing more complex patterns in the training data.\n   - However, this lower training error may be a sign of overfitting, where the model fails to generalize well to new data.\n   - Therefore, increasing the depth of a decision tree can increase its training error if it leads to overfitting.\n\nTherefore, the statement \"Increasing the depth of a decision tree cannot increase its training error\" is **False**. ///Answer: B"}
{"subject": "", "question": "Question: \t Consider a source $S$ with some distribution $P_S$ over the alphabet $\\mathcal{A} = \\{a, b, c, d, e, f\\}$. Consider the following encoding $\\Gamma$ over a code alphabet $\\mathcal{D}$ of size $D$ with the following codeword lengths:\t\t\begin{center}\t\t\t\begin{tabular}{ |c|c|c|c|c|c|c| } \t\t\t\t\\hline\t\t\t\t& $a$ & $b$ & $c$ & $d$ & $e$ & $f$ \\ \t\t\t\t\\hline\t\t\t\t$l(\\Gamma(\\cdot))$ & 1 & 1 & 1 & 2 & 2 & 4 \\ \t\t\t\t\\hline\t\t\t\\end{tabular}\t\t\\end{center}  True or false: If $D=4$ then $\\Gamma$ is necessarily prefix-free.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The question asks whether the given encoding $\\Gamma$ is necessarily prefix-free if the size of the code alphabet $\\mathcal{D}$ is 4. To answer this question, we need to understand the concept of prefix-free codes and how the codeword lengths in $\\Gamma$ affect this property.\n\n1. Define Prefix-Free Codes:\n- A prefix-free code is a type of uniquely decodable code in which no codeword is a prefix of any other codeword. This property ensures that a sequence of codewords can be uniquely decoded without any ambiguity.\n- In other words, no codeword in a prefix-free code can be a prefix of another codeword, which simplifies the decoding process.\n\n2. Analyze the Given Encoding $\\Gamma$:\n- The table provided gives the codeword lengths for each symbol in the alphabet $\\mathcal{A}$ under the encoding $\\Gamma$. For example, symbol $a$ has a codeword length of 1, symbol $b$ has a codeword length of 1, and so on.\n- If $D=4$, it means that there are 4 different codewords available in the code alphabet $\\mathcal{D}$.\n\n3. Determine if $\\Gamma$ is Prefix-Free:\n- To determine if $\\Gamma$ is necessarily prefix-free when $D=4$, we need to consider the codeword lengths provided in the table.\n- For $\\Gamma$ to be prefix-free, no codeword should be a prefix of any other codeword in the encoding. This is crucial for unambiguous decoding.\n- Looking at the codeword lengths in the table, we see that symbols $a$, $b$, and $c$ all have codeword lengths of 1, which means they are of equal length and can potentially overlap as prefixes.\n- If two symbols have the same codeword length, there is a possibility of one being a prefix of the other, violating the prefix-free property.\n\n4. Conclusion:\n- Based on the analysis of the given encoding $\\Gamma$ and the codeword lengths, we can conclude that $\\Gamma$ is not necessarily prefix-free when $D=4.\n- Therefore, the answer to the question is \"False.\"\n\nBy breaking down the problem, understanding the principles of prefix-free codes, and analyzing the specific encoding provided, we can confidently determine the correctness of the statement and provide a detailed explanation supported by Computer Science Theory. ///Answer: B"}
{"subject": "", "question": "Question: Given the distribution of positive and negative samples for attributes A1 and A2, which is the best attribute for splitting?\n\n|A1|P|N|\n|---|---|---|\n|a|7|0|\n|b|1|4|\n\n|A2|P|N|\n|--|--|--|\n|x|5|1|\n|y|3|3|?\n\nOptions:\nA. A1\nB. A2\nC. They are the same\nD. There is not enough information to answer the question\n", "answer": "Explanations: {\n    \"explaination\": \"To determine the best attribute for splitting, we need to calculate the information gain for each attribute. However, in this case, both attributes A1 and A2 have the same distribution of positive and negative samples. Therefore, there is not enough information to determine which attribute is the best for splitting.\",\n    \"answer\": \"There is not enough information to answer the question\"\n} ///Answer: D"}
{"subject": "", "question": "Question: Which statement is correct?\n\nOptions:\nA. The Viterbi algorithm works because words are independent in a sentence\nB. The Viterbi algorithm works because it is applied to an HMM model that makes an independence assumption on the word dependencies in sentences\nC. The Viterbi algorithm works because it makes an independence assumption on the word dependencies in sentences\nD. The Viterbi algorithm works because it is applied to an HMM model that captures independence of words in a sentence\n", "answer": "Explanations: The correct statement is: 'The Viterbi algorithm works because it is applied to an HMM model that makes an independence assumption on the word dependencies in sentences'. This is because the Viterbi algorithm is specifically designed to work with Hidden Markov Models (HMM), which assume that the observations (words) are dependent on the hidden states but are conditionally independent of each other given the hidden states. ///Answer: B"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. Bluetooth is?\n\nOptions:\nA. a short-range wireless technology.\nB. designed both for data and voice transmission.\nC. a standard for RFID tags.\nD. able to transmit 1Mbit/sec in 10m distance.\n", "answer": "Explanations: The incorrect statement is: 'able to transmit 1Mbit/sec in 10m distance.' While Bluetooth technology is known for its relatively short range, it typically has a maximum data transmission speed of around 3 Mbit/sec in close proximity. The statement suggesting 1Mbit/sec in 10m distance is inaccurate. ///Answer: D"}
{"subject": "", "question": "Question: Assume we run the Louvain algorithm to find communities in the following graph. Let\u00a0\u0394Q(\ud835\udc56 \u2192 \ud835\udc34) and \u0394Q(\ud835\udc56 \u2192 \ud835\udc35) denote the modularity gain of joining node \ud835\udc56 to community\u00a0\ud835\udc34 and \ud835\udc35 respectively. Which is true?\n\nOptions:\nA. \u0394Q(\ud835\udc56\u2192\ud835\udc34)>\u0394Q(\ud835\udc56\u2192\ud835\udc35)\nB. \u0394Q(\ud835\udc56\u2192\ud835\udc34)=\u0394Q(\ud835\udc56\u2192\ud835\udc35)\nC. \u0394Q(\ud835\udc56\u2192\ud835\udc34)<\u0394Q(\ud835\udc56\u2192\ud835\udc35)\nD. All options are possible.\n", "answer": "Explanations: Answer: d) All options are possible.\n\nExplanation: When running the Louvain algorithm to find communities in a graph, the modularity gain of joining a node \ud835\udc56 to community \ud835\udc34 or \ud835\udc35 can vary based on the specific characteristics of the graph. Different nodes may have different impacts on the modularity of joining one community over another. Therefore, it is possible for any of the options (a), (b), or (c) to be true depending on the structure of the graph and the specific calculations of modularity gains for each node. Hence, all options are possible. ///Answer: D"}
{"subject": "", "question": "Question: In practice, what is the typical size of an RSA modulus?\n\nOptions:\nA. 64 bits\nB. 256 bits\nC. 1024 bits\nD. 8192 bits\n", "answer": "Explanations: Final Answer: 3. 1024 bits\n\nExplanation: The typical size of an RSA modulus is commonly considered to be 1024 bits, although 2048 bits or 4096 bits are more frequently used for stronger security. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following attack vectors apply to mobile Android systems?\n\nOptions:\nA. Apps may maliciously declare intent filters to receive intents\n            from benign apps.\nB. Overprivileged apps may be abused as a confused deputy, allowing\n            malicious apps to steal access to their privileges.\nC. Hardware vendors like \\$am\\$ung are primarily interested in making\n            money and not in providing software updates, resulting in outdated\n            software that is vulnerable to attacks.\nD. Malicious apps can intercept network traffic of benign apps.\n", "answer": "Explanations: Let's analyze each possible answer:\n\n1. Apps may maliciously declare intent filters to receive intents from benign apps:\n   - This is a valid attack vector on mobile Android systems. Malicious apps can declare intent filters to receive intents from benign apps and potentially exploit this to carry out attacks.\n\n2. Overprivileged apps may be abused as a confused deputy, allowing malicious apps to steal access to their privileges:\n   - This is also a valid attack vector on mobile Android systems. If an overprivileged app is exploited by a malicious app, it can lead to the unauthorized access of privileges.\n\n3. Hardware vendors like \\$am\\$ung are primarily interested in making money and not in providing software updates, resulting in outdated software that is vulnerable to attacks:\n   - While this statement reflects a common issue with software updates on some Android devices, it is not specifically an attack vector. It is more related to the security implications of outdated software but not a direct attack vector.\n\n4. Malicious apps can intercept network traffic of benign apps:\n   - This is another valid attack vector on mobile Android systems. Malicious apps can intercept network traffic of benign apps and potentially access sensitive information or manipulate data.\n\nBased on the analysis, the correct answers are:\n- Apps may maliciously declare intent filters to receive intents from benign apps.\n- Overprivileged apps may be abused as a confused deputy, allowing malicious apps to steal access to their privileges.\n- Malicious apps can intercept network traffic of benign apps.\n\nTherefore, the correct option is: Apps may maliciously declare intent filters to receive intents from benign apps, Overprivileged apps may be abused as a confused deputy, allowing malicious apps to steal access to their privileges, and Malicious apps can intercept network traffic of benign apps. ///Answer: A"}
{"subject": "", "question": "Question: In a zero-knowledge interactive proof, the prover wants to prove :?\n\nOptions:\nA. nothing.\nB. a statement without interaction.\nC. something he doesn't know.\nD. a statement without revealing anything else besides that the statement holds.\n", "answer": "Explanations: In a zero-knowledge interactive proof, the prover wants to prove a statement without revealing anything else besides that the statement holds. This is because in a zero-knowledge proof, the prover wants to convince the verifier that they know the solution to a problem without revealing any additional information about how they arrived at the solution. The key idea is to demonstrate knowledge of the solution without giving away any details that could be used to replicate or reverse-engineer the solution. ///Answer: D"}
{"subject": "", "question": "Question: The exponent of the group $\\mathbb{Z}_9^*$ is?\n\nOptions:\nA. 6.\nB. 9.\nC. 8.\nD. 3.\n", "answer": "Explanations: To find the exponent of a group, we need to determine the smallest positive integer $n$ such that $a^n \\equiv 1 \\pmod{m}$ for all $a$ in the group. Here, the group in question is $\\mathbb{Z}_9^*$, which consists of the integers relatively prime to 9.\n\nThe elements in $\\mathbb{Z}_9^*$ are $\\{1, 2, 4, 5, 7, 8\\}$.\n\nLet's calculate the powers of each element modulo 9:\n- $1^1 \\equiv 1 \\pmod{9}$\n- $2^1 \\equiv 2 \\pmod{9}$, $2^2 \\equiv 4 \\pmod{9}$, $2^3 \\equiv 8 \\pmod{9}$, $2^4 \\equiv 7 \\pmod{9}$, $2^5 \\equiv 5 \\pmod{9}$, $2^6 \\equiv 1 \\pmod{9}$\n- $4^1 \\equiv 4 \\pmod{9}$, $4^2 \\equiv 7 \\pmod{9}$, $4^3 \\equiv 1 \\pmod{9}$\n- $5^1 \\equiv 5 \\pmod{9}$, $5^2 \\equiv 7 \\pmod{9}$, $5^3 \\equiv 1 \\pmod{9}$\n- $7^1 \\equiv 7 \\pmod{9}$, $7^2 \\equiv 4 \\pmod{9}$, $7^3 \\equiv 1 \\pmod{9}$\n- $8^1 \\equiv 8 \\pmod{9}$, $8^2 \\equiv 1 \\pmod{9}$\n\nFrom these calculations, we see that for all elements in $\\mathbb{Z}_9^*$, the highest power such that $a^n \\equiv 1 \\pmod{9}$ is $n = 6$. Therefore, the exponent of the group $\\mathbb{Z}_9^*$ is 6.\n\nThus, the correct answer from the options provided is 6. ///Answer: A:"}
{"subject": "", "question": "Question: Which of the following functions  \\( f :\\mathbb{Z} \times \\mathbb{Z} \to \\mathbb{Z} \\) are surjective?\n\nOptions:\nA. \\(  f(m,n)=m+n   \\)\nB. \\( f(m,n)=m^2+n^2  \\)\nC. \\(   f(m,n)=m  \\)\nD. \\(   f(m,n)=|n|  \\)\n", "answer": "Explanations: Let's first recall the definition of a surjective function. A function \\( f : A \\to B \\) is said to be surjective if for every element \\( b \\) in the codomain \\( B \\), there exists at least one element \\( a \\) in the domain \\( A \\) such that \\( f(a) = b \\).\n\nNow, let's analyze each of the given functions:\n\n1. \\( f(m,n) = m + n \\):\nTo check if this function is surjective, we need to see if for every integer \\( z \\) in the codomain \\( \\mathbb{Z} \\), there exists a pair of integers \\( (m,n) \\) such that \\( f(m,n) = m + n = z \\).\nConsidering that \\( m, n \\) are integers, we can see that for any given integer \\( z \\), we can choose \\( m = z \\) and \\( n = 0 \\). This will satisfy \\( f(m,n) = m + n = z \\) for any integer \\( z \\).\nTherefore, this function is surjective.\n\n2. \\( f(m,n) = m^2 + n^2 \\):\nTo check if this function is surjective, we need to see if for every integer \\( z \\) in the codomain \\( \\mathbb{Z} \\), there exists a pair of integers \\( (m,n) \\) such that \\( f(m,n) = m^2 + n^2 = z \\).\nHowever, this function is not surjective because it only produces non-negative values (since squares are always non-negative) and does not cover all integers in the codomain \\( \\mathbb{Z} \\).\n\n3. \\( f(m,n) = m \\):\nTo check if this function is surjective, we need to see if for every integer \\( z \\) in the codomain \\( \\mathbb{Z} \\), there exists a pair of integers \\( (m,n) \\) such that \\( f(m,n) = m = z \\).\nSince we can always choose \\( n = 0 \\) for any integer \\( z \\), we can see that this function covers all integers in the codomain \\( \\mathbb{Z} \\).\nTherefore, this function is surjective.\n\n4. \\( f(m,n) = |n| \\):\nTo check if this function is surjective, we need to see if for every integer \\( z \\) in the codomain \\( \\mathbb{Z} \\), there exists a pair of integers \\( (m,n) \\) such that \\( f(m,n) = |n| = z \\).\nSince the absolute value function always produces non-negative values, this function does not cover all integers in the codomain \\( \\mathbb{Z} \\).\nTherefore, this function is not surjective.\n\nSo, the surjective functions are:\n- \\( f(m,n) = m + n \\)\n- \\( f(m,n) = m \\)\n\nTherefore, the correct response is:\n- \\(  f(m,n)=m+n   \\)\n- \\(   f(m,n)=m  \\) ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion. Given a ring $R$, $R^\\star$ is\\ldots?\n\nOptions:\nA. a group.\nB. the set of invertible elements in $R$.\nC. $R-\\{0\\}$.\nD. the set of units.\n", "answer": "Explanations: A: The false assertion among the given options is: $R-\\{0\\}$. \n\nHere's the reasoning:\n- $R^\\star$ is the set of invertible elements in $R$, not the entire ring $R$ excluding the zero element. In a ring $R$, the invertible elements are those that have multiplicative inverses within the ring. So, $R^\\star$ is a subset of $R$ consisting of elements that have multiplicative inverses. \n\nTherefore, the false assertion is that $R^\\star$ is equal to $R-\\{0\\}$. ///Answer: A:"}
{"subject": "", "question": "Question: Mean Square Error loss:?\n\nOptions:\nA. Minimizing the distance between the predicted point and the true point\nB. Maximizing the probability of the correct class\nC. Minimizing the score of false classes when they are close, or bigger than, the score of the true class\nD. Maximizing the accuracy\n", "answer": "Explanations: First, let's break down the characteristics of Mean Square Error (MSE) loss:\n\n1. Minimizing the distance between the predicted point and the true point:\n   - MSE loss measures the average squared difference between the predicted values and the actual values. By minimizing this difference, we aim to reduce the error in the predictions.\n\n2. Maximizing the probability of the correct class:\n   - MSE loss is not directly related to maximizing the probability of the correct class. It is more commonly used in regression problems where the goal is to minimize the average squared differences between predicted and actual values.\n\n3. Minimizing the score of false classes when they are close, or bigger than, the score of the true class:\n   - MSE loss does not explicitly consider the scores of false classes in classification problems. It focuses on the squared differences between predicted and actual values, which might not be suitable for classification tasks.\n\n4. Maximizing the accuracy:\n   - MSE loss is not typically used for maximizing accuracy in classification tasks. It is more suitable for regression problems where the goal is to minimize the error between predicted and actual values.\n\nIn summary, MSE loss is mainly used for regression tasks to minimize the squared differences between predicted and actual values, rather than maximizing the probability of correct classes or accuracy in classification problems.\n\nTherefore, the statement that best aligns with MSE loss is: \"Minimizing the distance between the predicted point and the true point.\" ///Answer: A"}
{"subject": "", "question": "Question: Which defense(s) highlight the principle of least privilege in software\n    security?\n\nOptions:\nA. DEP bits by disallowing execution on certain memory pages because code is restricted to code pages.\nB. CFI protection on the forward edge because the check limits reachable targets.\nC. Applying updates regularly because software updates always reduce privileges.\nD. A stack canary because it will signal any stack-based attack.\n", "answer": "Explanations: The principle of least privilege in software security states that entities should only have the minimum level of access or permissions necessary to perform their tasks. Let's analyze each defense mentioned in the options to identify which one(s) highlight the principle of least privilege:\n\n1. **DEP bits by disallowing execution on certain memory pages because code is restricted to code pages**: Data Execution Prevention (DEP) helps prevent certain types of attacks by marking specific memory locations as non-executable. This defense aligns with the principle of least privilege as it restricts the execution of code in certain memory pages, reducing the potential attack surface.\n\n2. **CFI protection on the forward edge because the check limits reachable targets**: Control Flow Integrity (CFI) protection helps prevent attackers from redirecting the control flow of a program to malicious code. By limiting the reachable targets through checks, CFI aligns with the least privilege principle by restricting the possible paths the program can take.\n\n3. **Applying updates regularly because software updates always reduce privileges**: While applying updates regularly is a good security practice to patch vulnerabilities, it may not directly highlight the principle of least privilege. Software updates may enhance security by fixing bugs and vulnerabilities, but they do not inherently reduce privileges; they may add new features or security measures.\n\n4. **A stack canary because it will signal any stack-based attack**: Stack canaries are used to detect buffer overflow attacks by placing a unique value in the stack before the return address. While stack canaries enhance security by detecting attacks, they do not directly relate to the principle of least privilege as they are more focused on detecting specific types of attacks rather than limiting privileges.\n\nBased on the analysis, the defenses that highlight the principle of least privilege in software security are:\n- DEP bits by disallowing execution on certain memory pages because code is restricted to code pages.\n- CFI protection on the forward edge because the check limits reachable targets. ///Answer: A"}
{"subject": "", "question": "Question: What is the minimum distance of a linear block code over $\\mathbb{F}_{7}$ that has $$H = \\left(\begin{array}{cccccc}1&0&0&0&3&6\\0&1&0&0&6&6\\0&0&1&0&6&3\\end{array}\right)$$ as the parity check matrix? Check the correct answer.\n\nOptions:\nA. 1\nB. 0\nC. 2\nD. 3\nE. 4\nF. 5\n", "answer": "Explanations: The minimum distance of a linear block code is the smallest number of positions in which any two distinct codewords differ. In other words, it is the minimum number of errors that can be corrected by the code.\n\nTo find the minimum distance of a linear block code with a given parity check matrix, we need to look for the smallest non-zero weight of the columns of the parity check matrix. The weight of a column is the number of non-zero entries in that column.\n\nLooking at the given parity check matrix H, we can see that the weight of the first column is 1, the weight of the second column is 1, and the weight of the third column is 1. Therefore, the minimum distance of the linear block code is 1.\n\nSo, the correct answer is:\n1 ///Answer: A"}
{"subject": "", "question": "Question: Consider a neural net with $K$ nodes per hidden layer. In a standard such net we have $K^{2}$ parameters/weights (ignoring the bias terms) per layer. Consider a convolutional net where the data is laid out in a one-dimensional fashion and the filter/kernel has $M$ non-zero terms. Ignoring the bias terms, how many parameters are there per layer?\n\nOptions:\nA. (a) $K^{2}$\nB. (b) $M^{2}$\nC. (c) $K M$\nD. (d) $K$\nE. (e) $M$\nF. (f) $1$\n", "answer": "Explanations: **Hint for Students:** Try to think about how the number of parameters in a convolutional net is related to the size of the filter/kernel and the number of nodes per hidden layer.\n\n**Solution:**\n\nIn a convolutional neural network with a one-dimensional layout for the data and a filter/kernel with $M$ non-zero terms, the number of parameters per layer is given by the product of the number of nodes per hidden layer ($K$) and the size of the filter/kernel ($M$).\n\nTherefore, the correct answer is:\n\n**Answer: (c) $K M$**\n\nThis implies that for each hidden layer in the convolutional neural network, there are $K M$ parameters, ignoring the bias terms. ///Answer: C"}
{"subject": "", "question": "Question: Let $\\mathcal{C}$ be a $(n,k)$ Reed-Solomon code on $\\mathbb{F}_q$. Let $\\mathcal{C}'$ be the $(2n,k)$ code such that each codeword of $\\mathcal{C}'$ is a codeword of $\\mathcal{C}$ repeated twice, i.e., if $(x_1,\\dots,x_n) \\in\\mathcal{C}$, then $(x_1,\\dots,x_n,x_1,\\dots,x_n)\\in\\mathcal{C'}$. What is the minimum distance of $\\mathcal{C}'$?\n\nOptions:\nA. $2n-2k+2$\nB. $2n-k+1$\nC. $2n-2k+1$\nD. $2n-k+2$\n", "answer": "Explanations: The minimum distance of the $(2n,k)$ code $\\mathcal{C}'$ constructed by repeating each codeword of the $(n,k)$ Reed-Solomon code $\\mathcal{C}$ twice is $2n-2k+2$.\nExplanation:\n- When each codeword of $\\mathcal{C}$ is repeated twice to form $\\mathcal{C}'$, the minimum distance of the resulting code is the sum of the minimum distance of $\\mathcal{C}$ and 2.\n- The minimum distance of a Reed-Solomon code $\\mathcal{C}$ with parameters $(n,k)$ over $\\mathbb{F}_q$ is $d_{\\min} = n-k+1$.\n- Therefore, for the code $\\mathcal{C}'$ with parameters $(2n,k)$, the minimum distance is $d_{\\min}' = 2n-k+1 = 2n-2k+2$.\nSo, the correct answer is $2n-2k+2$. ///Answer: A"}
{"subject": "", "question": "Question: The ElGamal cryptosystem is based on\\dots?\n\nOptions:\nA. nothing.\nB. the discrete logarithm problem.\nC. the RSA problem.\nD. the factorization problem.\n", "answer": "Explanations: The correct answer is B. the discrete logarithm problem.\n\nElGamal cryptosystem is based on the difficulty of solving the discrete logarithm problem, which involves finding the exponent when given the base and the result in a finite field. In ElGamal cryptosystem, the security relies on the assumption that it is computationally hard to compute the discrete logarithm. This makes it a suitable choice for secure encryption. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} statement. When $x\\rightarrow+\\infty$ \\ldots?\n\nOptions:\nA. $x^3 + 2x + 5 = \\mathcal{O}(x^3)$.\nB. $\\frac{1}{x^2} = \\mathcal{O}(\\frac{1}{x})$.\nC. $2^{\\frac{x}{\\log x}} = \\mathcal{O}(2^x)$.\nD. $n^x = \\mathcal{O}(x^n)$ for any constant $n>1$.\n", "answer": "Explanations: The incorrect statement is: $n^x = \\mathcal{O}(x^n)$ for any constant $n>1$.\n\nExplanation:\n1. $x^3 + 2x + 5 = \\mathcal{O}(x^3)$ is correct because for large values of $x$, the term $2x + 5$ becomes negligible compared to $x^3$.\n2. $\\frac{1}{x^2} = \\mathcal{O}(\\frac{1}{x})$ is correct since as $x$ approaches infinity, $\\frac{1}{x}$ dominates $\\frac{1}{x^2}$.\n3. $2^{\\frac{x}{\\log x}} = \\mathcal{O}(2^x)$ is correct because $2^{\\frac{x}{\\log x}}$ grows at a slower rate compared to $2^x$ as $x$ goes to infinity.\n4. $n^x = \\mathcal{O}(x^n)$ for any constant $n>1$ is incorrect. In Big O notation, $f(x) = \\mathcal{O}(g(x))$ means that $|f(x)| \\leq M|g(x)|$ for some constant $M$ and for sufficiently large $x$. In this case, for $n>1$, $n^x$ grows exponentially faster than $x^n$, so $x^n = \\mathcal{O}(n^x)$ is not true. ///Answer: D"}
{"subject": "", "question": "Question: Consider the group $(\\mathbb{Z} / 23 \\mathbb{Z}^*, \\cdot)$. Find how many elements of the group are generators of the group. (Hint: $5$ is a generator of the group.)?\n\nOptions:\nA. $10$\nB. $22$\nC. $11$\nD. $2$\n", "answer": "Explanations: To find how many elements of the group are generators of the group, we need to find the number of elements that have the property that their powers generate all elements of the group.\n\nIn the group $(\\mathbb{Z} / 23 \\mathbb{Z}^*, \\cdot)$, the order of the group is $\\phi(23) = 22$ (where $\\phi$ is Euler's totient function). \n\nGiven that $5$ is a generator of the group, we know that the number of generators of the group is equal to the number of elements coprime to $23$ (since generators are elements with order equal to the order of the group).\n\nTherefore, the number of generators of the group is $\\phi(22) = 10$.\n\nSo, the answer is:\n\na) $10$ ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textit{incorrect} assertion. Given an RSA public key $(e,N)$, we \\textit{can} efficiently compute square roots in $\\mathbb{Z}_{N}^*$ if we have an efficient algorithm that \\ldots?\n\nOptions:\nA. factors $N$.\nB. recovers the corresponding secret key $d$.\nC. computes $\\varphi(N)$, the order of $\\mathbb{Z}_N^*$.\nD. given a $y\\in \\mathbb{Z}_N$ computes an $x$ such that $x^e \\equiv y \\pmod{N}$.\n", "answer": "Explanations: Let's go through each option:\n\na) If we can efficiently factor $N$, then we can compute the prime factors $p$ and $q$ of $N$. With $p$ and $q$, we can compute $\\varphi(N) = (p-1)(q-1)$. This information allows us to efficiently compute square roots in $\\mathbb{Z}_N^*$.\n\nb) If we can recover the corresponding secret key $d$, then we can efficiently compute $\\varphi(N)$, which in turn allows us to compute square roots in $\\mathbb{Z}_N^*$.\n\nc) Computing $\\varphi(N)$ directly gives us the order of $\\mathbb{Z}_N^*$, which allows us to compute square roots in $\\mathbb{Z}_N^*$.\n\nd) This option is the incorrect assertion. Given $y\\in \\mathbb{Z}_N$, computing an $x$ such that $x^e \\equiv y \\pmod{N}$ does not necessarily allow us to efficiently compute square roots in $\\mathbb{Z}_N^*$. This is because finding a square root of $y$ modulo $N$ may not be equivalent to finding a square root in $\\mathbb{Z}_N^*$.\n\nTherefore, the incorrect assertion is d) given a $y\\in \\mathbb{Z}_N$ computes an $x$ such that $x^e \\equiv y \\pmod{N}$. ///Answer: D"}
{"subject": "", "question": "Question: Let $\\mathcal{C}$ be a binary $(n,k)$ linear code with minimum distance $d_{\\min} = 4$. Let $\\mathcal{C}'$ be the code obtained by adding a parity-check bit $x_{n+1}=x_1 \\oplus x_2 \\oplus \\cdots \\oplus x_n$ at the end of each codeword of $\\mathcal{C}$. Let $d_{\\min}'$ be the minimum distance of $\\mathcal{C}'$. Which of the following is true?\n\nOptions:\nA. $d_{\\min}' = 4$\nB. $d_{\\min}' = 5$\nC. $d_{\\min}' = 6$\nD. $d_{\\min}'$ can take different values depending on the code $\\mathcal{C}$.\n", "answer": "Explanations: When we add a parity-check bit to each codeword of a binary $(n,k)$ linear code $\\mathcal{C}$, the minimum distance of the resulting code $\\mathcal{C}'$ may change. In this case, we are given that the original code $\\mathcal{C}$ has a minimum distance of $d_{\\min} = 4$. \n\nIf we add a parity-check bit as described, we are essentially creating a new code where each codeword has an additional bit that is the XOR of all the original bits. This means that any two codewords that differ in only one bit will now differ in two bits, since changing one bit will also change the parity-check bit.\n\nBy adding this parity-check bit, we are effectively increasing the minimum distance of the code by 1. Therefore, the minimum distance of the new code $\\mathcal{C}'$ will be $d_{\\min}' = 5$. This is because any two codewords must differ in at least 5 positions now (the original 4 positions where they differed, plus the parity-check bit).\n\nSo, the correct answer is \"$d_{\\min}' = 5\". The other options are not correct because the minimum distance of the new code can be determined based on the change in the number of differing positions between codewords after adding the parity-check bit, which will always increase the minimum distance by 1 in this case. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements is correct?\n\nOptions:\nA. When applying stochastic gradient descent on the objective function $f(\\boldsymbol{w}):=\\sum_{n=1}^{30}\\left\\|\\boldsymbol{w}-\\boldsymbol{x}_{n}\\right\\|^{2}$ where $\\boldsymbol{x}_{n}$ are the datapoints, a stochastic gradient step is roughly $30 \\times$ faster than a full gradient step.\nB. In practice, it could be good to let your model first overfit your task, and then apply drop-out or other regularization techniques.\nC. When applying stochastic gradient descent on the objective function $f(\\boldsymbol{w}):=\\sum_{n=1}^{30} n \\cdot\\|\\boldsymbol{w}\\|^{2}$, a stochastic gradient (for the $n$-th summand) is given by $2 n \\cdot \\boldsymbol{w}$.\nD. The function $f(\\boldsymbol{u} ; \\boldsymbol{v}):=g\\left(\\boldsymbol{u} \\boldsymbol{v}^{\\top}\\right)$ is convex over the set of pairs of vectors $(\\boldsymbol{u} ; \\boldsymbol{v}) \\in \\mathbb{R}^{2} \\times \\mathbb{R}^{2}$, when $g: \\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R}$ is defined as $g(\\boldsymbol{X}):=X_{12}+X_{21}$.\n", "answer": "Explanations: The correct statement is:\n\n\"When applying stochastic gradient descent on the objective function $f(\\boldsymbol{w}):=\\sum_{n=1}^{30}\\left\\|\\boldsymbol{w}-\\boldsymbol{x}_{n}\\right\\|^{2}$ where $\\boldsymbol{x}_{n}$ are the datapoints, a stochastic gradient step is roughly $30 \\times$ faster than a full gradient step.\"\n\nThis statement is correct for the following reasons:\n\n1. **Understanding Stochastic Gradient Descent (SGD):**\n   - Stochastic gradient descent is an optimization algorithm commonly used in machine learning to update the model parameters iteratively. In SGD, instead of computing the gradient of the entire dataset (full gradient), we compute the gradient of the loss function using only a single data point or a small subset of data points at each iteration.\n\n2. **Comparison with Full Gradient Step:**\n   - In this case, the objective function $f(\\boldsymbol{w})$ is defined as the sum of squared distances between the model parameter vector $\\boldsymbol{w}$ and the data points $\\boldsymbol{x}_n$.\n   - When using SGD on this objective function, we update the model parameters based on the gradient computed for each individual data point.\n   - Since there are 30 data points in this case, a single SGD step processes only one data point at a time, making it roughly 30 times faster than a full gradient step which would require processing all 30 data points simultaneously.\n\n3. **Efficiency of SGD:**\n   - The speed advantage of SGD over full gradient descent comes from the fact that it processes data points one at a time, making it computationally faster, especially for large datasets.\n   - While SGD may introduce more noise due to the random sampling of data points, it often converges faster due to more frequent updates of the model parameters.\n\nTherefore, the statement correctly highlights the speed advantage of stochastic gradient descent over full gradient descent in terms of processing time when dealing with the given objective function and dataset. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following lock acquisition orders (locks are acquired from left to right), for thread 1 (T1) and thread 2 (T2), will result in a deadlock ? Assume that A, B, C, D are lock instances.\n\nOptions:\nA. T1: A,B,C,D      T2: A,B,C,D\nB. T1: A,D,C,B      T2: A,D,C,B\nC. T1: A,B,C,D      T2: D,C,B,A\nD. T1: A,B,C,D      T2: A,B,E,F\nE. T1: A,B,C,D      T2: E,B,A,F\n", "answer": "Explanations: T1: A,B,C,D      T2: D,C,B,A Justification: In this case, T1 acquires locks in ascending order while T2 acquires locks in descending order, which can lead to a deadlock if T1 and T2 hold locks that the other thread needs to continue executing. ///Answer: C"}
{"subject": "", "question": "Question: In principal component analysis, the left singular vectors $\\mathbf{U}$ of a data matrix $\\mathbf{X}$ of shape ( $d$ features, $n$ datapoints) are used to create a new data matrix $\\mathbf{X}^{\\prime}=\\mathbf{U}^{\\top} \\mathbf{X}$. To achieve dimensionality reduction, we keep only certain rows of the matrix $\\mathbf{X}^{\\prime}$. We keep those rows that have:?\n\nOptions:\nA. the lowest variance.\nB. the highest variance.\nC. smallest L2 norm.\nD. L2 norm closest to 1\n", "answer": "Explanations: The rows of the matrix $\\mathbf{X}^{\\prime}$ represent the principal components of the data, where each row captures a different direction of maximum variance in the data. \n\n1. Keeping the rows with the lowest variance would result in discarding the components with the least amount of information. This would lead to a significant loss of important information, as the components with higher variance usually contain the most relevant information about the data.\n\n2. Keeping the rows with the highest variance is the correct approach in PCA for dimensionality reduction. The principal components are ordered in decreasing variance, meaning the first few components capture the most variance in the data. By keeping the rows with the highest variance, we retain the most important information while reducing the dimensionality of the data.\n\n3. Keeping the rows with the smallest L2 norm does not have a direct relation to the variance of the data. The L2 norm measures the magnitude of a vector, and selecting rows based on this criterion may not preserve the most important information captured by the principal components.\n\n4. Keeping the rows with an L2 norm closest to 1 may not be a meaningful criterion in PCA for dimensionality reduction. The norm of the principal components can vary significantly, and selecting rows based on their proximity to a specific norm value may not be related to capturing the most variance in the data. \n\nIn conclusion, the correct approach for dimensionality reduction in PCA is to keep the rows with the highest variance in the matrix $\\mathbf{X}^{\\prime}$, as these components contain the most important information about the data distribution. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textit{wrong} assertion.\n\nOptions:\nA. If $\\Big(\\frac{b}{n}\\Big)=+$1 for $b \\in \\mathbb{Z}_n^* $, then $b$ is a quadratic residue.\nB. If $\\Big(\\frac{b}{p}\\Big)=+$1 for $b \\in \\mathbb{Z}_p^* $, then $b$ is a quadratic residue, where $p$ is prime.\nC. If $b$ is a quadratic residue for $b \\in \\mathbb{Z}_n^* $, then $\\Big(\\frac{b}{n}\\Big)=+1$.\nD. If $b$ is a quadratic residue for $b \\in \\mathbb{Z}_p^* $, then $\\Big(\\frac{b}{p}\\Big)=+1$, where $p$ is prime.\n", "answer": "Explanations: The wrong assertion is: \n\nIf $\\Big(\\frac{b}{n}\\Big)=+1$ for $b \\in \\mathbb{Z}_n^*$, then $b$ is a quadratic residue. \n\nTo show that this is wrong, we can consider the counterexample $n=21$ and $b=2$. \n\nFirst, we calculate $\\Big(\\frac{2}{21}\\Big)$. Since $21 = 3 \\times 7$, we have $\\Big(\\frac{2}{21}\\Big) = \\Big(\\frac{2}{3}\\Big) \\times \\Big(\\frac{2}{7}\\Big)$. \n\nCalculating each term separately gives $\\Big(\\frac{2}{3}\\Big) = -1$ (since $2$ is not a quadratic residue modulo $3$) and $\\Big(\\frac{2}{7}\\Big) = 1$ (since $2$ is a quadratic residue modulo $7$). \n\nTherefore, $\\Big(\\frac{2}{21}\\Big) = -1 \\times 1 = -1$, which means that $2$ is not a quadratic residue modulo $21$. \n\nThus, the assertion is wrong. ///Answer: A:"}
{"subject": "", "question": "Question: Given a document collection, if we change the ordering of the words in the documents, which of the following will not change?\n\nOptions:\nA. Singular values in Latent Semantic Indexing (LSI)\nB. The entities extracted using a Hidden Markov Model (HMM)\nC. The embedding vectors produced by Word2vec\nD. All the previous will change\n", "answer": "Explanations: To determine which of the given options will not change when the ordering of words in a document collection is altered, we need to analyze the nature of each of the options in relation to the document content and structure.\n\n1. Singular values in Latent Semantic Indexing (LSI):\n   - Correct/Incorrect: Correct\n   - Explanation: The singular values in Latent Semantic Indexing (LSI) are related to the mathematical decomposition of the term-document matrix into singular values. This decomposition is based on the frequency of words in the document collection and their relationships. The singular values are determined by the overall structure of the document collection and are not affected by the specific ordering of words within individual documents.\n\n2. The entities extracted using a Hidden Markov Model (HMM):\n   - Correct/Incorrect: Incorrect\n   - Explanation: Hidden Markov Models (HMMs) are probabilistic models used for sequence prediction and analysis. The entities extracted using HMMs are dependent on the sequential structure of the data, including the ordering of words. Therefore, changing the ordering of words in the documents will likely affect the entities extracted using an HMM.\n\n3. The embedding vectors produced by Word2vec:\n   - Correct/Incorrect: Incorrect\n   - Explanation: Word2vec is a popular technique for generating word embeddings based on the context of words in a corpus. The embedding vectors produced by Word2vec are influenced by the neighboring words and their positions in the document. Therefore, altering the ordering of words will lead to changes in the embedding vectors produced by Word2vec.\n\n4. All the previous will change:\n   - Correct/Incorrect: Incorrect\n   - Explanation: As discussed above, the singular values in LSI are not affected by the ordering of words, while the entities extracted using HMMs and the embedding vectors produced by Word2vec are influenced by the sequential structure of the documents. Therefore, not all the previous options will change when the ordering of words is modified.\n\nBased on the analysis provided, the correct answer is:\n{'Correct Answer(s)': 1. Singular values in Latent Semantic Indexing (LSI)} ///Answer: A"}
{"subject": "", "question": "Question: What should be the type of value ? // list: List[String] list.foldLeft(value)((x, y) => x + y.length)?\n\nOptions:\nA. List[Int]\nB. Int\nC. List[String]\nD. List[Char]\n", "answer": "Explanations: The correct type of value should be 'Int'. \n\nExplanation: The foldLeft method is used to combine the elements of a collection using a binary operation. In this case, the operation is adding the length of each string in the list to the accumulator value. Since we are adding the lengths of the strings, the accumulator value should be an integer. ///Answer: B"}
{"subject": "", "question": "Question: The Diffie-Hellman key agreement protocol \\ldots?\n\nOptions:\nA. \\ldots was invented by Rivest, Shamir and Adleman.\nB. \\ldots can be broken with a factoring oracle.\nC. \\ldots resists to active adversaries.\nD. \\ldots resists to passive adversaries.\n", "answer": "Explanations: 0: False. The Diffie-Hellman key agreement protocol was actually invented by Whitfield Diffie and Martin Hellman in 1976.\n\n1: True. The Diffie-Hellman key agreement protocol is based on the discrete logarithm problem, which is believed to be hard to solve efficiently, even for a quantum computer. Therefore, it is resistant to attacks using a factoring oracle.\n\n2: False. The Diffie-Hellman key agreement protocol is vulnerable to man-in-the-middle attacks if not properly authenticated, making it susceptible to active adversaries.\n\n3: True. The Diffie-Hellman key agreement protocol provides perfect forward secrecy, meaning that even if an adversary passively records the communication, they cannot decrypt it later, ensuring resistance to passive adversaries. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\textbf{incorrect} assertion. Let $H:\\left\\{ 0,1 \\right\\}^*\\rightarrow\\left\\{ 0,1 \\right\\}^n$ be a hash function.\n\nOptions:\nA. We can use $H$ to design a commitment scheme.\nB. We can use $H$ to design a key derivation function.\nC. Finding $x,y\\in\\left\\{ 0,1 \\right\\}^*$ such that $x\\neq y$ and $h(x) = h(y)$ can be done in $O(2^{n/2})$ time.\nD. Given $x\\in\\left\\{ 0,1 \\right\\}^*$, finding a $y \\in \\left\\{ 0,1 \\right\\}^*$ such that $x\\neq y$ and $h(x) = h(y)$ can be done in $O(2^{n/2})$ time.\n", "answer": "Explanations: To solve this problem, we need to carefully consider each assertion and determine which one is incorrect.\n\nOption 2 states that finding two distinct values $x, y \\in \\left\\{ 0,1 \\right\\}^*$ such that $x \\neq y$ and $h(x) = h(y)$ can be done in $O(2^{n/2})$ time. This assertion is actually correct due to the birthday paradox, which shows that the probability of a collision in a hash function increases significantly as the number of hashed values approaches $2^{n/2}$. Therefore, Option 2 is not the incorrect assertion.\n\nOption 3 states that given a value $x \\in \\left\\{ 0,1 \\right\\}^*$, finding a different value $y \\in \\left\\{ 0,1 \\right\\}^*$ such that $x \\neq y$ and $h(x) = h(y)$ can be done in $O(2^{n/2})$ time. This assertion is also correct for the same reasons as Option 2. The birthday paradox applies here as well, making it possible to find collisions in a hash function in approximately $O(2^{n/2})$ time. Therefore, Option 3 is not the incorrect assertion.\n\nOption 1 states that we can use the hash function $H$ to design a commitment scheme. This is a valid use of hash functions in cryptographic protocols, where they are indeed utilized to create commitment schemes. Therefore, Option 1 is not the incorrect assertion.\n\nOption 4 suggests that we can use the hash function $H$ to design a key derivation function. Key derivation functions are commonly constructed using hash functions to generate cryptographic keys from a given input. Therefore, this assertion is also correct.\n\nConsidering the above analysis, the incorrect assertion among the options provided is:\n\nFinding $x,y \\in \\left\\{ 0,1 \\right\\}^*$ such that $x \\neq y$ and $h(x) = h(y)$ can be done in $O(2^{n/2})$ time.\n\nTherefore, the correct answer is Option 2. ///Answer: D"}
{"subject": "", "question": "Question: The Murphy Law states that if there is a single security hole in an exposed cryptosystem, then\\dots?\n\nOptions:\nA. hope for the best\nB. nobody will look for it\nC. nobody will find it\nD. someone will ultimately find it\n", "answer": "Explanations: The Murphy Law states that if there is a single security hole in an exposed cryptosystem, then someone will ultimately find it.\n\n- The correct option is: someone will ultimately find it.\n\nExplanation:\n1. The Murphy Law is a principle that states that if anything can go wrong, it will go wrong.\n2. In the context of a cryptosystem, a security hole refers to a vulnerability or weakness in the system that could potentially be exploited by attackers.\n3. When the Murphy Law is applied to a cryptosystem with a security hole, it implies that eventually, someone will discover and exploit that vulnerability.\n4. This highlights the importance of conducting thorough security assessments and regularly updating and patching cryptosystems to prevent unauthorized access and protect sensitive information. ///Answer: D"}
{"subject": "", "question": "Question: In a Gaussian Mixture Model, assuming $D, K \\ll N$, the number of free parameters, after marginalization of the latent variables $z_{n}$, is?\n\nOptions:\nA. (a)  quadratic in $D$\nB. (b)  cubic in $D$\nC. (c)  linear in $N$\n", "answer": "Explanations: The number of free parameters in a Gaussian Mixture Model (GMM) can be calculated based on the parameters of the model. In a GMM, each component is characterized by its mean vector, covariance matrix, and mixing coefficient. \n\nGiven:\n- $D$ as the dimensionality of the data,\n- $K$ as the number of mixture components,\n- $N$ as the number of data points.\n\nThe number of parameters in a GMM can be calculated as follows:\n1. Mean parameters: Each mean vector for a component has $D$ elements. So, there are $D$ parameters per component. Therefore, for $K$ components, the total number of mean parameters is $K \\times D$.\n2. Covariance parameters: Each covariance matrix for a component has $D \\times D$ elements, but it is symmetric. Considering only the unique elements, there are $\\frac{D(D+1)}{2}$ parameters per component. So, for $K$ components, the total number of unique covariance parameters is $K \\times \\frac{D(D+1)}{2}$.\n3. Mixing coefficients: There are $K-1$ mixing coefficients since the last one is determined by the others to sum up to 1. So, there are $K-1$ mixing coefficients.\n\nTherefore, the total number of parameters in a GMM is:\n$$\n\\text{Total Parameters} = K \\times D + K \\times \\frac{D(D+1)}{2} + (K-1)\n$$\n\nWhen marginalizing the latent variables $z_{n}$ (responsibilities), each data point $n$ is associated with a specific component, reducing the number of free parameters. Since $D, K \\ll N$, we can simplify the parameter count by considering that $N$ data points can be used to estimate the parameters, reducing the degrees of freedom. This leads to the number of free parameters being linear in $N$.\n\nTherefore, the correct option is:\n(c) linear in $N$. ///Answer: C:"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. UMTS uses?\n\nOptions:\nA. MILLENAGE for key establishment.\nB. the block cipher KASUMI.\nC. the block cipher AES.\nD. the block cipher DES.\n", "answer": "Explanations: The incorrect statement is: UMTS uses the block cipher DES for key establishment. \n\nUMTS (Universal Mobile Telecommunications System) does not use the block cipher DES for key establishment. DES (Data Encryption Standard) is an outdated encryption algorithm, and newer standards like AES (Advanced Encryption Standard) have replaced it in modern systems. For example, LTE (Long-Term Evolution) networks, which are the successors to UMTS, typically use AES for encryption and key establishment. ///Answer: C"}
{"subject": "", "question": "Question: Which loss function(s) should you use? (One or multiple answers)?\n\nOptions:\nA. mean square error (MSE) loss\nB. cross entropy loss\nC. hinge loss\nD. L1 loss\n", "answer": "Explanations: The correct choices are: 1, 2, 3, 4.\n\nExplanation: Each loss function serves a specific purpose based on the nature of the problem. MSE is for regression with continuous output, Cross Entropy for multi-class classification with probabilities, Hinge Loss for binary SVM classification focusing on margins, and L1 Loss for regression with outliers. Selecting the appropriate loss function is crucial for optimizing model performance and achieving desired results in machine learning tasks. ///Answer: A"}
{"subject": "", "question": "Question: Consider the linear mask $\\alpha := \\mathsf{0xf0}$ and the input $X := \\mathsf{0xe2}$. We have $\\alpha \\cdot X = $ \\dots?\n\nOptions:\nA. $\\mathsf{0}$\nB. $\\mathsf{1}$\nC. $\\mathsf{0xe0}$\nD. $\\mathsf{0xf2}$\n", "answer": "Explanations: To calculate the output of $\\alpha \\cdot X$, we perform a bitwise AND operation between $\\alpha$ and $X$. \n\nGiven:\n- $\\alpha = \\mathsf{0xf0}$ (which is 11110000 in binary)\n- $X = \\mathsf{0xe2}$ (which is 11100010 in binary)\n\nPerforming the bitwise AND operation:\n```\n    11110000\nAND 11100010\n    --------\n    11100000\n```\n\nConverting the result $11100000$ back to hexadecimal gives $\\mathsf{0xe0}$.\n\nTherefore, the correct answer is:\n3. $\\mathsf{0xe0}$ ///Answer: C"}
{"subject": "", "question": "Question: Which one is \\emph{not} a security requirement on hash functions?\n\nOptions:\nA. onewayness\nB. bijectivity\nC. pseudo-randomness\nD. collision resistance\n", "answer": "Explanations: The security requirement that is \\emph{not} a characteristic of hash functions is \"bijectivity\". \n\nExplanation:\n- Onewayness: A hash function should be computationally infeasible to reverse, meaning given the output (hash value), it should be difficult to determine the input.\n- Pseudo-randomness: A hash function should produce output that appears random and unpredictable, even if the input is similar or only slightly different.\n- Collision resistance: A hash function should make it difficult to find two different inputs that produce the same output (hash value).\n\nBijectivity, on the other hand, refers to a function where each input has a unique output and each output has a unique input. This is not a security requirement for hash functions as they are designed to compress data and create fixed-length representations, potentially resulting in different inputs mapping to the same output (hash value). ///Answer: B"}
{"subject": "", "question": "Question: Why is it not recommended to use plain RSA in practice?\n\nOptions:\nA. because of quantum computers.\nB. because of the length of the public keys.\nC. because of the trapdoor one-wayness of the RSA function.\nD. because of the homomorphic property of the RSA function.\n", "answer": "Explanations: Let's break down the options provided:\n\n1. \"Because of quantum computers\": Quantum computers have the potential to break RSA encryption by efficiently factoring large numbers through algorithms like Shor's algorithm.\n\n2. \"Because of the length of the public keys\": RSA requires longer key lengths compared to other asymmetric encryption algorithms for the same level of security, which can impact performance and efficiency.\n\n3. \"Because of the trapdoor one-wayness of the RSA function\": RSA relies on the difficulty of factoring large composite numbers, which could potentially be weakened by advancements in factoring algorithms or computational power.\n\n4. \"Because of the homomorphic property of the RSA function\": RSA is not homomorphic, meaning it does not support computations on encrypted data without decryption. Homomorphic encryption allows operations to be performed on ciphertext directly.\n\nConsidering these points, the most relevant reason for not using plain RSA in practice is:\n\n<b>Because of the length of the public keys</b> ///Answer: B"}
{"subject": "", "question": "Question: Which of the following graph analysis techniques do you believe would be most appropriate to identify communities on a social graph?\n\nOptions:\nA. Cliques\nB. Random Walks\nC. Shortest Paths\nD. Association rules\n", "answer": "Explanations: Another effective technique to identify communities on a social graph would be **Random Walks**. By simulating random walks on the graph, we can identify clusters of nodes that are more likely to be visited together, indicating potential communities or groups within the network. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. The advantage of a distinguisher of two distributions $P_0$ and $P_1$?\n\nOptions:\nA. is always the Euclidean distance between $P_0$ and $P_1$.\nB. is $\\mathsf{Adv}_{\\mathcal{A}} (P_0 , P_1 ) = \\Pr[P = P_1|A \\rightarrow 1]-\\Pr[P = P_0| A \\rightarrow 1]$.\nC. is $\\mathsf{Adv}_{\\mathcal{A}} (P_0 , P_1 ) = \\Pr[A \\rightarrow 0|P = P_1 ]-\\Pr[A \\rightarrow 1|P = P_0]$.\nD. can touch the statistical distance $\\frac{1}{2}\\Sigma_{x}|P_0(x) - P_1(x)|$ between $P_0$ and $P_1$, when he makes only one query.\n", "answer": "Explanations: Reasoning:\n1. In cryptography, the advantage of a distinguisher quantifies how well an adversary can distinguish between two probability distributions.\n2. The advantage of a distinguisher is calculated as the difference between the probabilities that the distinguisher outputs the correct distribution minus the incorrect distribution.\n3. The advantage can touch the statistical distance between two distributions when the adversary makes only one query, as the statistical distance measures the similarity between probability distributions.\n4. The statistical distance between two distributions $P_0$ and $P_1$ is defined as $\\frac{1}{2}\\sum_{x} |P_0(x) - P_1(x)|$.\n5. Therefore, the correct assertion is:\n   - 'can touch the statistical distance $\\\\frac{1}{2}\\\\Sigma_{x}|P_0(x) - P_1(x)|$ between $P_0$ and $P_1$, when he makes only one query.'\n 'can touch the statistical distance $\\\\frac{1}{2}\\\\Sigma_{x}|P_0(x) - P_1(x)|$ between $P_0$ and $P_1$, when he makes only one query.' ///Answer: D"}
{"subject": "", "question": "Question: A model which has a high bias necessarily has a low variance.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: First, it's important to understand the definitions of bias and variance in the context of machine learning models.\n\n- Bias refers to the error introduced by approximating a real-world problem, which means the difference between the expected prediction of our model and the true value.\n- Variance, on the other hand, refers to the variability of model predictions for a given data point or a value which tells us spread of our data.\n\nNow, let's analyze the statement \"A model which has a high bias necessarily has a low variance\":\n\n- If a model has high bias, it means that the model is oversimplified and unable to capture the complexity of the data. This often results in underfitting.\n- A model with low variance means that the model's predictions are consistent across different training datasets.\n- In general, high bias models tend to have low variance because they are simple and make consistent (but incorrect) predictions.\n\nTherefore, the statement is generally true. A model with high bias tends to have low variance. However, it's important to note that this relationship is not always absolute and can vary depending on the specific characteristics of the model and the data. ///Answer: A"}
{"subject": "", "question": "Question: For this question,\u00a0one or more\u00a0assertions can be correct. Tick only the correct assertion(s).\u00a0There will be a penalty for wrong assertions ticked.Which of the following associations can be considered as illustrative examples for inflectional\nmorphology (with here the simplifying assumption that canonical forms are restricted to the roots\nonly)?\n\nOptions:\nA. (activate, action)\nB. (hypothesis, hypotheses)\nC. (to go, went)\nD. (speaking, talking)\n", "answer": "Explanations: In the context of inflectional morphology in the field of linguistics, inflectional morphology deals with the modification of words to express different grammatical categories such as tense, aspect, mood, person, number, case, and gender. Let's analyze the associations provided:\n\n1. (activate, action) - This pair does not depict inflectional morphology. \"Activate\" and \"action\" are different words that do not share a morphological relationship.\n   \n2. (hypothesis, hypotheses) - This association is a good example of inflectional morphology as \"hypothesis\" is the singular form and \"hypotheses\" is the plural form, showing the inflection for number.\n\n3. (to go, went) - This pair represents inflectional morphology as \"to go\" and \"went\" show the transformation of the verb \"go\" into its past tense form.\n\n4. (speaking, talking) - This pair does not exemplify inflectional morphology. \"Speaking\" and \"talking\" are different words with different roots and do not show a morphological relationship.\n\nCorrect assertions: (hypothesis, hypotheses), (to go, went) ///Answer: B:"}
{"subject": "", "question": "Question: Tick the non-commutative group.\n\nOptions:\nA. $\\mathbb{Z}_n^*$.\nB. $S_n$, the set of all permutations over the set $\\left\\{1,2,\\dots,n \\right\\}$.\nC. $E_{a,b}(K)$, an elliptic curve over a field $K$.\nD. $\\mathsf{GF}(2^k)^* $ .\n", "answer": "Explanations: The answer is: A\n\nChain of thought reasoning:\n- A non-commutative group is a group where the order in which elements are combined affects the result.\n- $\\mathbb{Z}_n^*$ is the set of integers modulo n that are coprime to n under multiplication. This is a non-commutative group because the order in which you multiply elements matters.\n- $S_n$, the set of all permutations over the set $\\left\\{1,2,\\dots,n \\right\\}$, is also non-commutative because the order of composition matters.\n- $E_{a,b}(K)$, an elliptic curve over a field $K$, forms a commutative group under point addition.\n- $\\mathsf{GF}(2^k)^*$ is the multiplicative group of a finite field, which is also commutative. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textit{correct} assertion.\n\nOptions:\nA. In a finite field $K$, every element has exactly two square roots.\nB. In a finite field $K$, 1 has exactly one square roots and it is 1.\nC. The set of quadratic residues in $\\mathbb{Z}_n$ is a field.\nD. An element can have more than two square roots in $\\mathbb{Z}_n$.\n", "answer": "Explanations: ##Answer:\nAn element can have more than two square roots in $\\mathbb{Z}_n$.\n\n##Reasons:\n1. In a finite field $K$, the number of square roots of an element can vary.\n2. While every non-zero element in a finite field $K$ has exactly two square roots (except 0, which has only one square root), in $\\mathbb{Z}_n$ (the integers modulo $n$), an element can have more than two square roots.\n3. For example, in $\\mathbb{Z}_{15}$, the element 4 has four square roots: 2, 4, 11, and 13, because $2^2 \\equiv 4^2 \\equiv 11^2 \\equiv 13^2 \\equiv 4 \\pmod{15}$.\n4. Therefore, the correct assertion is that an element can have more than two square roots in $\\mathbb{Z}_n$, whereas in a finite field, every non-zero element has exactly two square roots. ///Answer: D"}
{"subject": "", "question": "Question: What is a good distance metric to be used when you want to compute the similarity between documents independent of their length?A penalty will be applied for any incorrect answers.\n\nOptions:\nA. Cosine similarity\nB. Euclidean distance\nC. Manhattan distance\nD. Chi-squared distance\n", "answer": "Explanations: When computing the similarity between documents independent of their length, it is important to consider a distance metric that is not affected by the varying lengths of the documents. Cosine similarity is a commonly used distance metric for this purpose because it measures the cosine of the angle between two vectors, which is not influenced by the magnitudes of the vectors.\n\nEuclidean distance, Manhattan distance, and Chi-squared distance are not ideal for this scenario because they are affected by the absolute differences in the values of the vectors, which can be influenced by the lengths of the documents.\n\nTherefore, the most suitable distance metric for computing the similarity between documents independent of their length is Cosine similarity. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. Vernam is perfectly secure when \\ldots?\n\nOptions:\nA. \\ldots the key is at least as long as the message\nB. \\ldots a key is never used twice\nC. \\ldots the key is a random prime number\nD. \\ldots the key source is truly random\n", "answer": "Explanations: The false assertion is: \"\\ldots the key is a random prime number.\"\n\nExplanation: In the Vernam cipher, the key should be a random sequence of bits that is at least as long as the message to be encrypted. It does not necessarily have to be a prime number. The key being a prime number is not a requirement for the security of the Vernam cipher. ///Answer: C"}
{"subject": "", "question": "Question: What is the benefit of LDA over LSI?\n\nOptions:\nA. LSI is sensitive to the ordering of the words in a document, whereas LDA is not\nB. LDA has better theoretical explanation, and its empirical results are in general better than LSI\u2019s\nC. LSI is based on a model of how documents are generated, whereas LDA is not\nD. LDA represents semantic dimensions (topics, concepts) as weighted combinations of terms, whereas LSI does not\n", "answer": "Explanations: The benefit of Latent Dirichlet Allocation (LDA) over Latent Semantic Indexing (LSI) is that LDA represents semantic dimensions (topics, concepts) as weighted combinations of terms, whereas LSI does not. This means that LDA provides a more interpretable and intuitive representation of the underlying topics in a collection of documents. \n\nIn LSI, the representation is based on linear algebra techniques that capture the latent relationships between terms and documents through singular value decomposition. While this can be effective in capturing latent semantic information, the resulting dimensions may not directly correspond to interpretable topics or concepts.\n\nOn the other hand, LDA is a generative probabilistic model that assumes each document is a mixture of a set of topics, and each topic is a distribution over terms. This allows LDA to provide a clear and meaningful representation of the topics present in the document collection. Additionally, LDA has a solid theoretical foundation based on Bayesian statistics, which helps in better understanding the underlying assumptions and processes involved in topic modeling.\n\nTherefore, the correct statement is: LDA represents semantic dimensions (topics, concepts) as weighted combinations of terms, whereas LSI does not. ///Answer: D"}
{"subject": "", "question": "Question: Recall can be defined as:?\n\nOptions:\nA. P(relevant documents | retrieved documents)\nB. P(retrieved documents relevant documents)\nC. P(retrieved documents number of documents)\nD. P(relevant documents number of documents)\n", "answer": "Explanations: To define recall, we need to understand that it is a measure used in information retrieval and search engine evaluation. Recall is the proportion of relevant documents that were retrieved by a search engine or information retrieval system out of all the relevant documents available.\n\nThe correct definition of recall from the given options is: 1: \"P(retrieved documents relevant documents)\"\n\nTherefore, the answer to the multiple-choice question is: Recall can be defined as: \"P(retrieved documents relevant documents)\" ///Answer: B"}
{"subject": "", "question": "Question: The needed number of DES operations to attack DES is about ...\n\nOptions:\nA. $2^{54}.$\nB. $2^{64}.$\nC. $2^{128}.$\nD. $2^{256}.$\n", "answer": "Explanations: To attack DES, the needed number of operations is about $2^{56}$. This is because DES has a key size of 56 bits, so there are a total of $2^{56}$ possible keys that need to be tested in a brute force attack. This means that an attacker would need to try $2^{56}$ different keys in order to successfully break the encryption. Therefore, the correct answer is $2^{56}$. ///Answer: A"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. Pedersen Commitment is?\n\nOptions:\nA. unconditionally hiding.\nB. computationally binding.\nC. based on the hardness of the discrete logarithm problem.\nD. based on DSA.\n", "answer": "Explanations: To identify the incorrect statement regarding Pedersen Commitment, we need to understand the properties and foundations of Pedersen Commitment scheme.\n\n1. \"Unconditionally hiding\":\n   - Pedersen Commitment scheme is unconditionally hiding, meaning that without the knowledge of the opening (the committed value), a party cannot learn any information about the committed value. This property holds regardless of computational power.\n\n2. \"Computationally binding\":\n   - Pedersen Commitment scheme is computationally binding, implying that it is computationally infeasible for a party to alter the committed value after making the commitment without being detected, assuming the computational hardness of certain problems like the discrete logarithm problem.\n\n3. \"Based on the hardness of the discrete logarithm problem\":\n   - Pedersen Commitment scheme is indeed based on the hardness of the discrete logarithm problem. The security of Pedersen Commitment relies on the difficulty of computing discrete logarithms in certain groups.\n\n4. \"Based on DSA (Digital Signature Algorithm)\":\n   - This statement is incorrect. Pedersen Commitment is not based on DSA. It is a cryptographic primitive used for commitment schemes and cryptographic protocols, particularly in zero-knowledge proofs and secure multiparty computations. The scheme uses mathematical operations, such as modular exponentiation, and the hardness assumptions related to the discrete logarithm problem for its security.\n\nTherefore, the incorrect statement among the options provided is: \"based on DSA.\" ///Answer: D"}
{"subject": "", "question": "Question: Let $n$ be an integer. What is the cardinality of $\\mathbf{Z}^*_n$?\n\nOptions:\nA. $n$\nB. $n-1$\nC. $\\varphi(n)$\nD. $\\varphi(n-1)$\n", "answer": "Explanations: To determine the cardinality of $\\mathbf{Z}^*_n$, we need to understand that $\\mathbf{Z}^*_n$ represents the set of positive integers less than $n$ that are coprime to $n$. In other words, it represents the set of numbers between $1$ and $n$ that are relatively prime to $n$.\n\n1. $n$: This answer is incorrect. The cardinality of $\\mathbf{Z}^*_n$ is not equal to $n$. This is because not all numbers less than $n$ are coprime to $n$. For example, if $n = 10$, then $\\mathbf{Z}^*_n$ would include numbers such as $1, 3, 7, 9$, but not all numbers from $1$ to $10$.\n\n2. $n-1$: This answer is correct. The cardinality of $\\mathbf{Z}^*_n$ is $n-1$. This is because we are looking for the number of positive integers less than $n$ that are coprime to $n$. Since $1$ is always coprime to any integer $n$, we have $n-1$ other numbers between $1$ and $n$ that are coprime to $n$.\n\n3. $\\varphi(n)$: This answer is incorrect. The Euler's Totient function, $\\varphi(n)$, gives the number of positive integers less than $n$ that are coprime to $n$. However, it does not represent the cardinality of $\\mathbf{Z}^*_n$. The Euler's Totient function gives the number of positive integers less than $n$ that are coprime to $n$, but it does not include $n$ itself.\n\n4. $\\varphi(n-1)$: This answer is incorrect. The Euler's Totient function $\\varphi(n-1)$ does not represent the cardinality of $\\mathbf{Z}^*_n$. The correct representation for the cardinality is $n-1$ as explained in choice 2.\n\nTherefore, the correct answer is choice 2: $n-1$. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following is correct regarding community detection?\n\nOptions:\nA. High betweenness of an edge indicates that the communities are well connected by that edge\nB. The Louvain algorithm attempts to minimize the overall modularity measure of a community graph\nC. High modularity of a community indicates a large difference between the number of edges of the community and the number of edges of a null model\nD. The Girvan-Newman algorithm attempts to maximize the overall betweenness measure of a community graph\n", "answer": "Explanations: The correct statement regarding community detection is:\n\nHigh modularity of a community indicates a large difference between the number of edges of the community and the number of edges of a null model\n\nExplanation:\n1. High betweenness of an edge indicates that the communities are well connected by that edge - This statement is partially correct. High betweenness of an edge means that it is a bridge between communities, but it doesn't necessarily indicate that the communities are well connected by that edge. It means that the edge plays a significant role in connecting different parts of the network.\n\n2. The Louvain algorithm attempts to minimize the overall modularity measure of a community graph - This statement is incorrect. The Louvain algorithm is a popular community detection algorithm that aims to maximize the modularity of the network, not minimize it. Modularity measures the strength of division of a network into communities.\n\n3. High modularity of a community indicates a large difference between the number of edges of the community and the number of edges of a null model - This statement is correct. Modularity is a measure that quantifies the quality of a division of a network into communities. High modularity indicates that the division is good because there are more edges within communities compared to what would be expected in a random network (null model).\n\n4. The Girvan-Newman algorithm attempts to maximize the overall betweenness measure of a community graph - This statement is incorrect. The Girvan-Newman algorithm is a method for detecting communities in complex networks by iteratively removing edges with the highest betweenness centrality. It aims to split the network into communities by targeting edges with high betweenness, not to maximize the overall betweenness measure of a community graph. ///Answer: C"}
{"subject": "", "question": "Question: A query \\(q\\) has been submitted to two distinct Information Retrieval engines operating on the same document collection containing 1'000 documents, with 50 documents being truly relevant for \\(q\\).The following result lists have been produced by the two IR engines, \\(S_1\\) and \\(S_2\\) respectively:\n\n\\(S_1\\text{:}\\)\n\\(d_1\\)\n\\(d_2\\text{ (*)}\\)\n\\(d_3\\text{ (*)}\\)\n\\(d_4\\)\n\\(d_5\\text{ (*)}\\)\n\n\\(S_2\\text{:}\\)\n\\(d^\\prime_1\\text{ (*)}\\)\n\\(d^\\prime_2\\text{ (*)}\\)\n\\(d^\\prime_3\\)\n\\(d^\\prime_4\\)\n\\(d^\\prime_5\\)\n\nIn these result lists, the stars \\(\\text{(*)}\\) identify the truly relevant documents.\nBy convention, we consider that any non retrieved document has been retrieved at rank 6.If Average Precision is used as evaluation metric, which of the two IR engines is performing better for the query \\(q\\)?\n\nOptions:\nA. \\(S_1\\)\nB. \\(S_2\\)\nC. Both engines perform equally.\nD. This evaluation metric cannot be computed.\n", "answer": "Explanations: {\n\u00a0 \u00a0 \"explaination\": \"Average Precision is a metric used to evaluate Information Retrieval systems based on the precision of the retrieved relevant documents at different ranks. It considers the precision at each relevant document rank and calculates the average. In this case, since it is given that the non-retrieved documents are considered to be at rank 6, we can calculate the Average Precision for both IR engines based on the provided result lists.\",\n\u00a0 \u00a0 \"answer\": \"S1\"\n} ///Answer: A"}
{"subject": "", "question": "Question: Which of the following operation is not part of AES?\n\nOptions:\nA. AddRoundKey\nB. SubBytes\nC. MixColumns\nD. MulRoundKey\n", "answer": "Explanations: To determine which operation is not part of AES, we need to understand the different operations involved in the AES encryption process.\n\n1. AddRoundKey: This operation involves bitwise XORing the current state with the round key.\n\n2. SubBytes: This operation involves substituting each byte of the state with a corresponding byte from the S-box.\n\n3. MixColumns: This operation involves matrix multiplication of each column of the state with a fixed matrix.\n\n4. MulRoundKey: This operation is not a standard operation in the AES encryption process.\n\nTherefore, the operation \"MulRoundKey\" is not part of AES. ///Answer: D"}
{"subject": "", "question": "Question: In classification, which of the following is true regarding class imbalance?\n\nOptions:\nA. Classes should have the same distribution in the validation set and in the full dataset.\nB. Oversampling the larger class can reduce the impact of the skewed distribution.\nC. Oversampling rare classes in the testing set can reduce the impact of skewed distribution.\nD. The leave-one-out methodology produces the same class distribution in the training and the testing set.\n", "answer": "Explanations: Oversampling the larger class can reduce the impact of the skewed distribution. Class imbalance refers to when one class has significantly more instances than another class. Oversampling the larger class can help balance out the distribution and improve the model's performance. The other statements are false. Classes do not need to have the same distribution in the validation set and the full dataset. Oversampling rare classes in the testing set can introduce bias and is not recommended. The leave-one-out methodology does not guarantee the same class distribution in the training and testing set. ///Answer: B"}
{"subject": "", "question": "Question: Suppose that you can prove the security of your symmetric encryption scheme against the following attacks. In which case is your scheme going to be the \\textbf{most} secure?\n\nOptions:\nA. Key recovery under known plaintext attack.\nB. Key recovery under chosen ciphertext attack.\nC. Decryption under known plaintext attack.\nD. Decryption under chosen ciphertext attack.\n", "answer": "Explanations: My answer: 0) Key recovery under known plaintext attack. If your symmetric encryption scheme is secure against key recovery under a known plaintext attack, it is likely to be the most secure among the given options. This is because known plaintext attacks are considered powerful and practical in many real-world scenarios, and security against such attacks indicates a strong level of security for the encryption scheme. ///Answer: A"}
{"subject": "", "question": "Question: Select the \\emph{weakest} algorithm.\n\nOptions:\nA. A5/4\nB. A5/2\nC. A5/3\nD. A5/1\n", "answer": "Explanations: To determine the weakest algorithm among the options provided, we need to understand that the A5 algorithm is a stream cipher used in the GSM cellular phone standard for encrypting data. The strength of an algorithm is often associated with the number of encryption rounds it goes through, with a higher number of encryption rounds generally providing more robust security.\n\nHere are the steps to identify the weakest algorithm among the options given:\n\n1. Understanding the A5 stream cipher versions:\n   - A5/1: This is the original and weakest version of the A5 algorithm, which has known vulnerabilities and is relatively easy to break.\n   - A5/2: This version is slightly stronger than A5/1 but still considered weak compared to modern encryption standards.\n   - A5/3: A5/3 is stronger than A5/2 and A5/1. It was designed to address some of the weaknesses present in the earlier versions.\n   - A5/4: A5/4 is the strongest version among the A5 algorithms. It offers the highest level of security among the options provided.\n\n2. Determining the weakest algorithm:\n   - A5/1 is known to be the weakest among the A5 versions due to its vulnerabilities and ease of breaking.\n   - A5/2 is stronger than A5/1 but still considered weak.\n   - A5/3 is stronger than A5/2 and A5/1, making it a better choice in terms of security.\n   - A5/4 is the strongest and most secure option among the A5 algorithms.\n\nTherefore, the weakest algorithm among the options provided is 'A. A5/1'. ///Answer: A"}
{"subject": "", "question": "Question: ``In any finite group, the order of an element is a factor of the order of the group\" is stated by?\n\nOptions:\nA. the Lagrange Theorem.\nB. the Chinese Remainder Theorem.\nC. the Fermat's Little Theorem.\nD. the Fundamental Theorem of Calculus.\n", "answer": "Explanations: Answer: A) the Lagrange Theorem.\n\nExplanation: \n1. The Lagrange Theorem states that in any finite group, the order of a subgroup divides the order of the group. This means that the order of any element in a group must divide the order of the group itself.\n2. In this case, when we talk about the order of an element, we are referring to the smallest positive integer n such that \\( a^n = e \\), where a is an element of the group and e is the identity element. This order of the element is denoted as ord(a).\n3. Since the order of an element is the same as the order of the subgroup generated by that element, according to Lagrange's Theorem, the order of the subgroup must divide the order of the group.\n4. Therefore, in any finite group, the order of an element is indeed a factor of the order of the group, as stated in the Lagrange Theorem.\n5. The Chinese Remainder Theorem is a result in number theory that deals with simultaneous congruences. Fermat's Little Theorem is a result in number theory stating a property of prime numbers and modular arithmetic. The Fundamental Theorem of Calculus deals with integration in calculus and is not related to group theory. ///Answer: A"}
{"subject": "", "question": "Question: Let $X_1,X_2,\\dots$ be i.i.d. binary random variables with $p_{X_i}(1) =\frac{1}{4}$ for every $i\\geq 1$. Let $Y_1$ be a uniform binary random variable, and let $Y_i = Y_{i-1} \\oplus X_{i-1}$ for every $i\\geq 2$, where $\\oplus$ denotes the modulo-2 sum. For any given $n\\geq 1$, what is the value of $H(Y_1,Y_2,\\dots,Y_n)$? [Hint: what is the value of $H(Y_i|Y_1,\\dots,Y_{i-1})$?]?\n\nOptions:\nA. $\\left(2-\frac{3}{4}\\log 3\right) n + \frac{3}{4}\\log 3 - 1$.\nB. $n$.\nC. $\\left(2-\frac{3}{4}\\log 3\right) n + 1$.\nD. $\\left(3 - \frac{3}{4}\\log 3\right) n +\frac{3}{4}\\log 3 -2$.\n", "answer": "Explanations: \n{'message_id': 29262, 'chat_id': 10877, 'content': 'To find the value of $H(Y_1,Y_2,\\\\dots,Y_n)$, we need to calculate $H(Y_i|Y_1,\\\\dots,Y_{i-1})$ first.\\n\\nGiven that $Y_i = Y_{i-1} \\\\oplus X_{i-1}$, we can see that $Y_i$ depends on $Y_{i-1}$ and $X_{i-1}$. Since $X_i$ are i.i.d. binary random variables with $p_{X_i}(1) = \\\\frac{1}{4}$, we have:\\n\\n$P(Y_i = 0 | Y_{i-1} = y) = P(X_{i-1} = 0) = \\\\frac{3}{4}$\\n\\n$P(Y_i = 1 | Y_{i-1} = y) = P(X_{i-1} = 1) = \\\\frac{1}{4}$\\n\\nNow, we can calculate the conditional entropy $H(Y_i|Y_1,\\\\dots,Y_{i-1})$:\\n\\n$H(Y_i|Y_1,\\\\dots,Y_{i-1}) = -\\\\sum_{y_{i-1}}\\\\sum_{y_i} P(Y_i=y_i, Y_{i-1}=y_{i-1}) \\\\log P(Y_i=y_i|Y_{i-1}=y_{i-1})$\\n\\n$= -\\\\sum_{y_{i-1}}\\\\sum_{y_i} P(Y_i=y_i|Y_{i-1}=y_{i-1}) P(Y_{i-1}=y_{i-1}) \\\\log P(Y_i=y_i|Y_{i-1}=y_{i-1})$\\n\\n$= -\\\\sum_{y_{i-1}}\\\\left( P(Y_i=0|Y_{i-1}=y_{i-1})P(Y_{i-1}=y_{i-1})\\\\log P(Y_i=0|Y_{i-1}=y_{i-1}) + P(Y_i=1|Y_{i-1}=y_{i-1})P(Y_{i-1}=y_{i-1})\\\\log P(Y_i=1|Y_{i-1}=y_{i-1}) \\\\right)$\\n\\n$= -\\\\left( \\\\frac{3}{4}\\\\cdot 0 + \\\\frac{3}{4}\\\\cdot \\\\log \\\\frac{3}{4} + \\\\frac{1}{4}\\\\cdot 0 + \\\\frac{1}{4}\\\\cdot \\\\log \\\\frac{3}{4} \\\\right)$\\n\\n$= -\\\\left(\\\\frac{3}{4}\\\\log\\\\frac{3}{4} + \\\\frac{1}{4}\\\\log\\\\frac{3}{4}\\\\right)$\\n\\n$= -\\\\left(\\\\frac{3}{4}\\\\log 3 + \\\\frac{3}{4}\\\\log\\\\frac{3}{4}\\\\right)$\\n\\n$= -\\\\left(\\\\frac{3}{4}\\\\log 3 - \\\\frac{3}{4}\\\\log 4\\\\right)$\\n\\n$= -\\\\left(\\\\frac{3}{4}\\\\log 3 - \n\\\\frac{3}{4}\\\\cdot 2\\\\right)$\\n\\n$= -\\\\left(\\\\frac{3}{4}\\\\log 3 - \\\\frac{6}{4}\\\\right)$\\n\\n$= -\\\\left(\\\\frac{3}{4}\\\\log 3 - \\\\frac{3}{2}\\\\right)$\\n\\nNow, to find $H(Y_1,Y_2,\\\\dots,Y_n)$, we sum up the conditional entropies for each $Y_i$ from $i=2$ to $n$, and add the entropy \nof $Y_1$, which is a uniform binary random variable:\\n\\n$H(Y_2,\\\\dots,Y_n) = (n-2)\\\\cdot H(Y_i|Y_1,\\\\dots,Y_{i-2})$\\n\\nTherefore,\\n\\n$H(Y_2,\\\\dots,Y_n) = (n-2)\\\\cdot \\\\left(-\\\\left(\\\\frac{3}{4}\\\\log 3 - \\\\frac{3}{2}\\\\right)\\\\right)$\\n\\nFinally,\\n\\n$H(Y_1,Y_2,\\\\dots,Y_n) = H(Y_2,\\\\dots,Y_n) + H(Y_1)$\\n\\nSubstitute the values and simplify to get the final answer.' ///Answer: D"}
{"subject": "", "question": "Question: A simple substitution cipher can be broken \\dots?\n\nOptions:\nA. by analysing the probability occurence of the language.\nB. only by using a quantum computer.\nC. by using the ENIGMA machine.\nD. by using public-key cryptogaphy.\n", "answer": "Explanations: A simple substitution cipher can be broken by analyzing the probability occurrence of the language. This method involves looking at the frequency of letters or pairs of letters in the ciphertext and comparing them to the known frequency distribution of letters in the given language. By identifying common patterns and using frequency analysis techniques, one can decipher the substitution cipher without the need for advanced technologies like quantum computers, the ENIGMA machine, or public-key cryptography. ///Answer: A"}
{"subject": "", "question": "Question: Consider an RSA encryption where the public key is published as $(m, e) = (55, 17)$. Which one of the following numbers is a valid decoding exponent?\n\nOptions:\nA. $53$\nB. $23$\nC. $43$\nD. $83$\n", "answer": "Explanations: To determine the valid decoding exponent, we need to calculate the private key using the given public key $(m, e) = (55, 17)$. The private key is calculated as the modular multiplicative inverse of the public exponent $e$ modulo $(p-1)(q-1)$, where $p$ and $q$ are the prime factors of the modulus $m$.\n\nGiven $m = 55$, we need to find the prime factors of $m$:\n$55 = 5 \\times 11$\n\nNow, calculate $(p-1)(q-1)$:\n$(5-1)(11-1) = 4 \\times 10 = 40$\n\nNext, find the modular multiplicative inverse of $e = 17$ modulo $40$:\nWe need to find $d$ such that $17d \\equiv 1 \\pmod{40}$.\n\nBy trying out different values of $d$, we find that $d = 23$ satisfies the equation:\n$17 \\times 23 = 391 \\equiv 1 \\pmod{40}$\n\nTherefore, the valid decoding exponent is $23$. \n\nSo, the correct answer is option 2: $23$. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textit{incorrect} assertion.\n\nOptions:\nA. Plain CBCMAC resists forgery attacks.\nB. GCM is a blockcipher mode of operation that provides both confidentiality and authenticity for messages.\nC. A message authentication scheme that resists a chosen message forgery attack will also resist a known message forgery attack.\nD. HMAC is a message authentication code based on a hash function.\n", "answer": "Explanations: The incorrect assertion is: c) A message authentication scheme that resists a chosen message forgery attack will also resist a known message forgery attack.\n\nExplanation:\na) Plain CBCMAC does not resist forgery attacks because it is vulnerable to chosen message forgery attacks.\nb) GCM is a block cipher mode of operation that provides both confidentiality and authenticity for messages.\nc) This assertion is incorrect because a message authentication scheme that resists a chosen message forgery attack may not necessarily resist a known message forgery attack.\nd) HMAC is a message authentication code based on a hash function. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following is correct regarding schemas and ontologies?\n\nOptions:\nA. An ontology is created from constructing mappings between schemas\nB. Ontologies can be used for reasoning about different schemas\nC. Ontologies always require a schema\nD. Semi-structured data cannot have a schema\n", "answer": "Explanations: Let's think step-by-step:\n\n1. An ontology is created from constructing mappings between schemas: Incorrect.\n2. Ontologies can be used for reasoning about different schemas: Correct.\n3. Ontologies always require a schema: Incorrect.\n4. Semi-structured data cannot have a schema: Incorrect.\n\nTherefore, the correct statement is: **Ontologies can be used for reasoning about different schemas**. ///Answer: B"}
{"subject": "", "question": "Question: You are doing your ML project. It is a regression task under a square loss. Your neighbor uses linear regression and least squares. You are smarter. You are using a neural net with 10 layers and activations functions $f(x)=3 x$. You have a powerful laptop but not a supercomputer. You are betting your neighbor a beer at Satellite who will have a substantially better scores. However, at the end it will essentially be a tie, so we decide to have two beers and both pay. What is the reason for the outcome of this bet?\n\nOptions:\nA. Because we use exactly the same scheme.\nB. Because it is almost impossible to train a network with 10 layers without a supercomputer.\nC. Because I should have used more layers.\nD. Because I should have used only one layer.\n", "answer": "Explanations: 1) Because it is almost impossible to train a network with 10 layers without a supercomputer.\n\nTraining a neural network with 10 layers and activation functions $f(x) = 3x$ can be extremely challenging without a supercomputer due to the increased complexity and computational requirements associated with deep networks. The training process may be slow and may not converge effectively on a regular laptop, leading to suboptimal performance compared to linear regression with least squares. ///Answer: B"}
{"subject": "", "question": "Question: Compared to the plain RSA cryptosystem and for equivalent key sizes, the plain Elgamal cryptosystem has\\dots?\n\nOptions:\nA. a simpler key generation algorithm.\nB. a simpler encryption algorithm.\nC. a simpler decryption algorithm.\nD. shorter ciphertexts.\n", "answer": "Explanations: The correct answer is: 'a simpler decryption algorithm.'\n\nExplanation: The Elgamal cryptosystem is known for having a simpler decryption algorithm compared to the RSA cryptosystem. In Elgamal, the decryption process involves simple modular arithmetic operations, while in RSA, the decryption process includes more complex mathematical operations such as modular exponentiation. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. In Differential Cryptanalysis, the corresponding differential circuit of \\ldots?\n\nOptions:\nA. \\ldots a linear circuit ($Y=M\\times X$) is $\\Delta X=a\\Rightarrow \\Delta Y=^tM\\times a$\nB. \\ldots a duplicate gate ($X=Y=Z$) is $\\Delta X=a\\Rightarrow \\Delta Y = \\Delta Z = a$\nC. \\ldots a XOR gate ($X\\oplus Y = Z$) is $(\\Delta X=a,\\ \\Delta Y=b)\\Rightarrow \\Delta Z = a\\oplus b$\nD. \\ldots a XOR to constant gate ($Y=X\\oplus K$) is $\\Delta X = a \\Rightarrow \\Delta Y = a$\n", "answer": "Explanations: To solve this problem, we need to understand how the corresponding differential circuit behaves for different types of gates in Differential Cryptanalysis.\n\nLet's go through each assertion:\n\n(a) For a linear circuit ($Y=M\\times X$), the corresponding differential behavior is $\u0394X=a \\Rightarrow \u0394Y=^tM\\times a$. This assertion is true because in a linear circuit, the change in the input directly affects the output based on the matrix multiplication.\n\n(b) For a duplicate gate ($X=Y=Z$), the corresponding behavior is $\u0394X=a \\Rightarrow \u0394Y = \u0394Z = a$. This assertion is true because in a duplicate gate, any change in the input is duplicated to both outputs.\n\n(c) For an XOR gate ($X\\oplus Y = Z$), the corresponding behavior is $(\u0394X=a, \u0394Y=b) \\Rightarrow \u0394Z = a\\oplus b$. This assertion is also true because in an XOR gate, the change in inputs results in the XOR operation of those changes to produce the output change.\n\n(d) For an XOR to constant gate ($Y=X\\oplus K$), the corresponding behavior is $\u0394X = a \\Rightarrow \u0394Y = a$. This assertion is false. In an XOR to constant gate, changing the input $X$ by $a$ will not directly result in $\u0394Y = a$. The output $Y$ depends on both $X$ and the constant $K$, so changing only $X$ will not result in a direct change of $Y$ by the same amount.\n\nTherefore, the false assertion is:\n(d) ... a XOR to constant gate ($Y=X\\oplus K$) is $\u0394X = a \\Rightarrow \u0394Y = a$. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement regarding 3G.\n\nOptions:\nA. Confidentiality is preserved.\nB. Message integrity is preserved.\nC. Network is authenticated.\nD. It is secure against replay attacks.\n", "answer": "Explanations: The false statement regarding 3G is: \\textbf{It is secure against replay attacks.}\n\n3G (third-generation) mobile communication systems do not inherently provide protection against replay attacks. Replay attacks involve an attacker intercepting and retransmitting data packets to impersonate the original sender or manipulate the communication. Without proper security measures such as timestamping or sequence numbers, 3G networks are vulnerable to replay attacks. This vulnerability could lead to unauthorized access, data manipulation, or impersonation.\n\nThe implications of this vulnerability can be severe, as it can compromise the confidentiality and integrity of the communication, leading to unauthorized access to sensitive information, data manipulation, and potentially unauthorized network access. To mitigate this risk, additional cryptographic protocols or mechanisms such as message authentication codes (MACs) or digital signatures should be implemented to prevent replay attacks effectively. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following statements regarding distant supervision is correct?\n\nOptions:\nA. Distant supervision may suffer from semantic drift\nB. Distant supervision may help to detect complex rules to identify relationships\nC. Distant supervision starts out with hand-written patterns to avoid the use of labeled data\nD. Distant supervision requires manually labeled training data to train a classifier\n", "answer": "Explanations: The correct option is \"Distant supervision may help to detect complex rules to identify relationships\" because although distant supervision may suffer from semantic drift and may not start out with hand-written patterns, it does not necessarily require manually labeled training data to train a classifier. This option highlights one of the key benefits of distant supervision, which is its ability to detect complex rules for relationship extraction. ///Answer: B"}
{"subject": "", "question": "Question: Let $f(x, y)$ be a general function over $\\mathbb{R}^{2}$. Mark any of the following statements that is always (independent of the function) correct?\n\nOptions:\nA. $\\max _{y^{\\prime}} \\min _{x^{\\prime}} f\\left(x^{\\prime}, y^{\\prime}\\right) \\leq \\min _{x^{\\prime}} \\max _{y^{\\prime}} f\\left(x^{\\prime}, y^{\\prime}\\right)$\nB. $\\min _{x^{\\prime}} f\\left(x^{\\prime}, y\\right) \\leq \\min _{y^{\\prime}} f\\left(x, y^{\\prime}\\right), \\forall x, y$\nC. $\\max _{x^{\\prime}} f\\left(x^{\\prime}, y\\right) \\leq \\max _{y^{\\prime}} f\\left(x, y^{\\prime}\\right), \\forall x, y$\nD. $\\min _{x^{\\prime}} f\\left(x^{\\prime}, y\\right) \\leq \\max _{y^{\\prime}} f\\left(x, y^{\\prime}\\right), \\forall x, y$\nE. $\\min _{x^{\\prime}} f\\left(x^{\\prime}, y\\right) \\leq \\max _{x^{\\prime}} f\\left(x^{\\prime}, y\\right), \\forall y$\nF. $\\min _{y^{\\prime}} \\max _{x^{\\prime}} f\\left(x^{\\prime}, y^{\\prime}\\right) \\leq \\max _{x^{\\prime}} \\min _{y^{\\prime}} f\\left(x^{\\prime}, y^{\\prime}\\right)$\n", "answer": "Explanations: Let's analyze each option step by step:\n\nOption 0: This statement is always true due to the minimax theorem in game theory, which states that the min-max value of a game is always less than or equal to the max-min value.\n\nOption 1: This statement is not always true. Consider a function where $f(x, y) = x + y$. Then, for $x=1, y=2$, we have $\\min_{x'} f(x', y) = \\min_{x'} (x' + 2) = 2$, and $\\min_{y'} f(x, y') = \\min_{y'} (1 + y') = 1$. Therefore, $\\min_{x'} f(x', y) = 2 \\nleq 1$.\n\nOption 2: This statement is not always true. Consider the same function $f(x, y) = x + y$. For $x=1, y=2$, we have $\\max_{x'} f(x', y) = \\max_{x'} (x' + 2) = 3$, and $\\max_{y'} f(x, y') = \\max_{y'} (1 + y') = 3$. Therefore, $\\max_{x'} f(x', y) = 3 \\nleq 3$.\n\nOption 3: This statement is always true. This is because the minimum of a function over one variable is always less than or equal to the maximum of that function over another variable.\n\nOption 4: This statement is always true. The minimum value of a function over one variable is always less than or equal to the maximum value of that function over the same variable.\n\nOption 5: This statement is always true. This is a variant of the minimax theorem, stating that the min-max value is less than or equal to the max-min value.\n\nTherefore, the correct options are 0, 3, 4, and 5. ///Answer: A"}
{"subject": "", "question": "Question: Which method can be used for dimensionality reduction ?\n\nOptions:\nA. PCA\nB. T-distributed Stochastic Neighbor Embedding (t-SNE)\nC. Autoencoders\nD. SVM\n", "answer": "Explanations: Answer: \na) PCA (Principal Component Analysis) is the correct method for dimensionality reduction. PCA works by transforming the original data into a new set of orthogonal variables called principal components, which capture the maximum amount of variance in the data with fewer dimensions. This method is commonly used for reducing the dimensionality of data while retaining as much information as possible.\n\nb) T-distributed Stochastic Neighbor Embedding (t-SNE) is not typically used for dimensionality reduction, but rather for visualization of high-dimensional data in lower dimensions. It is useful for exploring the structure of the data, but not primarily for reducing dimensionality.\n\nc) Autoencoders are a type of neural network that can be used for dimensionality reduction by learning an efficient data representation from the input. While they can perform dimensionality reduction, they are not as commonly used for this purpose as PCA.\n\nd) SVM (Support Vector Machine) is a supervised machine learning algorithm used for classification and regression tasks, not specifically for dimensionality reduction. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following statements are true about command injection?\n\nOptions:\nA. The root cause of command injection is the lack of distinction\n            between data plane and control/code plane.\nB. Command injection allows the attacker to launch new processes or\n            invoke internal shell commands.\nC. To mitigate command injection, it is best to replace powerful\n            shell commands (e.g., \\texttt{system()}) with less privileged alternatives\n            such as \\texttt{read\\_file()}.\nD. Command injection can be mitigated, in general, by\n            prohibiting the \"\\texttt{;}\" character.\nE. Command injection is unique to PHP web applications.\n", "answer": "Explanations: Start with analyzing the statements regarding command injection carefully to ensure my answers are accurate and distinct. Let's break each statement down and provide alternative answers to ensure diverse responses that are relevant:\n\n1. True: Command injection indeed allows attackers to execute arbitrary commands, which can lead to unauthorized access and other malicious activities. For added detail, focus on how command injection poses a significant security risk due to its potential impact on system integrity. \n2. False: Instead of focusing on replacing specific commands, emphasize the importance of input validation, output encoding, and parameterized queries as core prevention techniques against command injection attacks. Mention how using white-list validation for input data and adopting strict constraints help mitigate risks effectively. \n3. False: Instead of merely mentioning the prohibition of specific characters like \";\" as a mitigation strategy, highlight the importance of implementing proper input sanitization and context-aware output encoding to prevent command injection vulnerabilities comprehensively. Discuss the significance of input validation routines that check for unexpected characters or patterns in user-supplied data. \n4. False: Instead of stating that command injection is unique to PHP web applications, illustrate how command injection vulnerabilities can affect a wide range of applications, frameworks, and programming languages, emphasizing the need for secure coding practices across the board. Highlight common attack vectors and potential security measures to prevent command injection vulnerabilities. \n\nThis approach ensures that the answers provided are not repetitive and offer diversified insights on command injection and its mitigation strategies. ///Answer: B"}
{"subject": "", "question": "Question: Using a 4-gram character model, comparing \"banana\" and \"ananas\"...\n\nOptions:\nA. is the same as comparing \"aaabnn\" to \"aaanns\"\nB. is the same as comparing P(bana) to P(anas)\nC. is the same as comparing P(bana) to P(anan)\nD. is the same as comparing P(ban)/P(an) to P(nas)/P(na)\nE. None of the others\n", "answer": "Explanations: Answer: C) is the same as comparing P(bana) to P(anan)\n\nExplanation:\nIn a 4-gram character model, we are looking at sequences of 4 characters at a time. \n\nFor the word \"banana\":\n- The 4-grams would be \"bana\", \"anan\", and \"nana\".\n\nFor the word \"ananas\":\n- The 4-grams would be \"anan\", \"nana\", and \"anas\".\n\nWhen comparing the two words using a 4-gram character model, we are comparing the probabilities of each 4-gram occurring in the words.\n\nTherefore, comparing \"banana\" and \"ananas\" is the same as comparing the probabilities of the 4-gram \"bana\" in \"banana\" to the 4-gram \"anan\" in \"ananas\". \n\nHence, the correct answer is C) is the same as comparing P(bana) to P(anan). ///Answer: C"}
{"subject": "", "question": "Question: Consider the language $L_{\\text{best}}=\\{\\text{``Advanced crypto is great!''}^i \\mid i \\in \\mathbb{N}\\}$. Tick an assertion.\n\nOptions:\nA. $L_{\\text{best}} \\in \\mathcal{P}$.\nB. Every $x\\in L_{\\text{best}}$ has a logical value equal to $\\mathsf{True}$. The bigger $|x|$, the more true it is.\nC. Only Chuck Norris can recognize $L_{\\text{best}}$ in constant time. But he can recognize any language in constant time.\nD. $L_{\\text{best}}$ is a very very good language. We have never seen any more bester language. It's huge.\n", "answer": "Explanations: The correct option is: $L_{\\text{best}}$ is a very very good language. We have never seen any more bester language. It's huge.\n\nExplanation:\n\nThe language $L_{\\text{best}}$ consists of strings that are repetitions of the phrase \"Advanced crypto is great!\" for any number of times ($i \\in \\mathbb{N}$). This language is a simple, repetitive language with a specific fixed phrase being repeated infinitely.\n\nWhile it may be humorous to describe $L_{\\text{best}}$ as a very good language, the reality is that $L_{\\text{best}}$ is a trivial language with a very clear and straightforward pattern. The language does not pose any computational complexity or recognition challenges. Chuck Norris recognizing the language in constant time is an exaggeration for humor, but in reality, $L_{\\text{best}}$ can be easily recognized and understood by anyone due to its repetitive nature. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following statements about coverage-guided fuzzing is/are correct?\n\nOptions:\nA. Redundant seeds in the corpus will reduce fuzzing efficiency.\nB. Counting the number of times the covered code has been executed\n            provides a more fine-grained view of program behavior than only\n            \"covered/not covered\" binary code coverage.\nC. Due to the coverage feedback, a small random perturbation of a\n            seed can have a significant impact on further exploration.\nD. Fuzzers that have higher code coverage always find more\n            bugs.\n", "answer": "Explanations: Redundant seeds in the corpus will reduce fuzzing efficiency; Counting the number of times the covered code has been executed provides a more fine-grained view of program behavior than only \"covered/not covered\" binary code coverage; Due to the coverage feedback, a small random perturbation of a seed can have a significant impact on further exploration.\n\nThe statement that counting the number of times the covered code has been executed provides a more fine-grained view of program behavior than only \"covered/not covered\" binary code coverage is correct. This is because simply knowing whether a piece of code has been executed or not is not sufficient to understand the behavior of the program. By counting the number of times the code has been executed, fuzzers can gain a deeper insight into how different parts of the program are being interacted with, allowing for more targeted testing.\n\nAdditionally, the statement that due to the coverage feedback, a small random perturbation of a seed can have a significant impact on further exploration is also correct. This is because coverage-guided fuzzing relies on feedback from the execution of the program to guide the generation of new inputs. By making small random perturbations to seeds that have already been explored, fuzzers can potentially uncover new paths and edge cases in the program, leading to the discovery of more bugs. ///Answer: A"}
{"subject": "", "question": "Question: Consider a binary classification problem with classifier $f(\\mathbf{x})$ given by $$ f(\\mathbf{x})= \\begin{cases}1, & g(\\mathbf{x}) \\geq 0 \\\\ -1, & g(\\mathbf{x})<0\\end{cases} $$ and $\\mathbf{x} \\in \\mathbb{R}^{6}$. Consider a specific pair $(\\mathbf{x}, y=1)$ and assume that $g(\\mathbf{x})=8$. In particular this means that this point is classified correctly by $f$. Assume further that we have computed the gradient of $g$ at $\\mathbf{x}$ to be $\\nabla_{\\mathbf{x}} g(\\mathbf{x})=(+1,-2,+3,-4,+5,-6)$. You are allowed to make one step in order to (hopefully) find an adversarial example. In the following four questions, assume $\\epsilon=1$. What is the value of $g(\\mathbf{x}+\\delta)$ for this $\\ell_{\\infty}$-optimal choice assuming that $g$ is (locally) linear?\n\nOptions:\nA. $-5$\nB. $-2$\nC. $-7$\nD. $+7$\nE. $4$\nF. $0$\nG. $+13$\nH. $2$\nI. $-4$\nJ. $-13$\n", "answer": "Explanations: The value of g(\ud835\udc31+\u03b4) for the \u2113_\u221e-optimal choice assuming that g is locally linear is 7. \n\nJustification:\nWe need to calculate \ud835\udc31 + \u03b4, where \u03b4 is in the direction of -\u2207_\ud835\udc31 g(\ud835\udc31) = (-1, +2, -3, +4, -5, +6). Adding this perturbation to \ud835\udc31, we get: \n\n    \ud835\udc31 + \u03b4 = (x_1 - 1, x_2 + 2, x_3 - 3, x_4 + 4, x_5 - 5, x_6 + 6)\n\nGiven that \ud835\udc31 is correctly classified with g(\ud835\udc31) = 8, we can plug in the values of \ud835\udc31 into g to get:\n\n    g(\ud835\udc31 + \u03b4) = 1(-1) + 2(+2) + 3(-3) + 4(+4) + 5(-5) + 6(+6) = -1 + 4 - 9 + 16 - 25 + 36 = 21\n\nTherefore, the value of g(\ud835\udc31 + \u03b4) is 21, which is not a negative value. Hence, the correct answer is not 7, but rather +,7,. ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textbf{minimal} requirements on a channel for performing a Diffie-Hellman key exchange securely. The channel must\\dots?\n\nOptions:\nA. be authenticated, confidential and protect the integrity.\nB. protect the integrity.\nC. be confidential.\nD. be authenticated and protect the integrity.\n", "answer": "Explanations: The minimal requirements on a channel for performing a Diffie-Hellman key exchange securely are that the channel must be authenticated and protect the integrity. This ensures that the parties involved in the key exchange are who they claim to be, and that the exchanged key remains unchanged during transmission. ///Answer: D"}
{"subject": "", "question": "Question: If x is input variables and y are output predictions, what is the most useful setup to predict optimal traffic lights control well in advance: \n(one answer)?\n\nOptions:\nA.   x: \\{weather, time, date, accidents, constructions, bus timetable\\}\\\\  y: \\{vehicle density, pedestrian density, bike density\\} \nB.   x: \\{pedestrian density, bike density\\}\\\\  y: \\{vehicle density\\} \nC.   x: \\{vehicle density, pedestrian density, bike density\\}\\\\  y: \\{bus timetable\\}  \nD.   x: \\{weather, time, date, pedestrian density, bike density \\}\\\\  y: \\{vehicle density, accidents, constructions,\\}  \n", "answer": "Explanations: To predict optimal traffic light control well in advance, the most useful setup would likely be option A: \nA:   x: \\{weather, time, date, accidents, constructions, bus timetable\\}\\\\  y: \\{vehicle density, pedestrian density, bike density\\}\n\nThis setup includes a comprehensive range of input variables that can have a significant impact on traffic conditions, such as weather, time, accidents, and bus timetable. By considering these factors along with output predictions related to vehicle, pedestrian, and bike density, the model can better anticipate traffic patterns and optimize traffic light control accordingly. ///Answer: A"}
{"subject": "", "question": "Question: Your input is \"Distributed Information Systems\". Your model tries to predict \"Distributed\" and \"Systems\" by leveraging the fact that these words are in the neighborhood of \"Information\". This model can be:?\n\nOptions:\nA. Bag of Words\nB. Word Embeddings\nC. LDA\nD. kNN\n", "answer": "Explanations: The correct response is \"Word Embeddings.\" Word embeddings are a type of distributed representation that captures the semantic relationships between words by mapping them to dense vectors in a continuous vector space. This allows the model to understand the context and meaning of words based on their proximity to each other in the embedding space. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements regarding random forest classifiers is false?\n\nOptions:\nA. Feature bagging has the effect that individual decision trees of a random forest classifier are smaller than a standard decision tree on the same data\nB. Using bootstrapping while training random forests guarantees that all the training data is used during the training phase\nC. Training a random forest classifier can be easily parallelized\nD. Majority voting is an effective way of learning a new classifier based on the predictions of many weak classifiers\n", "answer": "Explanations: The first statement is true because feature bagging in random forest classifiers involves selecting a random subset of features for each tree, which results in smaller individual decision trees compared to a standard decision tree on the same data. \n\nThe second statement is false because using bootstrapping (sampling with replacement) while training random forests does not guarantee that all the training data is used during the training phase. \n\nThe third statement is true because training a random forest classifier can be easily parallelized, as each tree in the forest can be trained independently. \n\nThe fourth statement is true because majority voting, where the final prediction is based on the majority vote of all the individual trees in the random forest, is an effective way of learning a new classifier based on the predictions of many weak classifiers. \n\nTherefore, the false statement is the second one: \"Using bootstrapping while training random forests guarantees that all the training data is used during the training phase.\"\nThe answer to the question is: Using bootstrapping while training random forests guarantees that all the training data is used during the training phase ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements about testing is/are correct?\n\nOptions:\nA. Concolic execution specializes in finding bugs close to the path\n            of the provided concrete input.\nB. In static analysis, determining the correct target set of\n            indirect calls is typically challenging but required to minimize\n            over-approximation of targets.\nC. Compared to dynamic analysis, static analysis is more\n            susceptible to state space explosion.\nD. Tests prove the presence of bugs but not their absence.\nE. Tests prove the absence of bugs but not their presence.\n", "answer": "Explanations: A: Correct. Concolic execution is a hybrid testing approach that combines concrete execution with symbolic execution to explore paths in the program. It is particularly effective in finding bugs close to the path of the provided concrete input.\n\nB: Correct. In static analysis, determining the correct target set of indirect calls can be challenging because the analysis is done without actually executing the program. This is crucial to minimize over-approximation of targets and improve the accuracy of the analysis.\n\nC: Incorrect. Static analysis is actually less susceptible to state space explosion compared to dynamic analysis. Static analysis examines the code without executing it, which can be more efficient in terms of memory and time compared to dynamic analysis.\n\nD: Correct. Tests can help identify the presence of bugs in the code by revealing failures or unexpected behavior. However, passing a test does not guarantee the absence of bugs in the code.\n\nE: Incorrect. Tests cannot prove the absence of bugs in the code. Passing a test only shows that the code behaves as expected under the specific test conditions. The presence of bugs can still exist in untested parts of the code. ///Answer: A"}
{"subject": "", "question": "Question: Consider our standard least-squares problem $$ \\operatorname{argmin}_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w})=\\operatorname{argmin}_{\\mathbf{w}} \\frac{1}{2} \\sum_{n=1}^{N}\\left(y_{n}-\\mathbf{x}_{n}^{\\top} \\mathbf{w}\\right)^{2}+\\frac{\\lambda}{2} \\sum_{d=1}^{D} w_{d}^{2} $$ Here, $\\left\\{\\left(\\mathbf{x}_{n} y_{n}\\right)\\right\\}_{n=1}^{N}$ is the data. The $N$-length vector of outputs is denoted by $\\mathbf{y}$. The $N \\times D$ data matrix is called $\\mathbf{X}$. It's rows contain the tuples $\\mathbf{x}_{n}$. Finally, the parameter vector of length $D$ is called $\\mathbf{w}$. (All just like we defined in the course). Mark any of the following formulas that represent an equivalent way of solving this problem.\n\nOptions:\nA. $\\operatorname{argmin}_{\\boldsymbol{\\alpha}} \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top}\\left(\\mathbf{X X}^{\\top}+\\lambda \\mathbf{I}_{N}\\right) \\boldsymbol{\\alpha}-\\boldsymbol{\\alpha}^{\\top} \\mathbf{y}$\nB. $\\operatorname{argmin}_{\\mathbf{w}} \\sum_{n=1}^{N}\\left[1-y_{n} \\mathbf{x}_{n}^{\\top} \\mathbf{w}\\right]_{+}+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$. Recall: $[z]_{+}=\\max \\{0, z\\}$\nC. $\\operatorname{argmin}_{\\mathbf{w}}-\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) p(\\mathbf{w})$, where $p(\\mathbf{w})$ correspond to the density of a $D$-length vector of iid zero-mean Gaussians with variance $1 / \\lambda$ and $p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w})$ corresponds to the density of a vector of length $N$ of independent Gaussians of mean $\\mathbf{x}_{n}^{\\top} \\mathbf{w}$, variance 1 and observation $\\mathbf{y}_{n}$ for component $n$.\nD. $\\square \\operatorname{argmin}_{\\mathbf{w}} \\frac{1}{2} \\sum_{n=1}^{N} \\ln \\left(1+e^{\\mathbf{x}_{n}^{\\top} \\mathbf{w}}\\right)-y_{n} \\mathbf{x}_{n}^{\\top} \\mathbf{w}$\nE. $\\operatorname{argmin}_{\\mathbf{w}} \\frac{1}{2}\\|\\mathbf{y}-\\mathbf{X} \\mathbf{w}\\|^{2}+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$\n", "answer": "Explanations: Let's analyze each formula provided in relation to the standard least-squares problem to determine if they represent an equivalent way of solving the problem:\n\n1. $\\operatorname{argmin}_{\\boldsymbol{\\alpha}} \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top}\\left(\\mathbf{X X}^{\\top}+\\lambda \\mathbf{I}_{N}\\right) \\boldsymbol{\\alpha}-\\boldsymbol{\\alpha}^{\\top} \\mathbf{y}$:\n- By substituting $\\boldsymbol{\\alpha} = \\mathbf{X}^{\\top} \\mathbf{w}$, we can rewrite this formula in terms of $\\mathbf{w}$: \n$\\frac{1}{2} \\left(\\mathbf{X}^{\\top} \\mathbf{w}\\right)^{\\top}\\left(\\mathbf{X X}^{\\top}+\\lambda \\mathbf{I}_{N}\\right) \\left(\\mathbf{X}^{\\top} \\mathbf{w}\\right) - \\left(\\mathbf{X}^{\\top} \\mathbf{w}\\right)^{\\top} \\mathbf{y}$\n- Simplifying this expression, we arrive at:\n$\\frac{1}{2} \\mathbf{w}^{\\top} (\\mathbf{X X}^{\\top} + \\lambda \\mathbf{I}_{D}) \\mathbf{w} - \\mathbf{w}^{\\top} \\mathbf{X y}$\n- This formula is equivalent to the standard least-squares problem with L2 regularization. Marked as equivalent.\n\n2. $\\operatorname{argmin}_{\\mathbf{w}} \\sum_{n=1}^{N}\\left[1-y_{n} \\mathbf{x}_{n}^{\\top} \\mathbf{w}\\right]_{+}+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$:\n- This formula involves a hinge loss term $\\left[1-y_{n} \\mathbf{x}_{n}^{\\top} \\mathbf{w}\\right]_{+}$ and an L2 regularization term.\n- The hinge loss introduces a non-smooth component not present in the original least-squares problem.\n- Therefore, this formula is not equivalent to the standard least-squares problem. Marked as not equivalent.\n\n3. $\\operatorname{argmin}_{\\mathbf{w}}-\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) p(\\mathbf{w})$:\n- This formula involves maximizing the likelihood of $\\mathbf{y}$ given $\\mathbf{X}$ and $\\mathbf{w}$ and the prior density $p(\\mathbf{w})$ using negative log likelihood.\n- The likelihood and prior are specified as Gaussian distributions.\n- This formulation is consistent with the Bayesian interpretation of linear regression with Gaussian priors.\n- It is equivalent to solving the original least-squares problem with L2 regularization. Marked as equivalent.\n\n4. $\\operatorname{argmin}_{\\mathbf{w}} \\frac{1}{2} \\sum_{n=1}^{N} \\ln \\left(1+e^{\\mathbf{x}_{n}^{\\top} \\mathbf{w}}\\right)-y_{n} \\mathbf{x}_{n}^{\\top} \\mathbf{w}$:\n- This formula involves a logistic loss term and a linear term.\n- The presence of logistic loss makes this formula not equivalent to the original least-squares problem.\n- Therefore, this formula is not equivalent to the standard least-squares problem. Marked as not equivalent.\n\n5. $\\operatorname{argmin}_{\\mathbf{w}} \\frac{1}{2}\\|\\mathbf{y}-\\mathbf{X} \\mathbf{w}\\|^{2}+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$:\n- This formula represents the standard least-squares problem with L2 regularization.\n- It directly corresponds to the original objective function.\n- Therefore, this formula is equivalent to the standard least-squares problem. Marked as equivalent.\n\nIn conclusion, the formulas that represent an equivalent way of solving the standard least-squares problem are:\n- $\\operatorname{argmin}_{\\boldsymbol{\\alpha}} \\frac{1}{2} \\boldsymbol{\\alpha}^{\\top}\\left(\\mathbf{X X}^{\\top}+\\lambda \\mathbf{I}_{N}\\right) \\boldsymbol{\\alpha}-\\boldsymbol{\\alpha}^{\\top} \\mathbf{y}$\n- $\\operatorname{argmin}_{\\mathbf{w}}-\\log p(\\mathbf{y} \\mid \\mathbf{X}, \\mathbf{w}) p(\\mathbf{w})$\n- $\\operatorname{argmin}_{\\mathbf{w}} \\frac{1}{2}\\|\\mathbf{y}-\\mathbf{X} \\mathbf{w}\\|^{2}+\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$\n\nThese three formulations are equivalent ways of solving the standard least-squares problem with L2 regularization. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion regarding the security of the Diffie-Hellman key exchange over a subgroup $\\langle g \\rangle \\subset \\mathbb{Z}_p^*$.\n\nOptions:\nA. $\\langle g \\rangle$ should have prime order.\nB. We must ensure that $X\\in \\langle g \\rangle$ for every received $X$.\nC. The binary representation of the output of the key exchange is a uniformly distributed bitstring.\nD. We must ensure that $X\\neq1$ for every received $X$.\n", "answer": "Explanations: Let's evaluate each assertion:\n\n1) $\\langle g \\rangle$ should have prime order.\n- This statement is correct. To ensure the security of the Diffie-Hellman key exchange, the subgroup $\\langle g \\rangle$ should have prime order.\n\n2) We must ensure that $X\\in \\langle g \\rangle$ for every received $X$.\n- This statement is correct. It is essential to ensure that the received value $X$ belongs to the subgroup $\\langle g \\rangle$ to maintain the security of the key exchange.\n\n3) The binary representation of the output of the key exchange is a uniformly distributed bitstring.\n- This statement is incorrect. The output of the Diffie-Hellman key exchange is not a uniformly distributed bitstring. The shared secret key generated through the key exchange process is a value in the subgroup $\\langle g \\rangle$, which is not uniformly distributed.\n\n4) We must ensure that $X\\neq 1$ for every received $X$.\n- This statement is correct. To maintain security, it is crucial to avoid using $X=1$ in the key exchange process.\n\nTherefore, the incorrect assertion regarding the security of the Diffie-Hellman key exchange over a subgroup $\\langle g \\rangle \\subset \\mathbb{Z}_p^*$ is:\n3) The binary representation of the output of the key exchange is a uniformly distributed bitstring. ///Answer: C"}
{"subject": "", "question": "Question: One of the following ciphers is a \\emph{block} cipher. Which one?\n\nOptions:\nA. AES\nB. RC4\nC. A5/1\nD. MD5\n", "answer": "Explanations: AES\n\n- AES is a block cipher.\n- RC4 is a stream cipher.\n- A5/1 is a stream cipher.\n- MD5 is a cryptographic hash function. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following statements about index merging (when constructing inverted files) is correct?\n\nOptions:\nA. While merging two partial indices on disk, the inverted lists of a term are concatenated without sorting\nB. Index merging is used when the vocabulary does no longer fit into the main memory\nC. The size of the final merged index file is O (n log2 (n) M )), where M is the size of the available memory\nD. While merging two partial indices on disk, the vocabularies are concatenated without sorting\n", "answer": "Explanations: The correct statement is: While merging two partial indices on disk, the inverted lists of a term are concatenated without sorting.\n\nExplanation:\nIndex merging is a process used in information retrieval systems to combine multiple partial indices into a single index file. When merging two partial indices on disk, the inverted lists of a term are concatenated without sorting to speed up the merging process. Sorting can be a time-consuming operation, especially when dealing with large volumes of data, so concatenation without sorting is often used to improve efficiency.\n\nThe other statements are not accurate:\n- Index merging is not specifically used when the vocabulary does not fit into main memory. It is used to combine partial indices, regardless of memory constraints.\n- The size of the final merged index file is not necessarily O(n log2(n) M), where M is the size of available memory. The size of the final index file depends on various factors such as the size of the input indices, the number of unique terms, and the posting list lengths.\n- Concatenating vocabularies without sorting is not a common practice in index merging. The focus is typically on merging the inverted lists of terms efficiently. Sorting is often necessary in the construction of a final index to ensure fast retrieval and search performance. ///Answer: A"}
{"subject": "", "question": "Question: Let $G$ be a set and $*$ a commutative operation on pairs of elements from $G.$ Suppose there exists an element $e\\in G$ such that $a*e=e*a=a$ for all $a \\in G.$ Also, suppose there exist elements $b,c,d \\in G$ such that $b*c=d*c.$. True or false: If $b\not=d,$ then $(G,*)$ cannot be a group.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: To determine if $(G,*)$ can be a group, we need to check the group properties:\n1. Closure: Since $*$ is a binary operation on $G,$ the closure property holds.\n2. Associativity: Since $*$ is an operation on $G,$ we can assume associativity holds.\n3. Identity: There exists an element $e\\in G$ such that $a*e=e*a=a$ for all $a \\in G.$\n4. Inverses: For each element $a \\in G,$ there exists an element $a'$ such that $a*a'=a'*a=e.$\n\nNow, let's examine the condition given in the question: $b*c=d*c,$ where $b\\neq d.$\n\nIf $b*c=d*c,$ then by left multiplying both sides by $c',$ where $c'$ is the inverse of $c$:\n$$c'*b*c=c'*d*c,$$\n$$b=c'*d*c.$$\n\nSo, $b=d$ which contradicts the assumption $b\\neq d.$\n\nTherefore, the statement in the question is **true.** If $b\\neq d$ but $b*c=d*c,$ then $(G,*)$ cannot be a group. ///Answer: B"}
{"subject": "", "question": "Question: What is the gradient of $\\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x}$ with respect to $\\mathbf{x}$ (written as a vector)?\n\nOptions:\nA. (a) $\\mathbf{W} \\mathbf{x}$\nB. (b) $\\mathbf{W}^{\\top} \\mathbf{x}$\nC. (c) $\\square\\left(\\mathbf{W}+\\mathbf{W}^{\\top}\\right) \\mathbf{x}$\nD. (d) $\\mathbf{W}$\nE. (e) $\\mathbf{x x}^{\\top}$.\nF. (f) $\\mathbf{x}^{\\top} \\mathbf{x}$\nG. (g) $\\mathbf{W} \\mathbf{W}^{\\top}$.\n", "answer": "Explanations: To find the gradient of the expression $\\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x}$ with respect to $\\mathbf{x}$, we differentiate the expression with respect to each component of $\\mathbf{x}$.\n\nGiven $\\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x}$, where $\\mathbf{x}$ is a vector and $\\mathbf{W}$ is a matrix, let's denote $\\mathbf{y} = \\mathbf{W}\\mathbf{x}$ for simplicity.\n\nThen, the expression becomes $\\mathbf{x}^{\\top} \\mathbf{y}$.\n\nDifferentiating with respect to $\\mathbf{x}$, we get:\n\n$\\nabla_{\\mathbf{x}}(\\mathbf{x}^{\\top} \\mathbf{y}) = \\nabla_{\\mathbf{x}}(\\mathbf{x}^{\\top} \\mathbf{W}\\mathbf{x}) $\n\n$= \\nabla_{\\mathbf{x}}(\\mathbf{x}^{\\top} \\mathbf{W}\\mathbf{x}) $\n\n$= \\nabla_{\\mathbf{x}}(\\mathbf{y}^{\\top}\\mathbf{x}) $\n\n$= \\nabla_{\\mathbf{x}}(\\mathbf{y}^{\\top}) \\mathbf{x} + \\mathbf{y} $\n\n$= \\mathbf{W}^{\\top} \\mathbf{x} + \\mathbf{W} \\mathbf{x}$\n\nTherefore, the correct gradient of $\\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x}$ with respect to $\\mathbf{x}$ is $\\mathbf{W}^{\\top} \\mathbf{x} + \\mathbf{W} \\mathbf{x}$, which is equivalent to option (c): $\\square\\left(\\mathbf{W}+\\mathbf{W}^{\\top}\\right) \\mathbf{x}$. ///Answer: C"}
{"subject": "", "question": "Question: Which is an appropriate method for fighting skewed distributions of class labels in classification?\n\nOptions:\nA. Include an over-proportional number of samples from the larger class\nB. Use leave-one-out cross validation\nC. Construct the validation set such that the class label distribution approximately matches the global distribution of the class labels\nD. Generate artificial data points for the most frequent classes\n", "answer": "Explanations: The best way to address skewed distributions of class labels in classification is to generate artificial data points for the most frequent classes. This method helps balance out the number of samples in each class, making the model more accurate in predicting all classes. ///Answer: D"}
{"subject": "", "question": "Question: Select the morpho-syntactic categories that do not carry much semantic content and are thus usually filtered-out from indexing.\n\nOptions:\nA. Determiners\u00a0\nB. Conjunctions\nC. Nouns\nD. Adjectives\nE. Verbs\n", "answer": "Explanations: ##Answer:\nDeterminers, Conjunctions\n\n##Reasons:\n1. **Determiners**:\n   - Determiners are words that precede nouns to provide context or clarification. Examples include articles like \"the,\" \"a,\" \"an,\" demonstratives like \"this,\" \"that,\" possessives like \"my,\" \"your,\" quantifiers like \"some,\" \"many.\" \n   - Determiners do not carry much semantic content on their own and mainly serve grammatical functions. They are used to specify or limit the reference of a noun but do not add significant meaning to the overall text. Therefore, determiners are often filtered out from indexing to focus on more meaningful content.\n\n2. **Conjunctions**:\n   - Conjunctions are words used to connect words, phrases, or clauses. Examples include \"and,\" \"but,\" \"or,\" \"if,\" \"because.\" \n   - Conjunctions primarily serve to establish relationships between different parts of a sentence or text but do not contribute directly to the semantic content. They are important for sentence structure and coherence but are not typically indexed for semantic analysis.\n   \n3. **Exclusion from Indexing**:\n   - In natural language processing tasks such as text indexing or information retrieval, the focus is on identifying and analyzing meaningful content that conveys key information or context. Determiners and conjunctions, being primarily structural elements, are less relevant for semantic analysis and are often filtered out during indexing to prioritize content words that carry more substantial semantic meaning.\n   - By excluding determiners and conjunctions from indexing, the system can reduce noise, improve efficiency, and focus on indexing the essential content words like nouns, adjectives, and verbs that provide more semantic value to the text data. ///Answer: A"}
{"subject": "", "question": "Question: In an x86 multiprocessor system with JOS, select all the correct options. Assume every Env has a single thread.\n\nOptions:\nA. Two Envs could run on the same processor simultaneously.\nB. Two Envs could run on two different processors simultaneously.\nC. One Env could run on two different processors simultaneously.\nD. One Env could run on two different processors at different times.\n", "answer": "Explanations: The correct option is: 'Two Envs could run on two different processors simultaneously.'\n\nIn an x86 multiprocessor system, each processor has its own set of registers and can independently execute instructions. This means that multiple environments (Envs) can run concurrently on different processors without interfering with each other. Therefore, it is possible for two Envs to run on two different processors simultaneously in a multiprocessor system. ///Answer: B"}
{"subject": "", "question": "Question: Consider the following set of frequent 3-itemsets: {1, 2, 3}, {1, 2, 4}, {1, 2, 5}, {1, 3, 4}, {2, 3, 4}, {2, 3, 5}, {3, 4, 5}. Which one is not a candidate 4-itemset?\n\nOptions:\nA. {1,2,4,5}\nB. {1,3,4,5} \nC. {2,3,4,5}\nD. {1,2,3,4}\n", "answer": "Explanations: The candidate 4-itemset that is not present in the frequent 3-itemsets is {1,2,3,4}. This can be determined by checking if all the possible subsets of {1,2,3,4} are present in the frequent 3-itemsets. \nThe subsets of {1,2,3,4} are {1,2,3}, {1,2,4}, {1,3,4}, {2,3,4}. \nAmong these subsets, {1,2,3}, {1,2,4}, {1,3,4} are present in the frequent 3-itemsets, but {2,3,4} is not. Therefore, {1,2,3,4} is the candidate 4-itemset that is not present in the frequent 3-itemsets. ///Answer: D"}
{"subject": "", "question": "Question: Consider the following mysterious binary encoding:\begin{center}    \begin{tabular}{c|c}     symbol & encoding \\     \\hline      $a$ & $??0$\\      $b$ & $??0$\\      $c$ & $??0$\\      $d$ & $??0$ \\end{tabular} \\end{center} where with '$?$' we mean that we do not know which bit is assigned as the first two symbols of the encoding of any of the source symbols $a,b,c,d$. What can you infer on this encoding assuming that the code-words are all different?\n\nOptions:\nA. The encoding is uniquely-decodable.\nB. The encoding is uniquely-decodable but not prefix-free.\nC. We do not possess enough information to say something about the code.\nD. It does not satisfy Kraft's Inequality.\n", "answer": "Explanations: Given that the code-words are all different, we can infer the following:\n\n1. **Uniquely-decodable**: Since all code-words are distinct, there will be no ambiguity in decoding any given sequence of bits back to the original symbols. This ensures that the encoding is uniquely-decodable.\n\n2. **Prefix-freeness**: The encoding is not guaranteed to be prefix-free because we are not certain about the first two bits of each code-word. If the first two bits of one code-word are a prefix of another code-word, then the encoding is not prefix-free.\n\n3. **Kraft's Inequality**: Kraft's Inequality states that for a uniquely-decodable binary code, the codeword lengths must satisfy  \\[ \\sum 2^{-l_i} \\leq 1 \\] where $l_i$ is the length of the $i$-th code-word. Since we do not know the actual lengths of the code-words due to the unknown bits, we cannot definitively say if the code satisfies Kraft's Inequality.\n\nTherefore, the correct option is:\n\n2) The encoding is uniquely-decodable but not prefix-free. ///Answer: B"}
{"subject": "", "question": "Question: A MAC forgery is\\dots?\n\nOptions:\nA. a valid pair $(X,c)$ produced by the adversary.\nB. a valid pair $(X,c)$ produced by the MAC issuer.\nC. a valid pair $(X,c)$ produced by the MAC verifier.\nD. a key recovery attack.\n", "answer": "Explanations: A MAC forgery occurs when an adversary produces a valid pair $(X,c)$ that was not generated by the legitimate MAC issuer. This means that the adversary is able to create a message and its corresponding MAC without knowing the secret key used by the MAC issuer. In other words, the adversary successfully forges a valid MAC without authorization. \n\nTherefore, the correct answer is: 'a valid pair $(X,c)$ produced by the adversary.' This choice accurately describes the concept of a MAC forgery where the adversary creates a valid pair without the legitimate authorization. ///Answer: A"}
{"subject": "", "question": "Question: In JOS, suppose one Env sends a page to another Env. Is the page copied?\n\nOptions:\nA. Yes\nB. No\n", "answer": "Explanations: A: No, in JOS (J Operating System), when one Env (Environment) sends a page to another Env, the page is copied. This copying of the page is done to maintain memory isolation between Envs and prevent unintended sharing of memory contents. Therefore, the correct answer is Yes, the page is copied when sent between Envs in JOS. Thank you for guiding me through the process! ///Answer: A"}
{"subject": "", "question": "Question: Bluetooth pairing v2.0 is based on\\dots?\n\nOptions:\nA. bilinar mappings over elliptic curves.\nB. a short authenticated string.\nC. an ephemeral secret PIN code.\nD. a secure token.\n", "answer": "Explanations: First, Bluetooth pairing v2.0 is based on a short authenticated string for secure communication. This is done by generating an ephemeral secret PIN code to establish a secure connection. Therefore, the correct answers are Answer 2 and Answer 3.\n\nAnswers 2 and 3. ///Answer: B and C"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. The discrete logarithm?\n\nOptions:\nA. can be solved by a polynomial algorithm.\nB. is an easy problem in the Abelian group $\\mathbb{Z}_p$.\nC. is a hard problem in the multiplicative group $\\mathbb{Z}_p^*$.\nD. can be solved easily on a quantum computer.\n", "answer": "Explanations: The correct answer is (a) can be solved by a polynomial algorithm. This statement is incorrect because solving the discrete logarithm problem is generally considered computationally difficult and there is no known polynomial time algorithm to solve it efficiently. The other statements are true: (b) it is an easy problem in the Abelian group $\\mathbb{Z}_p$, (c) it is a hard problem in the multiplicative group $\\mathbb{Z}_p^*$, and (d) it can potentially be solved easily on a quantum computer using Shor's algorithm. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is/are true about fuzzing?\n\nOptions:\nA. Black box fuzzing may struggle to find inputs that reach deep into the program.\nB. The quality of initial seeds matters in mutational fuzzing.\nC. In structure-aware fuzzing, the mutator should only generate\n                  inputs that comply with all the format rules.\nD. Fuzzing is complete as soon as all code is covered.\n", "answer": "Explanations: 1) True. Black box fuzzing relies on randomly generating inputs without knowledge of the internal program structure. This can make it difficult to find inputs that reach deep into the program and trigger vulnerabilities that may be hidden in less commonly accessed code paths.\\n\\n2) True. In mutational fuzzing, the initial seeds are modified and mutated to generate new inputs. The quality of these initial seeds can greatly impact the effectiveness of the fuzzing process. High-quality initial seeds that are close to triggering vulnerabilities are more likely to produce meaningful results.\\n\\n3) True. In structure-aware fuzzing, the mutator must generate inputs that comply with the format rules of the input data expected by the program. This ensures that the generated inputs are valid and have a higher chance of triggering vulnerabilities in the target program.\\n\\n4) False. Fuzzing is not complete as soon as all code is covered. While achieving high code coverage is an important goal in fuzzing, it does not guarantee that all possible vulnerabilities have been found. Fuzzing should be seen as an ongoing process that requires continuous testing and refinement to uncover potential security issues. ///Answer: A"}
{"subject": "", "question": "Question: Find all solutions of $24x + [9]_{45} = [13]_{45}$ in the range $[0,44]$. How many different solutions are there?\n\nOptions:\nA. $0$\nB. $1$\nC. $2$\nD. $3$\n", "answer": "Explanations: To find the solutions of the equation $24x + [9]_{45} = [13]_{45}$ in the range $[0,44]$, we first simplify the equation using the properties of modular arithmetic.\n\nWe have $[9]_{45} = 9$ and $[13]_{45} = 13$. So, the equation becomes $24x + 9 \\equiv 13 \\pmod{45}$.\n\nSubtracting $9$ from both sides gives $24x \\equiv 4 \\pmod{45}$.\n\nTo find the solutions for $x$, we need to find the modular inverse of $24$ modulo $45$.\n\nThe modular inverse of $24$ modulo $45$ is the number $a$ such that $24a \\equiv 1 \\pmod{45}$.\n\nUsing the Extended Euclidean Algorithm or by inspection, we find that $24 \\cdot 24 \\equiv 1 \\pmod{45}$.\n\nTherefore, $24$ is its own modular inverse modulo $45$.\n\nMultiplying both sides of $24x \\equiv 4 \\pmod{45}$ by $24$, we get $x \\equiv 16 \\pmod{45}$.\n\nTherefore, the solutions for $x$ in the range $[0,44]$ are $16, 16 + 45, 16 + 2 \\cdot 45, ...$\n\nThe solutions in the range $[0,44]$ are $16$ and $16 + 45 = 61$. Since $61$ is outside the range $[0,44]$, there is only $1$ solution in the range $[0,44]$.\n\nTherefore, the correct option is:\n\nOption 2: $1$ ///Answer: B"}
{"subject": "", "question": "Question: Which one of the following notions is not in the fundamental trilogy of cryptography?\n\nOptions:\nA. authentication\nB. confidentiality\nC. integrity\nD. privacy\n", "answer": "Explanations: The fundamental trilogy of cryptography consists of three main goals that cryptographic systems aim to achieve. Let's analyze each option in relation to the fundamental trilogy:\n\n1. Authentication:\nAuthentication is a fundamental goal in cryptography. It ensures that the parties involved in a communication or transaction can verify each other's identities and confirm that the information received is from a trusted source.\n\n2. Confidentiality:\nConfidentiality is another essential goal in cryptography. It focuses on protecting the content of the information being transmitted from unauthorized access or disclosure. Encryption is commonly used to achieve confidentiality.\n\n3. Integrity:\nIntegrity is the third pillar of the fundamental trilogy of cryptography. It ensures that the data has not been tampered with or altered during transmission. Integrity mechanisms such as digital signatures and message authentication codes help verify the integrity of the data.\n\n4. Privacy:\nPrivacy, while an important concept in information security and data protection, is not traditionally included in the fundamental trilogy of cryptography. Privacy focuses on the control of personal information and data usage, while the fundamental trilogy of cryptography primarily addresses authentication, confidentiality, and integrity.\n\nTherefore, the notion that is not in the fundamental trilogy of cryptography is:\n- Privacy (option 4) ///Answer: D"}
{"subject": "", "question": "Question: How does LSI querying work?\n\nOptions:\nA. The query vector is treated as an additional term; then cosine similarity is computed\nB. The query vector is transformed by Matrix S; then cosine similarity is computed\nC. The query vector is treated as an additional document; then cosine similarity is computed\nD. The query vector is multiplied with an orthonormal matrix; then cosine similarity is computed\n", "answer": "Explanations: The correct answer is: 'The query vector is treated as an additional term; then cosine similarity is computed'. This is because in Latent Semantic Indexing (LSI) querying, the query vector is treated as an additional term in the document-term matrix, and then cosine similarity is computed between the query vector and the document vectors to retrieve relevant documents. The other options do not accurately describe how LSI querying works. ///Answer: A"}
{"subject": "", "question": "Question: Suppose there is a room with $n$ people and you want to generate a secret key for all pairs of participants. You need?\n\nOptions:\nA. $2^n$ keys.\nB. $n^2$ keys.\nC. $\\log_2(n-1)$ keys.\nD. $\\frac{n(n-1)}{2}$ keys.\n", "answer": "Explanations: To generate a secret key for all pairs of participants in a room with n people, we need to consider the number of unique pairs that can be formed from n people. \n\nThe number of unique pairs that can be formed from n people is given by the combination formula, which is $\\binom{n}{2} = \\frac{n!}{2!(n-2)!} = \\frac{n(n-1)}{2}$. \n\nTherefore, the correct answer is:\n\n{'Correct Answer(s)': 4. $\\frac{n(n-1)}{2}$ keys.}\n\nThis answer is correct because it correctly calculates the number of unique pairs that can be formed from n people in the room, which is essential for generating secret keys for all pairs of participants. The other options do not accurately reflect the correct calculation needed for this scenario. ///Answer: D"}
{"subject": "", "question": "Question: What is the time complexity to perfom a left-to-right multiplication of two $\\ell$-bit integers?\n\nOptions:\nA. $\\sqrt{\\ell}$.\nB. $\\ell $.\nC. $\\ell ^2$.\nD. $\\ell ^3$.\n", "answer": "Explanations: The time complexity to perform a left-to-right multiplication of two $\\ell$-bit integers is $\\ell^2$.\n\nWhen multiplying two $\\ell$-bit integers using the left-to-right multiplication method, each bit of the second integer needs to be multiplied by the first integer, resulting in $\\ell$ single-bit multiplications for each bit position. Since there are $\\ell$ bits in the second integer, the total number of single-bit multiplications needed is $\\ell \\times \\ell = \\ell^2$.\n\nTherefore, the time complexity to perform a left-to-right multiplication of two $\\ell$-bit integers is $\\ell^2$. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\textbf{incorrect} assertion. A random oracle\\dots?\n\nOptions:\nA. returns a value which is uniformly distributed and independent from the previous queries.\nB. is replaced by a hash function in practice.\nC. is a useful tool in theory.\nD. has unpredictable outputs before queries are made.\n", "answer": "Explanations: 1. This is a correct assertion. A random oracle returns a value that is uniformly distributed and independent from previous queries.\n\n2. This is also a correct assertion. In practice, a random oracle is often replaced by a hash function.\n\n3. This is a correct assertion. A random oracle is a useful tool in theoretical cryptography.\n\n4. This is the incorrect assertion. A random oracle has predictable outputs before queries are made. It is only after queries are made that the outputs become unpredictable. \n\nTherefore, the incorrect assertion is: 4. has unpredictable outputs before queries are made. ///Answer: D"}
{"subject": "", "question": "Question: If we have a $n$-bit key, the attack phase with time-memory tradeoff will roughly have\\dots?\n\nOptions:\nA. a time complexity of $1$ (constant).\nB. a time complexity of $2^{\\frac{2}{3}n}$\nC. a time complexity of $2^{\\frac{n}{2}}$\nD. a time complexity of $2^{n}$\n", "answer": "Explanations: To determine which of the given time complexities grows the fastest as $n$ approaches infinity, we need to compare them based on their growth rates.\n\nLet's analyze each option:\n\n1. $1$ (constant) - This time complexity does not depend on the input size $n$, so it remains constant regardless of the input. This means it grows the slowest compared to the other options.\n\n2. $2^{\\frac{2}{3}n}$ - This time complexity has an exponential growth rate with a base of $2$. The exponent $\\frac{2}{3}n$ is smaller than $n$, but it is still exponential. As $n$ approaches infinity, the exponential growth will eventually outpace any polynomial growth rate.\n\n3. $2^{\\frac{n}{2}}$ - This time complexity also has an exponential growth rate with a base of $2$. However, the exponent $\\frac{n}{2}$ is larger than in the previous option. As $n$ approaches infinity, this growth rate will be faster than the previous one.\n\n4. $2^{n}$ - This time complexity has the fastest growth rate among the options. It is an exponential function with a base of $2$ and an exponent of $n$. As $n$ approaches infinity, this growth rate is the fastest compared to the other options.\n\nTherefore, the correct answer is: a time complexity of $2^{n}$ ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\textit{correct} assertion. Given an alphabet $Z$, if a language $L \\subset Z^*$ belongs to the class $\\mathsf{co}\\operatorname{-}\\mathcal{NP}$ then \\ldots?\n\nOptions:\nA. there is a predicate $R$ such that $\\forall x\\in L$ there is a $w\\in Z^*$ which satisfies $R(x,w)$, and such that $R(x,w)$ can be computed in time that is polynomial in $|x|$.\nB. there is always an algorithm $\\mathcal{A}$ that, given an $x\\in Z^*$, determines if $x\\in L$ in time that is polynomial in $|x|$.\nC. the language $\\bar{L}=\\{x \\in Z^* \\mid x\\notin L\\}$ belongs to the class $\\mathcal{NP}$.\nD. necessarily, $L \\notin \\mathcal{P}$.\n", "answer": "Explanations: The correct assertion is: \"there is a predicate $R$ such that $\\\\forall x\\\\in L$ there is a $w\\\\in Z^*$ which satisfies $R(x,w)$, and such that $R(x,w)$ can be computed in time that is polynomial in $|x|$.\"\n\nExplanation: The class $\\mathsf{co}\\operatorname{-}\\mathcal{NP}$ is the class of languages whose complements are in $\\mathcal{NP}$. This means that if a language $L \\subset Z^*$ belongs to $\\mathsf{co}\\operatorname{-}\\mathcal{NP}$, then its complement $\\bar{L}$ belongs to $\\mathcal{NP}$. \n\nThe correct assertion captures the essence of $\\mathsf{co}\\operatorname{-}\\mathcal{NP}$ by stating that there exists a predicate $R$ that can be computed in polynomial time and can certify membership in $L$. This is in line with the definition of $\\mathsf{co}\\operatorname{-}\\mathcal{NP}$ where the complement of $L$ is in $\\mathcal{NP}$. ///Answer: A"}
{"subject": "", "question": "Question: Following are token counts that appear in 3 documents (D1, D2, and D3):\nD1 \u2013\u00a0tablet: 7;\u00a0memory: 5;\u00a0app: 8;\u00a0sluggish: 7\nD2 \u2013\u00a0memory: 5;\u00a0app: 3\nD3 \u2013\u00a0tablet: 3;\u00a0sluggish: 3\nBased on the cosine similarity, which 2 documents are the most similar?\n?\n\nOptions:\nA. D1 and D2\nB. D1 and D3\nC. D2 and D3\n", "answer": "Explanations: Answer: \n\nA) D1 and D3\n\nExplanation: \n\nThe cosine similarity between D1 and D3 is the highest because both documents share the terms \"tablet\" and \"sluggish\" which are not present in D2, making D1 and D3 more similar to each other. ///Answer: A"}
{"subject": "", "question": "Question: The exponent $\\lambda(21)$ of $\\mathbb{Z}_{21}^*$ is ...\n\nOptions:\nA. 21.\nB. 12.\nC. 6.\nD. 3.\n", "answer": "Explanations: First, we need to find the totient function value of 21. The totient function, denoted by $\\phi(n)$, gives the number of positive integers less than $n$ that are relatively prime to $n$.\n\nFor 21, the prime factorization is $21 = 3 \\times 7$. Therefore, $\\phi(21) = (3-1)(7-1) = 2 \\times 6 = 12$.\n\nNow, the exponent $\\lambda(21)$ of $\\mathbb{Z}_{21}^*$ is defined as the smallest positive integer $k$ such that $a^k \\equiv 1 \\pmod{21}$ for all $a \\in \\mathbb{Z}_{21}^*$. \n\nBy Euler's theorem, we know that for any integer $a$ relatively prime to 21, $a^{12} \\equiv 1 \\pmod{21}$. Therefore, the exponent $\\lambda(21)$ is 12.\n\nSo, the correct answer is: 12. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textit{incorrect} assertion. In a typical (secure) TLS session \\ldots?\n\nOptions:\nA. the client authenticates himself to the server.\nB. the client and the server securely establish a symmetric key.\nC. the client first sends the list of accepted cipher suites.\nD. the client is sure that he is communicating with an authentic server.\n", "answer": "Explanations: The incorrect assertion in a typical secure TLS session is: \"the client authenticates himself to the server.\"\n\nIn a TLS session, it is the server that authenticates itself to the client, not the other way around. The client verifies the server's identity using its digital certificate, which is issued by a trusted Certificate Authority (CA). This process ensures that the client is communicating with an authentic server.\n\nThe other assertions are correct:\n- The client and the server securely establish a symmetric key during the TLS handshake.\n- The client first sends the list of accepted cipher suites to the server as part of the negotiation process.\n- The client is sure that he is communicating with an authentic server through the server's digital certificate verification. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following statements about mitigations are true?\n\nOptions:\nA. The bug remains in the application, mitigations simply make exploitation harder\nB. The performance of certain mitigations depends on underlying architecture features (e.g., i386 versus x86-64)\nC. All mitigations fully stop an attack vector\nD. No mitigation requires hardware support to be implemented\n", "answer": "Explanations: Answer: A: The bug remains in the application, mitigations simply make exploitation harder\n B: The performance of certain mitigations depends on underlying architecture features (e.g., i386 versus x86-64)\n\nExplanation:\nA: The bug remains in the application, mitigations simply make exploitation harder. This statement is true because mitigations do not fix the underlying bug but rather implement measures to make it more difficult for attackers to exploit the vulnerability.\n\nB: The performance of certain mitigations depends on underlying architecture features (e.g., i386 versus x86-64). This statement is also true because different hardware architectures may require different approaches to apply mitigations effectively. Factors such as memory layout, instruction sets, and other architecture-specific features can impact the performance of mitigations.\n\nC: The statement \"All mitigations fully stop an attack vector\" is incorrect. While mitigations aim to reduce the risk of successful exploitation, they may not completely eliminate all attack vectors. Attackers may still find ways to bypass or overcome certain mitigations, especially as new vulnerabilities are discovered and new exploitation techniques are developed.\n\nD: The statement \"No mitigation requires hardware support to be implemented\" is incorrect. Some mitigations do require hardware support to be effectively implement security measures. For example, hardware-enforced security features like Intel SGX or ARM TrustZone rely on specific hardware capabilities to provide secure execution environments. ///Answer: A"}
{"subject": "", "question": "Question: Daemons are just long running processes. When applying mitigations to these\n    processes, several aspects change. Which ones?\n\nOptions:\nA. ASLR becomes less effective as multiple requests across\n            different users are handled in a single process.\nB. Stack canaries become less effective as multiple requests are\n            handled by the same thread.\nC. DEP becomes less effective as compiler optimizations are\n            turned on, allowing the attacker to inject new code.\nD. CFI becomes less effective as the concurrent clients cause\n            more targets to be available.\n", "answer": "Explanations: The correct answer is 2. Stack canaries become less effective as multiple requests are handled by the same thread.\n\nWhen multiple requests are handled by the same thread in a daemon process, the stack canaries that are put in place to detect buffer overflows may become less effective. This is because the same thread may handle different requests concurrently, potentially overwriting the stack canary values and allowing for buffer overflow attacks to succeed. Therefore, the effectiveness of stack canaries as a mitigation technique decreases in this scenario. ///Answer: B"}
{"subject": "", "question": "Question: In which of the following cases does the TLB need to be flushed?\n\nOptions:\nA. Inserting a new page into the page table for a user-space application.\nB. Deleting a page from the page table.\nC. Changing the read/write permission bit in the page table.\nD. Inserting a new page into the page table for kernel.\n", "answer": "Explanations: Option chosen: 3) Inserting a new page into the page table for kernel.\nWhy you have chosen this answer: The Translation Lookaside Buffer (TLB) is a cache that stores mappings of virtual memory to physical memory addresses. When a new page is inserted into the page table for the kernel, the TLB needs to be flushed to ensure that the new mappings are reflected in the TLB. This is because the kernel's address space is separate from the user-space applications, and the TLB needs to be updated with the new mappings for the kernel's page table. ///Answer: D"}
{"subject": "", "question": "Question: Find $[3^{288294}]_{35}$.\n\nOptions:\nA. $29$\nB. $11$\nC. $9$\nD. $33$\n", "answer": "Explanations: Let's compute $[3^{288294}]_{35}$ step by step:\n\n1. First, we know that $3^3 = 27 \\equiv -8 \\pmod{35}$.\n2. Next, we can see that $3^4 = 3 \\cdot 3^3 \\equiv 3 \\cdot (-8) \\equiv -24 \\equiv 11 \\pmod{35}$.\n3. Continuing this pattern, $3^5 \\equiv 3 \\cdot 11 \\equiv 33 \\pmod{35}$.\n4. Similarly, $3^6 \\equiv 3 \\cdot 33 \\equiv 99 \\equiv 29 \\pmod{35}$.\n\nNow, we need to find the remainder of $288294$ when divided by $6$:\n\n$288294 \\div 6 = 48049$ with a remainder of $0$.\n\nTherefore, $3^{288294} \\equiv 3^0 \\equiv 1 \\pmod{35}$.\n\nHence, $[3^{288294}]_{35} = 1$.\n\nAmong the given options, the closest one to the correct answer is Option 0: $29$. ///Answer: A"}
{"subject": "", "question": "Question: Mark any of the following functions that have unique maximizers:?\n\nOptions:\nA. $f(x) =-x^{2}, \\quad x \\in[-10,10]$\nB. $f(x) =\\ln (x), \\quad x \\in(0,10]$\nC. $f(x) =x^{2}, \\quad x \\in[-10,10]$\nD. $f(x) =\\cos (2 \\pi x), \\quad x \\in[-1,1]$\nE. $f(x) =\\cos (2 \\pi x), \\quad x \\in\\left[-\\frac{1}{2}, \\frac{1}{2}\\right]$\n", "answer": "Explanations: To determine if a function has a unique maximizer, we need to find where the function reaches its maximum value and whether this maximum value is attained at only one point or multiple points within the given interval.\n\nLet's analyze each function:\n\n1. $f(x) = -x^{2}$, $x \\in [-10,10]$:\nThis function is a downward-opening parabola, and its maximum value occurs at the vertex. In this case, the vertex is at $x = 0$, and the maximum value is $f(0) = 0$. Since the maximum value is attained at only one point, this function has a unique maximizer at $x = 0$.\n\n2. $f(x) = \\ln(x)$, $x \\in (0,10]$:\nThe natural logarithm function has a unique maximizer within the given interval. To find where it occurs, we take the derivative and set it equal to zero to find critical points:\n$f'(x) = \\frac{1}{x} = 0$\nThis gives $x = 1$ as the critical point. By evaluating the function at the critical point and the endpoints, we find that the maximum value is at $x = 10$. Therefore, this function has a unique maximizer at $x = 10$.\n\n3. $f(x) = x^{2}$, $x \\in [-10,10]$:\nThis function is an upward-opening parabola, and its maximum value occurs at the vertex. The vertex is at $x = 0$, and the maximum value is $f(0) = 0$. Similar to the first function, this function has a unique maximizer at $x = 0$.\n\n4. $f(x) = \\cos(2\\pi x)$, $x \\in [-1,1]$:\nThe cosine function has a period of $2\\pi$, which means it repeats every $2\\pi$ units. In this case, the function $\\cos(2\\pi x)$ in the interval $[-1,1]$ has multiple maximizers because the function repeats its maximum value within this interval.\n\n5. $f(x) = \\cos(2\\pi x)$, $x \\in \\left[-\\frac{1}{2}, \\frac{1}{2}\\right]$:\nSimilar to the fourth function, the cosine function in this interval also has multiple maximizers due to the periodic nature of the cosine function.\n\nTherefore, out of the given functions, only $f(x) = -x^{2}$ and $f(x) = \\ln(x)$ have unique maximizers within their respective intervals. ///Answer: B"}
{"subject": "", "question": "Question: Select the most accurate answer. For an RSA key to have a good security level in 2015, the modulus should be of length\\dots?\n\nOptions:\nA. around 80 bits.\nB. around 1300 bits.\nC. around 160 bits.\nD. around 512 bits.\n", "answer": "Explanations: 2) around 1300 bits.\n\nThis is the correct answer because in 2015, it was recommended to use RSA keys with a modulus length of at least 2048 bits for good security. A modulus length of 1300 bits would not be considered secure enough for RSA keys in 2015. ///Answer: B"}
{"subject": "", "question": "Question: Let $X$ be a random variable distributed over the alphabet $\\mathcal{X}=\\{0,1,\\ldots,n\\}$. Assume also that there exist $x_1,x_2\\in\\mathcal{X}$ such that $p_X(x_1)\neq p_X(x_2)$ (\textit{i.e.}, $X$ is not uniformly distributed over $\\mathcal{X}$). Let $Y=2^X$ and $Z=\\lfloor X/2 \rfloor$.  True or false: $H(Z)=H(Y)$?\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The question is asking whether the entropy of the random variable $Z$ is equal to the entropy of the random variable $Y$, given that $Y = 2^X$ and $Z = \\lfloor X/2 \\rfloor$.\n\nTo evaluate this, let's first understand the definitions:\n\n1. Entropy $H(X)$ of a discrete random variable $X$ is a measure of the uncertainty associated with the random variable. It is defined as $H(X) = -\\sum_{x \\in \\mathcal{X}} p_X(x) \\log p_X(x)$, where $p_X(x)$ is the probability mass function of $X$.\n\n2. Random variable $Y = 2^X$: This random variable $Y$ takes on values that are powers of 2 based on the values of $X$.\n\n3. Random variable $Z = \\lfloor X/2 \\rfloor$: This random variable $Z$ takes on values that are half of the values of $X$, rounded down to the nearest integer.\n\nGiven that $X$ is not uniformly distributed over $\\mathcal{X}$ and there exist $x_1, x_2 \\in \\mathcal{X}$ such that $p_X(x_1) \\neq p_X(x_2)$, it implies that the probabilities of different values of $X$ are not equal.\n\nNow, let's analyze $Y = 2^X$ and $Z = \\lfloor X/2 \\rfloor$:\n\n1. For $Y = 2^X$: Since $Y$ takes on values that are powers of 2 based on the values of $X$, the distribution of $Y$ is essentially a transformed version of the distribution of $X$. This transformation alters the distribution, potentially increasing the entropy.\n\n2. For $Z = \\lfloor X/2 \\rfloor$: The values of $Z$ are half of the values of $X$, rounded down. This transformation decreases the number of possible values that $Z$ can take compared to $X$, potentially reducing the entropy.\n\nGiven these considerations, it is unlikely that $H(Z) = H(Y)$ holds true. The transformation from $X$ to $Y$ tends to spread out the probabilities, potentially increasing the entropy, while the transformation from $X$ to $Z$ compresses the values, potentially decreasing the entropy.\n\nTherefore, the statement is usually false: $H(Z) \\neq H(Y)$. ///Answer: B"}
{"subject": "", "question": "Question: Which symmetric key primitive is used in WPA2 encryption?\n\nOptions:\nA. RC4 CBC Mode\nB. KASUMI ECB Mode\nC. MD5 OFB Mode\nD. AES CCM Mode\n", "answer": "Explanations: The correct answer is 4) AES CCM Mode.\n\n1) RC4 CBC Mode: RC4 is a stream cipher and CBC (Cipher Block Chaining) is a mode of operation, but WPA2 does not use RC4 for encryption.\n2) KASUMI ECB Mode: KASUMI is a block cipher used in 3G mobile communications, but it is not used in WPA2 encryption.\n3) MD5 OFB Mode: MD5 is a hashing algorithm, not a symmetric key encryption algorithm, and OFB (Output Feedback) is a mode of operation. This combination is not used in WPA2 encryption.\n4) AES CCM Mode: WPA2 uses AES (Advanced Encryption Standard) as the symmetric key encryption algorithm in CCM (Counter with CBC-MAC) mode for data confidentiality and integrity. ///Answer: D"}
{"subject": "", "question": "Question: Which of the following statements about index merging (when constructing inverted files) is correct?\n\nOptions:\nA. While merging two partial indices on disk, the inverted lists of a term are concatenated without sorting\nB. Index merging is used when the vocabulary does no longer fit into the main memory\nC. The size of the final merged index file is O(nlog2(n)*M), where M is the size of the available memory\nD. While merging two partial indices on disk, the vocabularies are concatenated without sorting\n", "answer": "Explanations: To answer this question, we need to understand the concept of index merging in the context of constructing inverted files for information retrieval systems. Let's break down the provided propositions and determine which one is correct:\n\n1. **While merging two partial indices on disk, the inverted lists of a term are concatenated without sorting**: This statement implies that when merging two partial indices, the inverted lists of a term are simply concatenated without any sorting process. This approach is commonly used in index merging to combine inverted lists efficiently without the need for sorting. This proposition aligns with the merge operation in external sorting algorithms like merge sort or external merge sort.\n\n2. **Index merging is used when the vocabulary does no longer fit into the main memory**: Index merging becomes necessary when the size of the vocabulary (terms present in the documents) exceeds the available main memory capacity. In such cases, the index needs to be stored on disk and merged in smaller chunks to manage memory constraints effectively.\n\n3. **The size of the final merged index file is O(nlog2(n)*M), where M is the size of the available memory**: This statement provides information about the complexity of the final merged index file concerning the size of available memory M. The complexity mentioned (O(nlog2(n)*M)) hints at the computational cost associated with merging indexes based on the available memory.\n\n4. **While merging two partial indices on disk, the vocabularies are concatenated without sorting**: This proposition suggests that during the merging process, the vocabularies (unique terms) from the partial indices are concatenated without any sorting operation. This approach can be efficient in scenarios where sorting the vocabularies is not necessary for merging the indexes.\n\nNow, let's determine which statement is correct based on the principles of index merging. The correct statement is likely to be the one that aligns with the standard practices of merging inverted indices efficiently while managing memory constraints. ///Answer: A"}
{"subject": "", "question": "Question: Dans un syst\u00e8me ind\u00e9formable, comment est d\u00e9finie la tension induite ? (plusieurs r\u00e9ponses possibles)?\n\nOptions:\nA. l'oppos\u00e9 (-) de l'int\u00e9grale du champ d'induction magn\u00e9tique (densit\u00e9 de flux) sur une surface\nB. l'oppos\u00e9 (-) de l'int\u00e9grale du champ \u00e9lectrique sur un contour ferm\u00e9\nC. la d\u00e9riv\u00e9e par rapport au temps du champ d'induction magn\u00e9tique (densit\u00e9 de flux)\nD. la d\u00e9riv\u00e9e par rapport au temps du flux totalis\u00e9\n", "answer": "Explanations: Predication in computer systems refers to the ability to conditionally execute instructions based on a specified condition. In Very Long Instruction Word (VLIW) processors, predication is almost universal because VLIW processors typically execute multiple instructions in parallel within a single clock cycle. By using predication, the processor can avoid branching and the associated pipeline stalls, improving efficiency and performance.\n\nIn VLIW processors, predication allows multiple instructions to be executed simultaneously, even if some of them are not actually needed. This helps to fully utilize the available resources and improve overall throughput. Additionally, predication can simplify the hardware design of VLIW processors by reducing the complexity of control logic for handling branches.\n\nIn contrast, predication is not commonly used in Reduced Instruction Set Computer (RISC) processors. RISC processors typically rely on a simple and efficient control flow model with conditional branches for handling control flow. Predication in a RISC processor could introduce additional complexity and overhead, potentially negating the benefits of a simplified instruction set.\n\nHowever, there are some cases where predication could make sense in a RISC processor. For example, in cases where there are many independent instructions that can be executed in parallel, predication could help improve the utilization of resources and overall performance. Additionally, predication could be beneficial in situations where branch prediction is difficult or ineffective, leading to frequent pipeline stalls.\n\nIn conclusion, while predication is almost universal in VLIW processors due to its benefits in parallel execution and resource utilization, it may also make sense in certain scenarios for RISC processors. Ultimately, the decision to implement predication in a RISC processor would depend on the specific use case and trade-offs between complexity, performance, and efficiency. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following hold true for cross-site scripting (XSS)?\n\nOptions:\nA. XSS is a form of code injection that gives the attacker\n            arbitrary code execution.\nB. Reflected XSS requires that the server stores the injected\n            code but the user does not need to click on any special link.\nC. Client-side XSS is a unique problem of GMail.\nD. XSS can only be used to leak private data of a user.\n", "answer": "Explanations: A. XSS is a form of code injection that gives the attacker arbitrary code execution.\nThis statement is true. Cross-site scripting (XSS) is a type of security vulnerability typically found in web applications. It allows attackers to inject malicious scripts into web pages viewed by other users. These scripts can then perform various malicious actions, such as stealing sensitive data, session hijacking, defacing websites, redirecting users to malicious websites, etc.\n\nB. Reflected XSS requires that the server stores the injected code but the user does not need to click on any special link.\nThis statement is also true. Reflected XSS occurs when the malicious script is reflected off a web server to the victim's browser. This typically happens when the user clicks on a malicious link that contains the injected script. The server does not store the script permanently, but it reflects it back in the response to the user's request.\n\nC. Client-side XSS is a unique problem of GMail.\nThis statement is false. Client-side XSS is not a unique problem of GMail; it is a general issue that can affect any web application that does not properly sanitize or validate user input. GMail, like any other web application, can be vulnerable to XSS attacks if not adequately protected.\n\nD. XSS can only be used to leak private data of a user.\nThis statement is false. While one of the common goals of XSS attacks is to steal private data, such as login credentials, cookies, or personal information, XSS vulnerabilities can also be exploited for other malicious purposes. For example, an attacker could use XSS to deface a website, redirect users to phishing pages, perform actions on behalf of the user, or spread malware.\n\nIn summary, options A and B are true statements regarding XSS, while options C and D are false. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{true} statement regarding $\\mathsf{GF}(2^k)$.\n\nOptions:\nA. Addition is performed as bitwise XOR.\nB. Multiplication is performed modulo $2^k$.\nC. It has order $\\varphi(2^k)$.\nD. Its characteristic is $k$.\n", "answer": "Explanations: Let's evaluate each option:\n\n- Multiplication is performed modulo $2^k$: This option is not correct. In $\\mathsf{GF}(2^k)$, multiplication is not performed modulo $2^k$ but rather by using irreducible polynomials and reduction modulo a specific polynomial.\n\n- It has order $\\varphi(2^k)$: This option is incorrect. The order of $\\mathsf{GF}(2^k)$ is $2^k - 1$, not $\\varphi(2^k)$, where $\\varphi$ denotes Euler's totient function.\n\n- Its characteristic is $k$: This option is incorrect. The characteristic of $\\mathsf{GF}(2^k)$ is 2, not $k$. The characteristic of a field is the smallest positive integer $n$ such that $n \\cdot 1 = 0$, and in binary fields, it is 2 due to the nature of binary arithmetic.\n\nBy the process of elimination, we can determine that the true statement is: Addition is performed as bitwise XOR. This is correct as addition in $\\mathsf{GF}(2^k)$ is indeed accomplished through bitwise XOR operations. ///Answer: A"}
{"subject": "", "question": "Question: Consider the following sequence of random variables $S_1,\\ldots,S_n,\\ldots$ Assume that the limit $H^\\star(\\mathcal{S})=k$ exists and is finite. Suppose that there exists $\\hat{n}>0$ such that for all $i\\geq \\hat{n}$ one has that the marginal distributions of $S_{i+1}$ and $S_i$ satisfy $p_{S_{i+1}}=p_{S_i}$. Denote with $\\mathcal{Y}_{\\hat{n}}$ the alphabet of the source $S_{\\hat{n}}$. True or false: Can one use this information to infer that the following holds: $|\\mathcal{Y}_{\\hat{n}}| \\geq 2^k $?\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False. The fact that the marginal distributions of $S_{i+1}$ and $S_i$ are equal for $i\\geq\\hat{n}$ does not necessarily imply that the alphabet size $|\\mathcal{Y}_{\\hat{n}}|$ is greater than or equal to $2^k$. The equality of marginal distributions only tells us about the distribution of the random variables, not the size of the alphabet itself. It is possible for the alphabet size to be smaller than $2^k$ and still have equal marginal distributions for $i\\geq\\hat{n}$. ///Answer: B"}
{"subject": "", "question": "Question: Matrix Factorizations: The function $f(\\mathbf{v}):=g\\left(\\mathbf{v} \\mathbf{v}^{\\top}\\right)$ is convex over the vectors $\\mathbf{v} \\in \\mathbb{R}^{2}$, when $g: \\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R}$ is defined as?\n\nOptions:\nA. (a)  if we define $g: \\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R}$ as $g(\\mathbf{X}):=X_{11}$.\nB. (b)  if we define $g: \\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R}$ as $g(\\mathbf{X}):=X_{11}+X_{22}$.\n", "answer": "Explanations: The correct answer is B: (b) if we define $g: \\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R}$ as $g(\\mathbf{X}):=X_{11}+X_{22}$. \n\nTo show that $f(\\mathbf{v})=g\\left(\\mathbf{v} \\mathbf{v}^{\\top}\\right)$ is convex over the vectors $\\mathbf{v} \\in \\mathbb{R}^{2}$, we need to show that the Hessian matrix of $f(\\mathbf{v})$ is positive semidefinite.\n\nFirst, let's express $\\mathbf{v} \\mathbf{v}^{\\top}$ in terms of its elements:\n$\\mathbf{v} \\mathbf{v}^{\\top} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} \\begin{bmatrix} v_1 & v_2 \\end{bmatrix} = \\begin{bmatrix} v_1^2 & v_1v_2 \\\\ v_1v_2 & v_2^2 \\end{bmatrix}$\n\nNow, using the definition of $g(\\mathbf{X})=X_{11}+X_{22}$, we have:\n$g(\\mathbf{v} \\mathbf{v}^{\\top}) = v_1^2 + v_2^2$\n\nThe Hessian matrix of $f(\\mathbf{v})$ is given by:\n$\\nabla^2 f(\\mathbf{v}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial v_1^2} & \\frac{\\partial^2 f}{\\partial v_1 \\partial v_2} \\\\ \\frac{\\partial^2 f}{\\partial v_2 \\partial v_1} & \\frac{\\partial^2 f}{\\partial v_2^2} \\end{bmatrix} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}$\n\nSince the Hessian matrix is a constant positive definite matrix, it is positive semidefinite for all $\\mathbf{v} \\in \\mathbb{R}^{2}$. Therefore, $f(\\mathbf{v})=g\\left(\\mathbf{v} \\mathbf{v}^{\\top}\\right)$ is convex over the vectors $\\mathbf{v} \\in \\mathbb{R}^{2}$ when $g(\\mathbf{X}):=X_{11}+X_{22}$. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion. Two-keys triple DES\\dots?\n\nOptions:\nA. is more secure than double encryption.\nB. is less secure than AES.\nC. is as secure as a block cipher using a key twice longer.\nD. is vulnerable to a certain variant of a meet-in-the-middle attacks.\n", "answer": "Explanations: Let's analyze each option one by one:\n\nOption 1: \"is more secure than double encryption.\" This statement is false because two-keys triple DES is not more secure than double encryption. In fact, double encryption is generally considered more secure due to the vulnerabilities associated with triple DES.\n\nOption 2: \"is less secure than AES.\" This statement is true. AES is a more secure encryption algorithm compared to triple DES.\n\nOption 3: \"is as secure as a block cipher using a key twice longer.\" This statement is false. Using a block cipher with a key twice as long as triple DES would generally be more secure.\n\nOption 4: \"is vulnerable to a certain variant of a meet-in-the-middle attacks.\" This statement is true. Triple DES is vulnerable to certain meet-in-the-middle attacks, making it less secure in certain scenarios.\n\nTherefore, the false assertion is Option 1: \"is more secure than double encryption.\" ///Answer: A"}
{"subject": "", "question": "Question: The Kerckhoffs Principle states that ...\n\nOptions:\nA. the security of a cryptosystem should be based on an NP-hard problem.\nB. all ciphertexts appear with the same probability.\nC. the adversary may know the description of a cryptosystem.\nD. the cryptosystem should be public.\n", "answer": "Explanations: The Kerckhoffs Principle states that the security of a cryptosystem should not rely on the secrecy of the algorithm, but rather on the secrecy of the key. This means that even if the adversary knows the algorithm used in the cryptosystem, they should not be able to decrypt the message without knowing the key.\n\nNow, let's analyze each given option:\n\nOption 1: the security of a cryptosystem should be based on an NP-hard problem.\nThis option does not directly relate to the Kerckhoffs Principle. While using NP-hard problems can increase the security of a cryptosystem, it is not a direct consequence of the Kerckhoffs Principle.\n\nOption 2: all ciphertexts appear with the same probability.\nThis option also does not directly relate to the Kerckhoffs Principle. While having all ciphertexts appear with the same probability can help in achieving perfect secrecy, it is not a requirement of the Kerckhoffs Principle.\n\nOption 3: the adversary may know the description of a cryptosystem.\nThis option directly relates to the Kerckhoffs Principle. It states that the security of the cryptosystem should not rely on the secrecy of the algorithm, meaning that the adversary may know the description of the cryptosystem but should not be able to decrypt messages without the key. Therefore, this option aligns with the Kerckhoffs Principle.\n\nOption 4: the cryptosystem should be public.\nThis option contradicts the Kerckhoffs Principle. The principle states that the security of the system should not rely on the secrecy of the algorithm, but on the secrecy of the key. Making the cryptosystem public would go against this principle.\n\nTherefore, the correct answer is Option 3: the adversary may know the description of a cryptosystem. This option aligns with the Kerckhoffs Principle by emphasizing that the security of the cryptosystem should not be compromised even if the adversary knows the algorithm used. ///Answer: C"}
{"subject": "", "question": "Question: The CRT states?\n\nOptions:\nA. $\\mathbb{Z}_{mn} \\equiv \\mathbb{Z}_{m} \\cup \\mathbb{Z}_{n}$\nB. $\\mathbb{Z}_{mn} \\equiv \\mathbb{Z}_{m} \\setminus \\mathbb{Z}_{n}$\nC. $\\mathbb{Z}_{mn} \\equiv \\mathbb{Z}_{m} \\cap \\mathbb{Z}_{n}$\nD. $\\mathbb{Z}_{mn} \\equiv \\mathbb{Z}_{m} \\times \\mathbb{Z}_{n}$\n", "answer": "Explanations: The Chinese Remainder Theorem asserts that the most appropriate selection is $\\mathbb{Z}_{mn} \\equiv \\mathbb{Z}_{m} \\times \\mathbb{Z}_{n}$. This is due to the fact that the direct product operation yields a set that represents the Cartesian product of $\\mathbb{Z}_{m}$ and $\\mathbb{Z}_{n}$, providing a comprehensive representation of the residue classes modulo $m$ and $n$. ///Answer: D"}
{"subject": "", "question": "Question: Church booleans are a representation of booleans in the lambda calculus. The Church encoding of true and false are functions of two parameters: Church encoding of tru: t => f => t Church encoding of fls: t => f => f What should replace ??? so that the following function computes not(b and c)? b => c => b ??? (not b)?\n\nOptions:\nA. (not b)\nB. (not c)\nC. tru\nD. fls\n", "answer": "Explanations: Let's break down the problem step by step and think about each part one by one.\n\n1. We want to compute the logical NOT of the AND operation between b and c.\n2. The Church encoding of NOT can be defined as: `not = x => x(fls)(tru)`\n3. The Church encoding of AND operation can be defined as: `and = b => c => b(c)(fls)`\n4. We want to replace ??? in the given function `b => c => b ??? (not b)(not c)` with a function that represents the logical NOT of the AND operation between b and c.\n\nPutting it all together:\n- The Church encoding of NOT is `not = x => x(fls)(tru)`\n- The Church encoding of AND operation is `and = b => c => b(c)(fls)`\n- The function to compute not(b and c) can be written as:\n  `b => c => b((not b)(not c))(fls)`\n  \nNow, let's replace ??? with the correct expression based on the Church encoding of NOT and AND operations. The updated function is:\n`b => c => b((not b)(not c))(fls)`\n\nThis function will correctly compute the logical NOT of the AND operation between b and c using Church encodings. ///Answer: D"}
{"subject": "", "question": "Question: Choose the \\emph{incorrect} statement?\n\nOptions:\nA. The cardinality of an elliptic curve can be bounded using Hasse theorem.\nB. Elliptic curves are defined over a finite field.\nC. Elliptic curves are defined over an Abelian group.\nD. Points on elliptic curves satisfy a cubic polynomial equation.\n", "answer": "Explanations: The incorrect statement is: \n\n3. Elliptic curves are defined over an Abelian group.\n\nExplanation: Elliptic curves are not defined over an Abelian group, but rather they have an associated Abelian group structure. An elliptic curve is a plane algebraic curve defined by a cubic equation, and the set of points on the curve together with an additional point at infinity forms an Abelian group under a geometric operation called the chord-tangent law. ///Answer: C"}
{"subject": "", "question": "Question: Which operation does the following function implement? extension [U](l: List[U]) def secret[T](t: T)(f: (T, U) => T): T = \\t var res = t \\t l.foreach(u => res = f(res, u)) \\t res?\n\nOptions:\nA. reduceLeft\nB. reduceRight\nC. foldLeft\nD. foldRight\n", "answer": "Explanations: The correct operation that the following function implements is c) foldLeft. \n\nIn the provided function, the 'secret' function takes a list 'l' of type 'U', an initial value 't' of type 'T', and a function 'f' that takes two parameters of types 'T' and 'U' and returns a value of type 'T'. The function iterates over each element in the list 'l' using the 'foreach' method, applying the function 'f' to the current element and the accumulator 'res'. The result of each iteration is stored in the 'res' variable.\n\nThe foldLeft operation is similar to the reduceLeft operation, but it also takes an initial value and applies the given function to each element in the list starting from the leftmost element. In this case, the 'secret' function is accumulating the result of applying the function 'f' to each element in the list starting from the left, which aligns with the behavior of foldLeft. \n\nTherefore, the correct answer is c) foldLeft. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following elements belong to $\\mathbf{Z}_{35}^*$?\n\nOptions:\nA. $12$.\nB. $20$.\nC. $14$.\nD. $15$.\n", "answer": "Explanations: First, let's recall that $\\mathbf{Z}_{35}^*$ represents the set of integers that are relatively prime to $35$ and are less than $35$. \n\nTo determine which elements belong to $\\mathbf{Z}_{35}^*$, we need to find all the numbers that are coprime to $35$. \n\nFirst, let's check if $12$ is coprime to $35$. The greatest common divisor of $12$ and $35$ is $1$, so $12$ belongs to $\\mathbf{Z}_{35}^*$.\n\nNext, let's check if $20$ is coprime to $35$. The greatest common divisor of $20$ and $35$ is $5$, so $20$ does not belong to $\\mathbf{Z}_{35}^*$.\n\nThen, let's check if $14$ is coprime to $35$. The greatest common divisor of $14$ and $35$ is $7$, so $14$ does not belong to $\\mathbf{Z}_{35}^*$.\n\nLastly, let's check if $15$ is coprime to $35$. The greatest common divisor of $15$ and $35$ is $5$, so $15$ does not belong to $\\mathbf{Z}_{35}^*$.\n\nTherefore, the elements that belong to $\\mathbf{Z}_{35}^*$ are $12$. ///Answer: A"}
{"subject": "", "question": "Question: Let P be the statement \u2200x(x>-3 -> x>3). Determine for which domain P evaluates to true: ?\n\nOptions:\nA. -3<x<3\nB. x>-3\nC. x>3\nD. None of the other options\n", "answer": "Explanations: The given statement is \u2200x(x > -3 \u2192 x > 3).\n\nTo determine for which domain the statement evaluates to true, let's consider the implication in the statement:\n\nIf x is greater than -3 (x > -3), then x must be greater than 3 (x > 3).\n\nThis means that for the statement to be true, whenever x is greater than -3, it must also be greater than 3. So, the domain for which the statement evaluates to true is x > 3.\n\nTherefore, the correct option is x > 3. ///Answer: C"}
{"subject": "", "question": "Question: Select all of the regions that ASLR randomizes the address when PIE is not enabled in x86-64 Linux:?\n\nOptions:\nA. The heap\nB. The stack\nC. The libraries\nD. The executable\nE. The gdb server stub\n", "answer": "Explanations: ASLR randomizes the address of the stack, libraries, and executable when PIE is not enabled in x86-64 Linux. The heap is not randomized by ASLR in this scenario. The gdb server stub is also not affected by ASLR. \n\nIncorrect explanation: ASLR randomizes the address of the heap and gdb server stub when PIE is not enabled in x86-64 Linux. The stack, libraries, and executable remain unchanged. ///Answer: A"}
{"subject": "", "question": "Question: Let $s$ be a security parameter and $n$ be a constant. Which of the following functions is negligible?\n\nOptions:\nA. $1/2$.\nB. $1/s$.\nC. $1/e^s$.\nD. $1/s^n$.\n", "answer": "Explanations: ##Answer:\n$1/s^n$.\n\n##Reasons:\n1. A function is considered negligible if it decreases faster than the inverse of any polynomial function as the security parameter $s$ grows. In this case, as $s$ grows, $1/s^n$ will decrease faster than $1/2$, $1/s$, and $1/e^s$.\n2. $1/2$ is a constant, $1/s$ is inversely proportional to $s$, and $1/e^s$ decreases exponentially but is not negligible compared to $1/s^n$ for a constant $n$. ///Answer: D:"}
{"subject": "", "question": "Question: Which statement is true for linear regression?\n\nOptions:\nA. A linear regression model can be expressd as an inner product between feature vectors and a weight vector.\nB. Linear regression, when using 'usual' loss functions, works fine when the dataset contains many outliers.\nC. A good fit with linear regression implies a causal relationship between inputs and outputs.\n", "answer": "Explanations: A linear regression model can be expressed as an inner product between feature vectors and a weight vector. So, the correct statement is: A linear regression model can be expressed as an inner product between feature vectors and a weight vector. ///Answer: A"}
{"subject": "", "question": "Question: Tick all correct answers:?\n\nOptions:\nA. Fuzz testing scales at least to 1'000s of lines of code.\nB. Compiler warnings scale to millions lines of code.\nC. Formal verification scales at least upto 100'000s of lines of code.\nD. Formal verification and concolic execution scale to the same extent.\n", "answer": "Explanations: To determine the correct answers, let's analyze each option:\n\n1) Fuzz testing scales at least to 1'000s of lines of code.\n- **Correct**: Fuzz testing can be applied effectively to large codebases, making this statement true.\n\n2) Compiler warnings scale to millions lines of code.\n- **Correct**: Compiler warnings are capable of scaling to very large codebases, often in the order of millions of lines of code, making this statement true.\n\n3) Formal verification scales at least up to 100'000s of lines of code.\n- **Incorrect**: Formal verification techniques are typically more resource-intensive and may not scale efficiently beyond certain thresholds, usually not as high as hundreds of thousands of lines of code. Therefore, this statement is false.\n\n4) Formal verification and concolic execution scale to the same extent.\n- **Incorrect**: Formal verification and concolic execution are different techniques with varying scalability limits. They do not necessarily scale to the same extent. Therefore, this statement is false.\n\nTherefore, the correct answers are options 1) and 2). ///Answer: A"}
{"subject": "", "question": "Question: A query transformed into the concept space of LSI has?\n\nOptions:\nA. s components (number of singular values)\nB. m components (size of vocabulary)\nC. n components (number of documents)\n", "answer": "Explanations: Question:\nA query transformed into the concept space of LSI has\n\nOptions:\n1. s components (number of singular values)\n2. m components (size of vocabulary)\n3. n components (number of documents)\n\nIn Latent Semantic Indexing (LSI), the concept space is determined by the number of singular values (s components). Singular values are used in the decomposition of the term-document matrix to create the concept space. The number of singular values chosen impacts the dimensionality reduction and the quality of the resulting concept space representation.\n\nTherefore, the correct answer is option 1: s components (number of singular values). ///Answer: A"}
{"subject": "", "question": "Question: We consider a classification problem on linearly separable data. Our dataset had an outlier---a point that is very far from the other datapoints in distance (and also far from margins in SVM but still correctly classified by the SVM classifier).\n                    We trained the SVM, logistic regression and 1-nearest-neighbour models on this dataset.\n                    We tested trained models on a test set that comes from the same distribution as training set, but doesn't have any outlier points.\n                    Consider the $K$-fold cross validation on a linear regression model with a sufficiently large amount of training data. When $K$ is large, the computational complexity of the $K$-fold cross validation with respect to $K$ is of order ?\n\nOptions:\nA. $\\mathcal{O}(K)$.\nB. $\\mathcal{O}(K(K-1))$.\nC. $\\mathcal{O}(1)$.\nD. $\\mathcal{O}(1/K)$\n", "answer": "Explanations: 1. The computational complexity of $K$-fold cross validation is directly related to the number of folds, $K$.\n2. In $K$-fold cross validation, the dataset is divided into $K$ equal-sized folds, and the model is trained and tested $K$ times, with a different fold used as the test set each time.\n3. For each iteration of training and testing, the model's performance metrics are calculated and averaged over all iterations to evaluate the model's overall performance.\n4. The computational complexity of $K$-fold cross validation is not dependent on the size of the training data, but rather on the number of folds, $K$.\n5. As $K$ increases, the computational complexity of $K$-fold cross validation also increases, as more iterations of training and testing are required.\n6. Therefore, the computational complexity of $K$-fold cross validation with respect to $K$ is of order $\\mathcal{O}(K)$, as the complexity scales linearly with the number of folds. ///Answer: A."}
{"subject": "", "question": "Question: Tick the \\textbf{true} statement.\n\nOptions:\nA. If $x \\in \\mathbb{Z}_n^*$ has an order of $m$, then $x^i \\equiv x^{i \\pmod{m}} \\pmod{n} $ for all $i\\in \\mathbb{Z}$.\nB. For all $x \\in \\mathbb{Z}_n$, we have $x^{\\varphi(n)}\\equiv 1 \\pmod{n}$.\nC. For all $n \\geq 2$, $\\mathbb{Z}_n^*$ has order of $n-1$.\nD. For all $n \\geq 2$ and all $x \\in \\mathbb{Z}_n$, $x$ is invertible if and only if $x$ divides $n$.\n", "answer": "Explanations: The true statements are:\n\n1. 'If $x \\in \\mathbb{Z}_n^*$ has an order of $m$, then $x^i \\equiv x^{i \\pmod{m}} \\pmod{n}$ for all $i\\in \\mathbb{Z}': This statement is true because if $x$ has an order of $m$, it means that $x^m \\equiv 1 \\pmod{n}$. Therefore, for any power $i$, we can write $i$ as $q \\cdot m + r$ where $q$ is the quotient and $r$ is the remainder when $i$ is divided by $m$. Then, $x^i \\equiv x^{q \\cdot m + r} \\equiv x^{r} \\pmod{n}$.\n\n2. 'For all $n \\geq 2$, $\\mathbb{Z}_n^*$ has an order of $n-1': This statement is true because $\\mathbb{Z}_n^*$ is the set of all integers relatively prime to $n$. Since there are $\\varphi(n)$ elements in $\\mathbb{Z}_n^*$ (where $\\varphi(n)$ is Euler's totient function), the order of $\\mathbb{Z}_n^*$ is $\\varphi(n)$. For $n \\geq 2$, we have $\\varphi(n) = n-1$.\n\nThe false statements are:\n\n1. 'For all $x \\in \\mathbb{Z}_n$, we have $x^{\\varphi(n)}\\equiv 1 \\pmod{n}': This statement is false because it should be specified that $x$ is an element of $\\mathbb{Z}_n^*$, not just $\\mathbb{Z}_n$. The Euler's theorem states that if $x$ is in $\\mathbb{Z}_n^*$, then $x^{\\varphi(n)} \\equiv 1 \\pmod{n}$.\n\n2. 'For all $n \\geq 2$ and all $x \\in \\mathbb{Z}_n$, $x$ is invertible if and only if $x$ divides $n': This statement is false because for an element $x$ in $\\mathbb{Z}_n$ to be invertible, it must be relatively prime to $n$. The condition for invertibility is not whether $x$ divides $n$, but whether $x$ and $n$ are coprime. ///Answer: A"}
{"subject": "", "question": "Question: Consider the following loaded dice with $6$ faces: $P(S_1=6)=5/6$ and $P(S_1 = x)=1/30$ for $x\\in\\{1,2,3,4,5\\}$. Suppose we throw it indefinitely. Hence, we have a source $S=S_1 S_2 S_3\\ldots$. Then, $H(S_n|S_{n-1}) \neq H(S_n)$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False\n\nThis statement is false because the entropy of a random variable does not change if we condition on another random variable. In other words, $H(S_n|S_{n-1}) = H(S_n)$. The entropy of $S_n$ remains the same regardless of any conditioning. ///Answer: B"}
{"subject": "", "question": "Question: What should the minimal length of the output of a hash function be to provide security against \\emph{collision attacks} of $2^{256}?$?\n\nOptions:\nA. $2^{256}$ bits.\nB. $2^{512}$ bits.\nC. $256$ bits.\nD. $512$ bits.\n", "answer": "Explanations: To provide security against collision attacks of $2^{256}$, we need a hash function output length that can accommodate at least $2^{256}$ different possible outputs without collisions occurring.\n\nIn general, the output length of a hash function is measured in bits. So, to support $2^{256}$ different possible outputs without collisions, the minimal length of the output should be 256 bits. This is because $2^{256}$ is equivalent to $2^{256}$ different possibilities in binary, which can be represented by 256 bits.\n\nTherefore, the correct answer is: '$256$ bits.' ///Answer: C."}
{"subject": "", "question": "Question: Select all the statements that are true.A penalty will be applied for any incorrect answers selected.\n\nOptions:\nA. The Luhn law states that if a set of words are ranked by the decreasing order of their frequencies, the high-ranked words are the best features for identifying the topics that occur in the document collection.\nB. The order of words are ignored in the bag-of-words model.\nC. High values of document frequency means that the word is not very discriminative.\nD. Documents that are orthogonal to each other gives a cosine similarity measure of 1.\nE. Cosine similarity is independent of the length of the documents.\n", "answer": "Explanations: To solve this problem, we need to evaluate each statement based on our knowledge of the key concepts in Natural Language Processing (NLP) as outlined earlier. Let's go through each statement and determine whether it is true or false:\n\n1. Cosine similarity is independent of the length of the documents.\n- This statement is true. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. It is independent of the length of the documents because it normalizes the similarity measure by the lengths of the vectors.\n\n2. The Luhn law states that if a set of words are ranked by the decreasing order of their frequencies, the high-ranked words are the best features for identifying the topics that occur in the document collection.\n- This statement is false. The Luhn law actually states that if a set of words are ranked by the decreasing order of their frequencies, the low-ranked words are the best features for identifying the topics that occur in the document collection. This is because common words like \"the\", \"and\", etc., are less informative for topic identification.\n\n3. High values of document frequency means that the word is not very discriminative.\n- This statement is true. In NLP, document frequency refers to the number of documents in which a word occurs. If a word has a high document frequency, it means that it appears in many documents and may not be very discriminative for distinguishing between different topics or documents.\n\n4. Documents that are orthogonal to each other gives a cosine similarity measure of 1.\n- This statement is false. Documents that are orthogonal to each other (meaning their vectors are perpendicular) give a cosine similarity measure of 0, not 1. Cosine similarity ranges from -1 to 1, with 1 indicating perfect similarity.\n\n5. The order of words is ignored in the bag-of-words model.\n- This statement is true. In the bag-of-words model, the order of words is ignored, and only the frequency of words in the document is considered. This model treats each document as a collection of words without any regard to the order in which they appear.\n\nTherefore, the correct statements are:\n- \"Cosine similarity is independent of the length of the documents.\"\n- \"High values of document frequency means that the word is not very discriminative.\"\n- \"The order of words is ignored in the bag-of-words model.\"\n\nIn this solution, we used our understanding of cosine similarity, document frequency, the bag-of-words model, and the Luhn law to evaluate each statement and determine its correctness in the context of NLP. ///Answer: E:"}
{"subject": "", "question": "Question: Let  $$G= \begin{pmatrix} 1 &1 &1 &0 &1 &0\\ 0 &1 &1 &1 &0 &0\\ 0 &1 &1 &0 &0 &0\\ 0 &1 &1 &1 &0 &1 \\end{pmatrix}$$ be the generator matrix of a $(6,4)$ linear code $\\mathcal C$ over $\\mathbb F_2$. True or false: $d_{min}=2$.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: To determine whether $d_{min} = 2$ for the code generated by the given generator matrix $G$, we need to compute the minimum Hamming distance of the code.\n\nThe minimum Hamming distance of a linear code is the smallest number of positions in which any two distinct codewords differ. It is crucial in determining the error-correcting capability of a code.\n\nTo find the minimum Hamming distance, we typically look for the minimum weight of the non-zero codewords in the code. The weight of a codeword is the number of non-zero entries it contains.\n\nGiven the generator matrix $G$, we can find the codewords of the code by taking all possible linear combinations of the rows of $G$. The resulting codewords are the rows of the parity-check matrix $H$ for the code.\n\nTo simplify the calculations, we can first row-reduce the given generator matrix $G$ to its row-echelon form. This process allows us to identify the linearly independent rows of $G$, which will be the rows of the parity-check matrix $H$.\n\nPerforming row operations on $G$, we get the row-echelon form of $G$ as follows:\n\n$$G = \\begin{pmatrix} 1 & 1 & 1 & 0 & 1 & 0 \\\\ 0 & 1 & 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 1 & 1 \\\\ 0 & 0 & 0 & 0 & 1 & 1 \\end{pmatrix}$$\n\nFrom the row-echelon form, we can see that the first three rows of $G$ are linearly independent, so the rows corresponding to these in $H$ will form a basis for the code's orthogonal complement.\n\nNow, we can generate the parity-check matrix $H$ by taking the rows that are orthogonal to the rows of $G$:\n\n$$H = \\begin{pmatrix} 1 & 0 & 1 & 1 & 0 & 0 \\\\ 0 & 1 & 1 & 0 & 1 & 0 \\end{pmatrix}$$\n\nNext, we need to find the weights of the non-zero codewords of the code. The weights correspond to the number of non-zero entries in each row of $H$.\n\nCalculating the weights of the rows of $H$, we have:\n\nRow 1: Weight = 3\nRow 2: Weight = 3\n\nThe smallest weight among the non-zero codewords is 3, which means the minimum Hamming distance $d_{min}$ of the code is at least 3, not 2.\n\nTherefore, the statement \"False\" is the correct choice. The minimum Hamming distance of the code is not 2; it is at least 3 based on the weights of the non-zero codewords. ///Answer: A"}
{"subject": "", "question": "Question: Consider the plain Rabin cryptosystem with modulus $n=pq$ with two large primes $p,q$. Tick the \\textit{incorrect} assertion.\n\nOptions:\nA. The encryption algorithm has lower complexity than the decryption algorithm.\nB. The decryption problem is equivalent to the problem of computing square roots in $\\mathbb{Z}_n^*$.\nC. Without using any redundancy in the messages, for every ciphertexts we can have up to 3 possible plaintexts.\nD. The key recovery problem is equivalent to the problem of factoring $n$.\n", "answer": "Explanations: The correct answer is: Without using any redundancy in the messages, for every ciphertexts we can have up to 3 possible plaintexts.\n\nIn the plain Rabin cryptosystem, without using any redundancy in the messages, for every ciphertext, there can be up to four possible plaintexts due to the four square roots modulo $n=pq$ that can correspond to a given ciphertext. This ambiguity in decryption is a characteristic of the plain Rabin cryptosystem and can lead to multiple possible plaintexts for a single ciphertext. Therefore, the assertion that there can be up to three possible plaintexts for every ciphertext is incorrect. ///Answer: C:"}
{"subject": "", "question": "Question: In WEP\\dots?\n\nOptions:\nA. the integrity of messages is well protected.\nB. the confidentiality of messages is well protected.\nC. the messages are well authenticated.\nD. there is no good privacy protection.\n", "answer": "Explanations: In WEP (Wired Equivalent Privacy), it is important to understand the main purpose and features of this security protocol. \n\nWEP is a security protocol used to secure wireless networks. However, it has been found to be weak and easily compromised due to vulnerabilities in its design. \n\nLet's break down the options provided:\n\n1. The integrity of messages is well protected: WEP does provide some level of message integrity protection through the use of a checksum in the data frame. However, due to known vulnerabilities, the integrity protection provided by WEP is not considered strong.\n\n2. The confidentiality of messages is well protected: WEP does provide some level of encryption to protect the confidentiality of messages. However, due to known vulnerabilities and the ease with which it can be cracked, the confidentiality provided by WEP is weak.\n\n3. The messages are well authenticated: WEP does not provide strong message authentication. While it does use shared keys for authentication, the authentication mechanism in WEP is also weak and easily bypassed.\n\n4. There is no good privacy protection: This option seems to capture the essence of the issues with WEP accurately. WEP does not provide strong privacy protection due to its known vulnerabilities and weak encryption mechanisms.\n\nTherefore, the most appropriate option in this context is: There is no good privacy protection. ///Answer: D"}
{"subject": "", "question": "Question: If we pick independent random numbers in $\\{1, 2, \\dots, N\\}$ with uniform distribution, $\\theta \\sqrt{N}$ times, we get at least one number twice with probability\\dots?\n\nOptions:\nA. $e^{\\theta ^2}$\nB. $1-e^{\\theta ^2}$\nC. $e^{-\\theta ^2 /2}$\nD. $1-e^{-\\theta ^2 /2}$\n", "answer": "Explanations: Let's denote the probability of not getting any number twice in $\\theta \\sqrt{N}$ draws as $P(\\text{no repeats})$. \n\nIn the first draw, we have $N$ choices, in the second draw we have $N-1$ choices (we can't choose the same number as the first draw), and so on. In the $\\theta \\sqrt{N}$ draws, we have $N, N-1, N-2, \\dots, N-\\theta \\sqrt{N}+1$ choices respectively.\n\nTherefore, the probability of not getting any number twice in $\\theta \\sqrt{N}$ draws is:\n\\[ P(\\text{no repeats}) = \\frac{N}{N} \\times \\frac{N-1}{N} \\times \\frac{N-2}{N} \\times \\dots \\times \\frac{N-\\theta \\sqrt{N}+1}{N} = \\frac{N!}{(N-\\theta \\sqrt{N})!N^{\\theta \\sqrt{N}}} \\]\n\nUsing Stirling's approximation $n! \\approx \\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n$, we have:\n\\[ P(\\text{no repeats}) \\approx \\frac{\\sqrt{2\\pi N} \\left(\\frac{N}{e}\\right)^N}{\\sqrt{2\\pi(N-\\theta \\sqrt{N})} \\left(\\frac{N-\\theta \\sqrt{N}}{e}\\right)^{N-\\theta \\sqrt{N}} N^{\\theta \\sqrt{N}}} \\]\n\nSimplifying, we get:\n\\[ P(\\text{no repeats}) \\approx e^{-\\frac{\\theta^2}{2}} \\]\n\nTherefore, the probability of getting at least one number twice in $\\theta \\sqrt{N}$ draws is:\n\\[ P(\\text{at least one repeat}) = 1 - P(\\text{no repeats}) = 1 - e^{-\\frac{\\theta^2}{2}} \\]\n\nSo, the correct answer is:\n[ ] $1 - e^{-\\theta^2/2}$ ///Answer: D"}
{"subject": "", "question": "Question: Feature degeneracy:?\n\nOptions:\nA. when a set of variables in the feature space are not linearly independent\nB. when a set of variables in the feature space create chaotic results\nC. when a set of variables in the feature space have low impact on the output space\nD. when a point in output space can be obtained by one and only one set of variables in the feature space.\n", "answer": "Explanations: Feature degeneracy occurs when a set of variables in the feature space are not linearly independent. This means that there is redundancy or multicollinearity among the variables, which can lead to issues in the model's performance.\n\nChaotic results can also arise from a set of variables in the feature space that do not provide clear or consistent information to the model. This can make it difficult to interpret the relationships between the features and the output.\n\nFurthermore, features that have low impact on the output space may not contribute significantly to the model's predictive power. This can result in inefficiencies and reduced accuracy in the model's predictions.\n\nLastly, if a point in the output space can be obtained by only one set of variables in the feature space, it indicates that the model may be overfitting to the training data. This lack of generalization can lead to poor performance on new, unseen data.\n\nOverall, feature degeneracy can have a variety of negative impacts on a model's performance, including instability, inefficiency, reduced accuracy, and overfitting. It is important to carefully consider feature selection and engineering to avoid these issues and improve the overall quality of the model. ///Answer: A"}
{"subject": "", "question": "Question: Consider the following context-free grammar \\(G\\) (where \\(\\text{S}\\) is the top-level symbol):\n\n\\(R_{01}: \\text{S} \\rightarrow \\text{NP VP}\\)\n\\(R_{02}: \\text{NP} \\rightarrow \\text{NP0}\\)\n\\(R_{03}: \\text{NP} \\rightarrow \\text{Det NP0}\\)\n\\(R_{04}: \\text{NP0} \\rightarrow \\text{N}\\)\n\\(R_{05}: \\text{NP0} \\rightarrow \\text{Adj N}\\)\n\\(R_{06}: \\text{NP0} \\rightarrow \\text{NP0 PNP}\\)\n\\(R_{07}: \\text{VP} \\rightarrow \\text{V}\\)\n\\(R_{08}: \\text{VP} \\rightarrow \\text{V NP}\\)\n\\(R_{09}: \\text{VP} \\rightarrow \\text{V NP PNP}\\)\n\\(R_{10}: \\text{PNP} \\rightarrow \\text{Prep NP}\\)\n\ncomplemented by the lexicon \\(L\\):\na        : Det\nblue     : Adj, N\ndrink    : N, V\ndrinks   : N, V\nfriends  : N\nfrom     : Prep\ngave     : V\nletter   : N\nmy       : Det\nneighbor : N\nnice     : Adj, N\nof       : Prep\npostman  : N\nran      : V\nthe      : Det\nto       : PrepHow many parse trees does the grammar \\(G\\) associate to the word sequence\"the postman ran the letter for the drinks on the friends\"?\n\nOptions:\nA. 0\nB. 1\nC. 2\nD. 4\nE. 8\n", "answer": "Explanations: The word sequence \"the postman ran the letter for the drinks on the friends\" consists of 9 words. Let's break down the sentence structure and see how many different parse trees can be generated by the grammar \\(G\\) for this word sequence.\n\n1. \"the postman\": This can be parsed as \\(\\text{Det NP0}\\) using rules \\(R_{03}\\) and \\(R_{04}\\).\n2. \"ran\": This can be parsed as \\(\\text{V}\\) using rule \\(R_{07}\\).\n3. \"the letter\": This can be parsed as \\(\\text{Det NP0}\\) using rules \\(R_{03}\\) and \\(R_{04}\\).\n4. \"for the drinks\": This can be parsed as \\(\\text{Prep NP}\\) using rule \\(R_{10}\\) and \\(\\text{Det NP0}\\) using rules \\(R_{03}\\) and \\(R_{04}\\).\n5. \"on the friends\": This can be parsed as \\(\\text{Prep NP}\\) using rule \\(R_{10}\\) and \\(\\text{Det NP0}\\) using rules \\(R_{03}\\) and \\(R_{04}\\).\n\nNow, let's count the number of different parse trees that can be generated for this word sequence:\n- The first phrase \"the postman\" can be parsed in 1 way.\n- The verb \"ran\" can be parsed in 1 way.\n- The second phrase \"the letter\" can be parsed in 1 way.\n- The prepositional phrase \"for the drinks\" can be parsed in 1 way.\n- The prepositional phrase \"on the friends\" can be parsed in 1 way.\n\nTherefore, the total number of parse trees for the word sequence \"the postman ran the letter for the drinks on the friends\" is \\(1 \\times 1 \\times 1 \\times 1 \\times 1 = 1\\).\n\nSo, the answer is 1 parse tree. ///Answer: B."}
{"subject": "", "question": "Question: When using linear regression, how do you help prevent numerical instabilities? (One or multiple answers)?\n\nOptions:\nA. reduce learning rate\nB. add a regularization term\nC. remove degenerate features\nD. add more features\n", "answer": "Explanations: To help prevent numerical instabilities in linear regression, the following methods can be employed:\n\n1) **Reduce learning rate**: By decreasing the learning rate, the size of the steps taken during gradient descent is reduced. This helps prevent overshooting the minimum of the cost function, which can lead to large numerical values and instability.\n\n2) **Add a regularization term**: Regularization techniques like L1 (Lasso) or L2 (Ridge) regularization add a penalty term to the cost function. This penalty discourages the coefficients from taking on large values, thus preventing numerical instabilities that may arise from overly large coefficients.\n\n3) **Remove degenerate features**: Degenerate features are features that are linearly dependent or nearly linearly dependent on other features. These features can cause numerical instabilities by inflating the magnitude of the coefficients. Removing such features can help stabilize the model.\n\nIt is important to note that adding more features (option 4) does not directly address numerical instabilities. In fact, adding more features could potentially introduce multicollinearity issues, leading to numerical instabilities. Therefore, option 4 is not a suitable method for preventing numerical instabilities in linear regression. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. Let $X$ be a random variable that is equal to zero with probability 1/2 and to 1 with probability 1/2. Since the variance of $X$ is $V(X)=E((X-E(X))^2)= E(X^2)-(E(X))^2$, here $V(X)$ is:?\n\nOptions:\nA. 1/4\nB. 1/2\nC. 1/3\nD. 1/8\n", "answer": "Explanations: First, let's find the expected value $E(X)$ of the random variable $X$:\n\n$E(X) = 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} = \\frac{1}{2}$\n\nNext, let's find $E(X^2)$:\n\n$E(X^2) = 0^2 \\cdot \\frac{1}{2} + 1^2 \\cdot \\frac{1}{2} = \\frac{1}{2}$\n\nNow, we can calculate the variance $V(X)$ using the formula $V(X) = E(X^2) - (E(X))^2$:\n\n$V(X) = \\frac{1}{2} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}$\n\nTherefore, the correct answer is $\\boxed{1/4}$. ///Answer: A:"}
{"subject": "", "question": "Question: Let $n$ be an integer. The extended Euclidean algorithm is typically used to\\dots?\n\nOptions:\nA. \\dots perform the addition of two integers in $\\mathbf{Z}_n^*$.\nB. \\dots compute the inverse of an element in $\\mathbf{Z}_n^*$.\nC. \\dots compute the square of an element of $\\mathbf{Z}_n^*$.\nD. \\dots compute the order of $\\mathbf{Z}_n^*$.\n", "answer": "Explanations: Reasoning:\n1. The Extended Euclidean Algorithm is a method used to find the greatest common divisor (GCD) of two integers and express the GCD as a linear combination of the two integers.\n2. In the context of modular arithmetic, the Extended Euclidean Algorithm is commonly used to find the modular multiplicative inverse of an element in a finite field, denoted as $\\mathbf{Z}_n^*$.\n3. The modular multiplicative inverse of an element $a$ in $\\mathbf{Z}_n^*$ is another element $b$ such that $ab \\equiv 1 \\pmod{n}$.\n4. The Extended Euclidean Algorithm is not used to perform addition, square computation, or compute the order of elements in $\\mathbf{Z}_n^*$.\n \\dots compute the inverse of an element in $\\mathbf{Z}_n^*$. ///Answer: B"}
{"subject": "", "question": "Question: Given that JOS has correctly initialized the IDT and installed all the interrupt handlers. Which of the following will JOS do if the CPU with CPL = 3 tries to read the memory in data segment with DPL = 0?\n\nOptions:\nA. Calling the Page Fault Handler.\nB. Calling the General Protection Fault handler.\nC. Shuting down the system .\nD. Reading out the memory content successfully.\n", "answer": "Explanations: In this scenario, we are dealing with memory protection mechanisms in a computing system. Let's break down the options and reasoning behind each one:\n\nOption 0: Calling the Page Fault Handler.\n- If the CPU with CPL = 3 (Current Privilege Level) tries to read memory in a data segment with DPL = 0 (Descriptor Privilege Level), it means that the CPU is trying to access memory with a lower privilege level than its own. This violates the protection mechanism in place. Therefore, the system would typically generate a page fault to handle this violation.\n\nOption 1: Calling the General Protection Fault handler.\n- General Protection Faults often occur when there is a violation of memory protection, such as accessing memory with an inappropriate privilege level. In this case, since the CPU with CPL = 3 is attempting to read memory in a data segment with DPL = 0, it is likely to trigger a General Protection Fault rather than a Page Fault.\n\nOption 2: Shutting down the system.\n- Shutting down the system is an extreme action and is not the typical response to a memory protection violation. The system would usually handle such violations through exception handling mechanisms rather than abruptly shutting down.\n\nOption 3: Reading out the memory content successfully.\n- This option is incorrect because if the CPU with CPL = 3 tries to read memory in a data segment with DPL = 0, it represents a violation of memory protection rules. As a result, the system should not allow the reading of memory content successfully in this scenario.\n\nBased on the analysis, the most appropriate option in this context would be:\nOption 1: Calling the General Protection Fault handler.\n\nThis choice aligns with the expected behavior when a CPU with a higher privilege level attempts to access memory with a lower privilege level, triggering a General Protection Fault as a response to the violation of memory protection rules. ///Answer: B"}
{"subject": "", "question": "Question: Which one of these is \\emph{not} a hard computational problem?\n\nOptions:\nA. Factoring.\nB. Extracting square roots.\nC. Computing the Jacobi symbol.\nD. Computing the discrete log.\n", "answer": "Explanations: Among the options provided, \"Extracting square roots\" is \\emph{not} a hard computational problem.\n\nTo understand why, let's briefly discuss each of the options:\n\n1. Factoring: Factoring involves breaking down a number into its prime factors. It is considered a hard computational problem, especially for large numbers, as it forms the basis of many encryption algorithms like RSA.\n\n2. Extracting square roots: Extracting square roots involves finding a number that, when multiplied by itself, gives a specified number. While this is a computational problem, it is not as hard as the other options listed.\n\n3. Computing the Jacobi symbol: The Jacobi symbol is a generalization of the Legendre symbol and is used in number theory. Computing the Jacobi symbol can be a computationally intensive task and is considered a hard problem.\n\n4. Computing the discrete log: The discrete logarithm problem involves finding the exponent to which a given number (the base) must be raised to produce another given number (the result). This problem is also considered hard and forms the basis of various cryptographic protocols.\n\nTherefore, among the options provided, \"Extracting square roots\" is the one that is \\emph{not} a hard computational problem compared to factoring, computing the Jacobi symbol, and computing the discrete log. ///Answer: B"}
{"subject": "", "question": "Question: Let $h$ be a cryptographic hash function based on the Merkle-Damg{\\aa}rd scheme. The Merkle-Damg{\\aa}rd Theorem states that\\dots?\n\nOptions:\nA. \\dots $h$ is collision-resistant.\nB. \\dots $h$ is resistant to a first preimage attack.\nC. \\dots if the compression function is collision-resistant, then $h$ is collision-resistant.\nD. \\dots if $h$ is collision-resistant, then the compression function is collision-resistant.\n", "answer": "Explanations: The Merkle-Damg{\\aa}rd Theorem states that if the compression function in a Merkle-Damg{\\aa}rd construction is collision-resistant, then the hash function built using this construction is collision-resistant. \n\nTherefore, the correct answer is: Answer 3: if the compression function is collision-resistant, then $h$ is collision-resistant. ///Answer: C"}
{"subject": "", "question": "Question: A basic statement in RDF would be expressed in the relational data model by a table?\n\nOptions:\nA. with one attribute\nB. with two attributes\nC. with three attributes\nD. cannot be expressed in the relational data model\n", "answer": "Explanations: The correct answer is 'cannot be expressed in the relational data model'.\n\nExplanation: RDF (Resource Description Framework) is a graph-based data model used to represent information in the form of subject-predicate-object triples. Each triple consists of a subject, a predicate, and an object. This structure is not easily represented in a relational data model, which uses tables with rows and columns to store data. \n\nIn RDF, a basic statement would typically involve at least three attributes: subject, predicate, and object. Each attribute would represent a different entity or relationship in the data. However, in a relational data model, it is not possible to directly represent this structure in a single table. \n\nTherefore, a basic statement in RDF cannot be expressed in the relational data model using a table with one, two, or three attributes. Instead, a more complex relational database design would be needed to capture the same information as in an RDF graph. ///Answer: D"}
{"subject": "", "question": "Question:  Consider the Parametric ReLU function defined as\n            $$f(x) = \\left\\{\begin{matrix}\n                    x  & \textup{if}\\; x > 0 \\\n                    ax & \textup{otherwise}\n                \\end{matrix}\right.$$\n            where $a \\in \\R$ is an arbitrary number.\n            Which of the following statements is true regarding the subgradients of $f(x)$ at $x = 0$?\n        ?\n\nOptions:\nA. A subgradient exists even though $f(x)$ is not necessarily differentiable at $x=0$.\nB. A subgradient does not exist at $x=0$.\nC. If a subgradient exists, then it is not unique.\nD. None of the mentioned answers.\n", "answer": "Explanations: First, let's recall that a subgradient of a function at a point is a generalization of the concept of a derivative for functions that are not necessarily differentiable. \n\nIn the case of the Parametric ReLU function $f(x)$ at $x=0$, we can see that the function is not differentiable at this point because the left and right limits do not match. However, we can still consider subgradients at this point.\n\nSince $f(x)$ is defined differently for $x > 0$ and $x \\leq 0$, we need to consider the subgradients separately for each case.\n\nFor $x > 0$, the function is simply $f(x) = x$, which is differentiable at $x=0$ with derivative 1. Therefore, a subgradient exists at $x=0$ for $x > 0$.\n\nFor $x \\leq 0$, the function is $f(x) = ax$. Since $f(0) = 0$, the subgradient at $x=0$ is the set of values in the interval $[-a, a]$, as any value in this range satisfies the definition of a subgradient.\n\nTherefore, in conclusion, the statement \"A subgradient exists even though $f(x)$ is not necessarily differentiable at $x=0\" is true. Additionally, since there are multiple possible values for the subgradient at $x=0$ depending on the value of $a$, the statement \"If a subgradient exists, then it is not unique\" is also true. \n\nThus, the correct answer is: \n- A subgradient exists even though $f(x)$ is not necessarily differentiable at $x=0.\n- If a subgradient exists, then it is not unique. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is/are true about fuzzing libraries?\n\nOptions:\nA. Fuzzing libraries is harder than standalone executables as no\n               single ``binary'' is available for a complex API.\nB. To fuzz effectively, every argument of every function must be\n                  fuzzed independently.\nC. Every function of a library is part of the API that needs to be fuzzed.\nD. FuzzGen's A\\textsuperscript{2}DG contains the control but not\n                  the data dependencies of the API calls.\n", "answer": "Explanations: The correct option is: To fuzz effectively, every argument of every function must be fuzzed independently.\n\nExplanation:\n\n- Fuzzing libraries can be more challenging than standalone executables due to the lack of a single \"binary\" for a complex API. This complexity arises from the need to understand and interact with various functions and components within the library.\n- Fuzzing effectively involves testing the input parameters of functions thoroughly to identify vulnerabilities and bugs. Each argument of every function should be fuzzed independently to cover a wide range of inputs and scenarios.\n- While every function of a library is indeed part of the API that needs to be fuzzed, the effectiveness of the fuzzing process lies in systematically testing each argument of each function independently to uncover potential issues.\n- FuzzGen's A\\textsuperscript{2}DG, which stands for Automatic API Directed Greybox Fuzzing, focuses on generating inputs that exercise different code paths. It does consider both control and data dependencies of API calls to create diverse and effective test cases. ///Answer: B"}
{"subject": "", "question": "Question: Which cryptographic primitive(s) is (are) used in S/Key - OTP ?\n\nOptions:\nA. Only encryption and a hash function\nB. Only encryption and a MAC algorithm\nC. Only a hash function\nD. Only a MAC\n", "answer": "Explanations: To determine the correct cryptographic primitive(s) used in S/Key - OTP, we need to understand the nature of S/Key - OTP and the typical cryptographic methods used in such systems.\n\nS/Key (one-time password) is a system that uses a sequence of one-time passwords for authentication. Each password is only used once and can't be reused. The system typically relies on a secure algorithm to generate these passwords.\n\nLet's evaluate each possible answer:\n\n1. Only encryption and a hash function: This answer is incorrect. While encryption and hash functions are commonly used in cryptographic systems, S/Key - OTP primarily relies on a hash function for generating the one-time passwords.\n\n2. Only encryption and a MAC algorithm: This answer is incorrect. While encryption and MAC (Message Authentication Code) algorithms are important in cryptography, they are not the main cryptographic primitives used in S/Key - OTP.\n\n3. Only a hash function: This answer is correct. S/Key - OTP primarily uses a hash function to generate the one-time passwords. The passwords are typically generated by repeatedly applying a cryptographic hash function to a secret key.\n\n4. Only a MAC: This answer is incorrect. MACs are used for verifying the integrity and authenticity of a message, not typically for generating one-time passwords in systems like S/Key - OTP.\n\nBased on the evaluation above, the correct answer is: {'Correct Answer(s)': 3. Only a hash function}. ///Answer: C"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion. A hash function can have the following roles:?\n\nOptions:\nA. Domain extender.\nB. Block cipher.\nC. Commitment.\nD. Pseudorandom generator.\n", "answer": "Explanations: To solve this problem, we need to understand the roles of the options provided:\n\n1. Domain extender: A domain extender is a function that takes a short input and produces a longer output. It is commonly used to extend the domain of hash functions to increase security.\n\n2. Block cipher: A block cipher is a cryptographic algorithm that operates on fixed-length groups of bits, called blocks. It is used for encryption and decryption.\n\n3. Commitment: Commitment schemes are cryptographic protocols that allow a party to commit to a chosen value while keeping it hidden from others, with the ability to reveal the committed value later.\n\n4. Pseudorandom generator: A pseudorandom generator is an algorithm that generates a sequence of numbers that appear random but are actually determined by a small set of initial values.\n\nNow, we need to identify the false assertion among the given options:\n\n- A hash function can act as a domain extender by extending the domain of the input data.\n- A hash function is not a block cipher. A block cipher is a separate cryptographic primitive used for encryption purposes.\n- Commitment can be achieved using cryptographic protocols, but it is not a direct role of a hash function.\n- A hash function can act as a pseudorandom generator by producing output that appears random.\n\nThe false assertion is \"Block cipher.\" A hash function is not a block cipher; they are different cryptographic primitives with distinct purposes and characteristics.\n\nTherefore, the false assertion is: Block cipher. ///Answer: B"}
{"subject": "", "question": "Question: For security reasons, you accept the performance and memory overhead\n    introduced by common sanitizers and deploy them in your user-facing\n    production server software.\n    Assuming that all memory safety bugs in your software are detected by the\n    sanitizers, which of the following properties do the sanitizers provide to\n    your code?\n\nOptions:\nA. Confidentiality of the program data\nB. Integrity of the program data\nC. Availability of the program\nD. Accountability of accesses to the program\n", "answer": "Explanations: {'Correct Answer(s)': '2. Integrity of the program data'}\n\nExplanation:\n1. Confidentiality of the program data: Sanitizers do not directly provide confidentiality of program data. They focus on identifying memory safety bugs such as buffer overflows, use-after-free errors, etc. to prevent security vulnerabilities. Confidentiality is typically addressed through encryption, access control mechanisms, etc.\n\n2. Integrity of the program data: This is the correct answer. Sanitizers help ensure the integrity of program data by detecting and preventing memory safety bugs that could potentially corrupt or manipulate data unintentionally.\n\n3. Availability of the program: While memory bugs can potentially lead to crashes and affect availability, the primary focus of sanitizers is on identifying and preventing memory safety issues rather than directly ensuring program availability.\n\n4. Accountability of accesses to the program: Sanitizers do not directly provide accountability of accesses to the program. This aspect is usually addressed through logging, auditing, and access control mechanisms. ///Answer: B"}
{"subject": "", "question": "Question: Consider the following joint distribution on $X$ and $Y$, where $X \\in\\{-1,0,1\\}$ and $Y \\in\\{0,1\\}: p(X=-1, Y=0)=0.05$, $p(X=-1, Y=1)=0.05, p(X=0, Y=0)=0.1, p(X=0, Y=1)=0.1, p(X=1, Y=0)=0.3, p(X=1, Y=1)=$ 0.4. You learn that $X \\geq 0$. What is the largest probability of being correct you can achieve when predicting $Y$ in this case?\n\nOptions:\nA. $\\frac{5}{9}$\nB. $1$\nC. $\\frac{2}{3}$\nD. $\\frac{1}{4}$\nE. $\\frac{1}{2}$\nF. $\\frac{1}{7}$\nG. $\\frac{1}{3}$\nH. $\\frac{6}{7}$\nI. $0$\nJ. $\\frac{4}{7}$\nK. $\\frac{3}{7}$\n", "answer": "Explanations: Given that $X\\geq 0$, we need to calculate the conditional probabilities $P(Y=0|X\\geq 0)$ and $P(Y=1|X\\geq 0)$. \n\nFirst, we find $P(X\\geq 0)$ by summing the probabilities for $X=0$ and $X=1$:\n$P(X\\geq 0) = 0.1 + 0.3 = 0.4$\n\nNext, we calculate the conditional probabilities:\n$P(Y=0|X\\geq 0) = \\frac{P(X=0,Y=0) + P(X=1,Y=0)}{P(X\\geq 0)}$\n$P(Y=0|X\\geq 0) = \\frac{0.1 + 0.3}{0.4} = \\frac{0.4}{0.4} = 1$\n\n$P(Y=1|X\\geq 0) = \\frac{P(X=0,Y=1) + P(X=1,Y=1)}{P(X\\geq 0)}$\n$P(Y=1|X\\geq 0) = \\frac{0.1 + 0.4}{0.4} = \\frac{0.5}{0.4} = \\frac{5}{4} = \\frac{5}{9}$\n\nTherefore, the largest probability of being correct when predicting $Y$ given $X\\geq 0$ is $\\frac{5}{9}$. So, Option 1: $\\frac{5}{9}$ is the correct answer. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is wrong regarding Ontologies?\n\nOptions:\nA. We can create more than one ontology that conceptualizes the same real-world entities\nB. Ontologies help in the integration of data expressed in different models\nC. Ontologies dictate how semi-structured data are serialized\nD. Ontologies support domain-specific vocabularies\n", "answer": "Explanations: The wrong statement regarding Ontologies is: \"Ontologies dictate how semi-structured data are serialized.\" Ontologies do not dictate how data is serialized, rather they help in organizing and structuring knowledge in a specific domain. ///Answer: C"}
{"subject": "", "question": "Question: Select \\emph{incorrect} statement.\n\nOptions:\nA. The high entropy of password is ensured by hashing the password before storing.\nB. A random salt provides some level of protection against dictionary attack.\nC. A password of length $8$ characters should contain non-alphanumerical characters in order to be non-recoverable by brute force.\nD. A special purpose device can recover $56$ bit passwords in a matter of minutes.\n", "answer": "Explanations: ##Answer:\nA password of length $8$ characters should contain non-alphanumerical characters in order to be non-recoverable by brute force.\n\n##Reasons:\n1. The statement that a password of length $8$ characters should contain non-alphanumerical characters in order to be non-recoverable by brute force is incorrect.\n2. Password strength is primarily determined by length, randomness, and unpredictability rather than the inclusion of special characters.\n3. Longer passwords with a mix of characters (alphanumeric, symbols, etc.) are generally more secure against brute force attacks.\n4. Adding non-alphanumeric characters can increase the complexity of passwords, but it is not a strict requirement for password security. Other factors like length, randomness, and the use of salting and hashing play a more significant role in password security. ///Answer: C"}
{"subject": "", "question": "Question: Choose the \\emph{correct} statement?\n\nOptions:\nA. Elliptic curves form a field.\nB. Elliptic curves form a ring.\nC. Elliptic curves form an Abelian group.\nD. Elliptic curves form an ideal.\n", "answer": "Explanations: The correct statement is: \"Elliptic curves form an Abelian group.\"\n\nExplanation:\n\n1. **Group Structure**:\n   - Elliptic curves have a natural group structure defined on them. This group structure arises from the geometric properties of the points on the curve.\n   - Given an elliptic curve, the points on the curve along with an additional \"point at infinity\" form an Abelian group denoted as \\(E(\\mathbb{R})\\) or \\(E(\\mathbb{F}_q)\\), depending on whether we are working with real or finite fields.\n\n2. **Abelian Group**:\n   - An Abelian group is a set equipped with an operation (usually denoted as addition) that satisfies the group axioms (closure, associativity, identity element, inverse element, and commutativity).\n   - The group structure on elliptic curves satisfies all these properties, making it an Abelian group.\n\n3. **Not a Field**:\n   - While it is true that the set of points on an elliptic curve can form a field extension over the base field, the elliptic curve itself as a set of points does not form a field.\n\n4. **Not a Ring or an Ideal**:\n   - Elliptic curves do not form a ring, as a ring requires both addition and multiplication operations with specific properties.\n   - Elliptic curves also do not form an ideal, which is a subset of a ring with special properties related to multiplication within the ring.\n\nIn conclusion, the correct statement is that \"Elliptic curves form an Abelian group\" due to the natural group structure defined on them satisfying the group axioms. ///Answer: C"}
{"subject": "", "question": "Question: (Stochastic Gradient Descent, cont) SGD in typical machine learning problems requires fewer parameter updates to converge than full gradient descent.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False\n\nIn typical machine learning problems, Stochastic Gradient Descent (SGD) actually requires more parameter updates to converge compared to full gradient descent. This is because in SGD, instead of computing the gradient of the loss function using the entire training dataset (as in full gradient descent), we compute the gradient using only a subset of the data (a mini-batch). While this can lead to faster updates and convergence in terms of computational efficiency, it also introduces more variance in the parameter updates, which can result in more oscillations during the optimization process and potentially slower convergence. ///Answer: B"}
{"subject": "", "question": "Question: When using the plain ElGamal cryptosystem over the group $\\mathbb{Z}_p^*$ with a fixed $\\ell$-bit prime $p$ and a fixed generator $g\\in\\mathbb{Z}_p^*$, the \\textit{tightest} complexity of generating a new key-pair is\\ldots?\n\nOptions:\nA. $\\mathcal{O}(4{\\ell})$.\nB. $\\mathcal{O}(\\ell^3)$.\nC. $\\mathcal{O}(\\ell^2)$.\nD. $\\mathcal{O}(\\log{\\ell}^4)$.\n", "answer": "Explanations: To generate a new key pair in ElGamal cryptosystem, we typically need to randomly choose a private key $x$ from the set $\\{1, 2, \\ldots, p-2\\}$, compute the corresponding public key $y = g^x \\bmod p$, and return the key pair $(y, x)$. \n\nThe tightest complexity for generating a new key pair involves finding the discrete logarithm $x = \\log_g y \\bmod p$. The best known algorithms for solving the discrete logarithm problem in general have a complexity of $\\mathcal{O}(\\sqrt{p}) \\approx \\mathcal{O}(2^{\\ell/2})$ when applied to $\\mathbb{Z}_p^*$.\n\nTherefore, the tightest complexity of generating a new key-pair in ElGamal cryptosystem over $\\mathbb{Z}_p^*$ with a fixed $\\ell$-bit prime $p$ and a fixed generator $g$ is $\\mathcal{O}(2^{\\ell/2})$ or equivalently $\\mathcal{O}(\\ell)$. \n\nAmong the given options, the closest one to this result is:\n\nA: $\\mathcal{O}(4{\\ell})$.\n\nSo, the correct choice is:\n\nA: $\\mathcal{O}(4{\\ell})$. ///Answer: A"}
{"subject": "", "question": "Question: How many different simple substitution ciphers do exist with respect to an alphabet of 26 characters?\n\nOptions:\nA. $26!$\nB. $2^{26}$\nC. $26^2$\nD. 26\n", "answer": "Explanations: The correct answer is (a) $26!$. This is because there are 26 different choices for the letter that A could be replaced with, 25 choices for the letter that B could be replaced with (since it can't be the same as A), and so on. This results in a total of 26 factorial, or $26!$, different simple substitution ciphers possible. ///Answer: A"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. Elliptic Curve Diffie-Hellman is?\n\nOptions:\nA. based on the difficulty of factoring the polynomial of EC.\nB. based on the difficulty of computing the discrete logarithm in EC.\nC. used in Bluetooth 2.1.\nD. used for epassports.\n", "answer": "Explanations: 'The correct answer is option 0: \"based on the difficulty of factoring the polynomial of EC.\"\\n\\nExplanation:\\n1. Option 0 states that Elliptic Curve Diffie-Hellman is based on the difficulty of factoring the polynomial of EC. This statement is incorrect because Elliptic Curve Diffie-Hellman (ECDH) is based on the difficulty of computing the discrete logarithm in EC, not factoring the polynomial of EC.\\n\\n2. Option 1 correctly states that ECDH is based on the difficulty of computing the discrete logarithm in EC. This is a fundamental concept in the security of elliptic curve cryptography.\\n\\n3. Option 2 correctly mentions that ECDH is used in Bluetooth 2.1. ECDH is a popular key exchange algorithm used in various protocols and systems for secure communication.\\n\\n4. Option 3 correctly states that ECDH is used for epassports. ECDH is used in various applications where secure key exchange is required, such as electronic passports.\\n\\nTherefore, option 0 is the incorrect statement as it misrepresents the basis of Elliptic Curve Diffie-Hellman.' ///Answer: A"}
{"subject": "", "question": "Question: Let $X$ be a random variable distributed over the alphabet $\\mathcal{X}=\\{0,1,\\ldots,n\\}$. Assume also that there exist $x_1,x_2\\in\\mathcal{X}$ such that $p_X(x_1)\neq p_X(x_2)$ (\textit{i.e.}, $X$ is not uniformly distributed over $\\mathcal{X}$). Let $Y=2^X$ and $Z=\\lfloor X/2 \rfloor$.  True or false: $H(Y)\\geq \\log_2(n+1)$?\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The entropy of a discrete random variable $Y$ is defined as $H(Y) = -\\sum_{y\\in\\mathcal{Y}} p_Y(y) \\log_2 p_Y(y)$, where $\\mathcal{Y}$ is the range of $Y$.\n\nIn this case, $Y = 2^X$, which means the possible values of $Y$ are $2^0, 2^1, \\ldots, 2^n$. Since $X$ is not uniformly distributed over $\\mathcal{X}$, there exist $x_1, x_2 \\in \\mathcal{X}$ such that $p_X(x_1) \\neq p_X(x_2)$.\n\nNow, let's consider the distribution of $Y=2^X$. Since $Y$ is a deterministic function of $X$, we have $p_Y(y) = p_X(x)$, where $y=2^x$. As $p_X(x_1) \\neq p_X(x_2)$, it follows that $p_Y(2^{x_1}) \\neq p_Y(2^{x_2})$.\n\nTherefore, $Y$ is not uniformly distributed over $\\{2^0, 2^1, \\ldots, 2^n\\}$. This implies that $H(Y) < \\log_2(n+1)$, so the statement $H(Y) \\geq \\log_2(n+1)$ is false.\n\nTherefore, the correct option is:\n\nOption 1: False ///Answer: B:"}
{"subject": "", "question": "Question: Let the Boolean query be represented by {(1, 0, -1), (0, -1, 1)}  and the document by (1, 0, 1). The document ?\n\nOptions:\nA. matches the query because it matches the first query vector\nB. matches the query because it matches the second query vector\nC. does not match the query because it does not match the first query vector\nD. does not match the query because it does not match the second query vector\n", "answer": "Explanations: matches the query because it matches the first query vector.\n\nTo determine if the document matches the query, we need to compare each component of the document vector to the corresponding component of the query vector. \n\nFor the first query vector (1, 0, -1):\n- The first component of the query is 1, and the first component of the document is also 1. They match.\n- The second component of the query is 0, and the second component of the document is 0. They match.\n- The third component of the query is -1, and the third component of the document is 1. They do not match.\n\nSince one of the components does not match, the document does not match the first query vector. ///Answer: C"}
{"subject": "", "question": "Question: Can documents which do not contain any keywords of the original query receive a positive similarity coefficient after relevance feedback?\n\nOptions:\nA. No\nB. Yes, independent of the values \u03b2 and \u03b3\nC. Yes,but only if \u03b2>0\nD. Yes,but only if \u03b3>0\n", "answer": "Explanations: Let's break this down step by step for better understanding:\n\n1. **What is relevance feedback?**\nRelevance feedback is a process in information retrieval where the system takes into account the feedback provided by the user on the initial set of retrieved documents to improve the relevance of subsequent search results.\n\n2. **What are \u03b2 and \u03b3 in relevance feedback?**\nIn relevance feedback, \u03b2 and \u03b3 are usually used to assign weights to the original query and the feedback information. Here, \u03b2 is the weight assigned to the original query and \u03b3 is the weight assigned to the feedback information.\n\n3. **Can documents without any keywords of the original query receive a positive similarity coefficient after relevance feedback?**\n- The answer is **\"Yes, but only if \u03b3 > 0.\"**\n- If the weight assigned to the feedback information (\u03b3) is greater than zero, it means that the system is considering the feedback provided by the user. In this case, even if the documents do not contain any keywords from the original query, they can potentially receive a positive similarity coefficient based on the feedback information.\n- However, if \u03b3 = 0, then the system is not considering the feedback information, and documents without any keywords of the original query are unlikely to receive a positive similarity coefficient.\n\nSo, the correct response is: **Yes, but only if \u03b3 > 0.** ///Answer: D"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion.\n\nOptions:\nA. The only way of finding a collision in a hash function while using (almost) no memory is exhaustive search.\nB. Floyd's cycle finding algorithm returns the closest bike shop.\nC. Exhaustive search is always a concern in cryptographic schemes.\nD. $\\mathbb{Z}_p^*$ is a field.\n", "answer": "Explanations: Reasoning:\n1. Finding a collision in a hash function means identifying two different inputs that produce the same hash output.\n2. Exhaustive search for a collision involves trying all possible input combinations until a collision is found, which is a brute force method.\n3. Floyd's cycle-finding algorithm is used to detect cycles in linked lists and is not related to cryptographic concepts like collision finding in hash functions.\n4. Exhaustive search is a concern in cryptographic schemes, especially when the key space or hash output space is small enough to be feasibly searched.\n5. $\\mathbb{Z}_p^*$ represents the multiplicative group of integers modulo a prime $p$, and it forms a group under multiplication when $p$ is a prime number.\n Exhaustive search is always a concern in cryptographic schemes. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following sets can you use Cantor\u2019s Diagonalization Argument to prove it is uncountable (multiple answers) ?\n\nOptions:\nA. The set of decimal representations of negative real numbers .\nB. Real numbers not containing 7 in their decimal representation.\nC. The set binary strings with finite length (i.e. 01, 10, 010, ...)\nD. Integers between 0 and 1\nE. Real numbers between 1 and 2.\n", "answer": "Explanations: A. The set of decimal representations of negative real numbers. Cantor's Diagonalization Argument can be used to show that this set is uncountable. If we attempt to list all the decimal representations of negative real numbers, we can construct a new real number by taking the diagonal digits of this list and changing them. This new real number will differ from every number in the list, proving that the set is uncountable.\n\nB. Real numbers not containing 7 in their decimal representation. Cantor's Diagonalization Argument can also be used to show that this set is uncountable. If we try to list all real numbers not containing 7 in their decimal representation, we can construct a new real number by taking the diagonal digits of this list and changing them. This new real number will not be present in the original list, demonstrating that the set is uncountable.\n\nE. Real numbers between 1 and 2. Cantor's Diagonalization Argument can once again be applied to prove that this set is uncountable. When attempting to list all real numbers between 1 and 2, a new real number can be constructed by taking the diagonal digits of this list and altering them. The resulting real number will not be found in the original list, indicating that the set is uncountable. ///Answer: A"}
{"subject": "", "question": "Question: For his awesome research, Tugrulcan is going to use the PageRank with teleportation and HITS algorithm, not on a network of webpages but on the retweet network of Twitter! The retweet network is a directed graph, where nodes are users and an edge going out from a user A and to a user B means that \"User A retweeted User B\". Which one is false about a Twitter bot that retweeted other users frequently but got never retweeted by other users or by itself?\n\nOptions:\nA. It will have a non-zero hub value\nB. It will have an authority value of zero\nC. It will have a PageRank of zero\nD. Its authority value will be equal to the hub value of a user who never retweets other users\n", "answer": "Explanations: The correct answer to the multiple-choice question is: \"It will have a PageRank of zero.\"\n\nExplanation:\n1. PageRank Algorithm:\n- The PageRank algorithm assigns a numerical weight to each node in a network, such that nodes with higher weights are considered more important or influential.\n- In the context of the retweet network of Twitter, a Twitter bot that retweets frequently but never gets retweeted by others or itself will not have any incoming edges (no other users retweeting it), leading to a lack of influence or importance in the network.\n- As a result, the PageRank of this Twitter bot will be zero since it does not receive any \"votes\" (retweets) from other users.\n\n2. HITS Algorithm:\n- The HITS algorithm evaluates the authority and hub values of nodes in a network, where authority represents the quality of content provided by a node and hub represents the ability to connect to high-quality content.\n- If a user never retweets other users, their hub value will be zero because they do not act as a connector to other high-quality nodes.\n- However, the authority value of this user will not necessarily be equal to the hub value of a user who never retweets other users. This is because authority is about the quality of the content provided, which may not be directly related to the act of retweeting.\n\nTherefore, the correct statement is that the Twitter bot will have a PageRank of zero, as it does not receive any retweets from other users, making it insignificant in terms of influence within the retweet network of Twitter. ///Answer: C"}
{"subject": "", "question": "Question: Commitments should \\ldots?\n\nOptions:\nA. resist forgery.\nB. be binding and hiding.\nC. allow a third party to open commitments.\nD. resist preimage attacks.\n", "answer": "Explanations: The correct option is: \"be binding and hiding.\"\n\nReasoning: Commitments in cryptography should be both binding, meaning that one cannot change the value being committed to after making the commitment, and hiding, meaning that one cannot learn any information about the committed value before it is revealed. This ensures the integrity and confidentiality of the committed information. ///Answer: B"}
{"subject": "", "question": "Question: Let $\\mathcal C_1$ be a linear code over $\\mathbb F_3^n$, and let $\\mathcal C_2$ be a linear code over $\\mathbb F_2^n$. True or false: $\\mathcal C_1 \\cup \\mathcal C_2$ is necessarily a linear code over $\\mathbb F_3^n$.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: First, let's recall the definition of a linear code. A linear code is a subspace of its underlying vector space. In other words, it is closed under vector addition and scalar multiplication.\n\nNow, let's consider $\\mathcal C_1 \\cup \\mathcal C_2$. This is the union of two linear codes, which means it consists of all the codewords in $\\mathcal C_1$ and $\\mathcal C_2$.\n\nSince we are taking the union of two sets, there is no guarantee that the resulting set will still be closed under vector addition and scalar multiplication. In other words, the union of two linear codes may not necessarily be a linear code itself.\n\nTherefore, the correct answer is: False. $\\mathcal C_1 \\cup \\mathcal C_2$ is not necessarily a linear code over $\\mathbb F_3^n$. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{false} assertion. The ElGamal cryptosystem\\dots?\n\nOptions:\nA. is based on the Discrete Logarithm problem.\nB. produces randomized ciphertexts.\nC. produces ciphertexts as long as the plaintexts.\nD. encrypts messages with limited length.\n", "answer": "Explanations: 1. The ElGamal cryptosystem is based on the Discrete Logarithm problem: TRUE - ElGamal encryption is indeed based on the Discrete Logarithm problem for its security.\n   \n2. The ElGamal cryptosystem produces randomized ciphertexts: TRUE - ElGamal encryption generates different ciphertexts for the same plaintext due to the randomness introduced during encryption.\n\n3. The ElGamal cryptosystem produces ciphertexts as long as the plaintexts: FALSE - ElGamal encryption produces ciphertexts that are longer than the plaintexts due to the nature of the encryption process.\n\n4. The ElGamal cryptosystem encrypts messages with limited length: TRUE - ElGamal encryption has a limit on the length of the message that can be encrypted, determined by the parameters of the scheme. ///Answer: C"}
{"subject": "", "question": "Question: Select all sentences that are true.You will get a penalty for wrong ticks.\n\nOptions:\nA. PoS tagging aims to resolve morpho-syntactic ambiguities associated with words\nB. PoS tagging aims to suppress lexical variability of words\nC. Lemmatization reduces word forms into their surface forms\nD. PoS tagging becomes easy if lemmatization has already been performed on the words\nE. PoS tagging is an easy task, because it's unambiguous\n", "answer": "Explanations: 1. Part-of-Speech (PoS) tagging aims to label words in a text with their corresponding part of speech, such as noun, verb, adjective, etc. It helps in resolving morpho-syntactic ambiguities associated with words. This statement is true because one of the main purposes of PoS tagging is to disambiguate words based on their roles in a sentence.\n\n2. PoS tagging does not aim to suppress lexical variability of words. In fact, it aims to capture and represent the variability by assigning appropriate part-of-speech tags to words based on their usage in context. This statement is not true.\n\n3. Lemmatization is the process of reducing a word to its base or root form (lemma). It does not reduce word forms into their surface forms. This statement is incorrect as it misrepresents the purpose of lemmatization.\n\n4. While lemmatization can help PoS tagging by reducing inflected words to their base forms, it does not necessarily make PoS tagging easy. PoS tagging involves various complexities beyond just word forms, such as context and syntax. Therefore, this statement is not entirely true.\n\n5. PoS tagging is not always an easy task as it involves dealing with ambiguities, homonyms, and context-dependent classifications. It is not unambiguous, as words can have multiple parts of speech depending on their usage in a sentence. Hence, this statement is not true.\n\nBased on the analysis:\n- Option 1 is true as PoS tagging aims to resolve morpho-syntactic ambiguities.\n- Option 3 is false as lemmatization does not reduce word forms into their surface forms.\n- Option 4 is partially true but oversimplified, as lemmatization alone does not make PoS tagging easy.\n- Option 2 and Option 5 are false as they misrepresent the nature and challenges of PoS tagging.\n\nThe correct options are: \n- Option 1: PoS tagging aims to resolve morpho-syntactic ambiguities associated with words\n- Option 4: PoS tagging becomes easy if lemmatization has already been performed on the words\n\nTherefore, the correct options are Options 1 and 4. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{true} assertion. Let $X$ be a random variable defined by the visible face showing up when throwing a dice. Its expected value $E(X)$ is:?\n\nOptions:\nA. 3.5\nB. 3\nC. 1\nD. 4\n", "answer": "Explanations: First, let\\'s clarify what a random variable is. A random variable is a variable that can take on different values as a result of random outcomes. In this case, the random variable $X$ represents the visible face showing up when throwing a dice.\\n\\nThe expected value of a random variable, denoted by $E(X)$ or $\\\\mu$, represents the average value of the variable when the experiment is repeated a large number of times. It is calculated by summing the products of each possible outcome by its probability.\\n\\nFor a fair six-sided dice, each face has an equal probability of $\\\\frac{1}{6}$ of showing up. Therefore, the possible outcomes for $X$ are 1, 2, 3, 4, 5, and 6, each with a probability of $\\\\frac{1}{6}$.\\n\\nNow, let\\'s calculate the expected value $E(X)$:\\n\\n$E(X) = \\\\sum_{i=1}^{6} x_i \\\\cdot P(X = x_i)$\\n\\n$E(X) = 1 \\\\cdot \\\\frac{1}{6} + 2 \\\\cdot \\\\frac{1}{6} + 3 \\\\cdot \\\\frac{1}{6} + 4 \\\\cdot \\\\frac{1}{6} + 5 \\\\cdot \\\\frac{1}{6} + 6 \\\\cdot \\\\frac{1}{6}$\\n\\n$E(X) = \\\\frac{1}{6} + \\\\frac{2}{6} + \\\\frac{3}{6} + \\\\frac{4}{6} + \\\\frac{5}{6} + \\\\frac{6}{6}$\\n\\n$E(X) = \\\\frac{21}{6}$\\n\\n$E(X) = 3.5$\\n\\nTherefore, the \\\\textbf{true} assertion is 0: \"3.5\". The expected value of the random variable $X$ representing the visible face showing up when throwing a dice is indeed 3.5. ///Answer: A"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement. The brute force attack \\dots?\n\nOptions:\nA. can be applicable after decades according to Moore's law.\nB. can break a cipher with a $128$-bit key on your PC today.\nC. has higher worst case complexity than average case complexity.\nD. refers to a way of getting the secret key, exhaustively.\n", "answer": "Explanations: First, let's break down the statements provided:\n\n1. \"can be applicable after decades according to Moore's law.\"\n2. \"can break a cipher with a $128$-bit key on your PC today.\"\n3. \"has higher worst case complexity than average case complexity.\"\n4. \"refers to a way of getting the secret key, exhaustively.\"\n\nNow, let's analyze each statement to identify the incorrect one:\n\n1. The statement \"can be applicable after decades according to Moore's law\" is generally true. Moore's law states that the number of transistors on a microchip doubles approximately every two years, leading to an increase in computing power over time. This can potentially make brute force attacks more feasible in the future due to increased computational capabilities.\n\n2. The statement \"can break a cipher with a $128$-bit key on your PC today\" is incorrect. A brute force attack on a cipher with a $128$-bit key is currently infeasible due to the vast number of possible key combinations (2^128). It would take an immense amount of time and computational power to try all possible keys, making it practically impossible to break such a cipher with a brute force attack on a standard PC.\n\n3. The statement \"has higher worst case complexity than average case complexity\" is generally true. In the context of algorithms, the worst-case complexity refers to the maximum number of operations an algorithm performs for any input of a specific size, while the average case complexity refers to the expected number of operations for a random input. Brute force attacks often have a higher worst-case complexity because they involve trying all possible solutions until the correct one is found.\n\n4. The statement \"refers to a way of getting the secret key, exhaustively\" is true. A brute force attack is a method of cracking a password or encryption key by systematically trying all possible combinations until the correct one is found. It is an exhaustive approach to finding the secret key through trial and error.\n\nBased on the analysis above, the incorrect statement is: \"can break a cipher with a $128$-bit key on your PC today.\" ///Answer: B"}
{"subject": "", "question": "Question: The advantage of an algorithm $\\mathcal{A}$ having to distinguish a distribution $P$ which is either $P_0$ or $P_1$ is given by?\n\nOptions:\nA. $\\Pr[\\mathcal{A} \\rightarrow 0 | P = P_0] - \\Pr[\\mathcal{A} \\rightarrow 1 | P = P_1]$.\nB. $\\Pr[\\mathcal{A} \\rightarrow 0 | P = P_0]$.\nC. $8 \\times \\mathsf{Dec}^n(|P_1-P_0|)$.\nD. $\\Pr[\\mathcal{A} \\rightarrow 1 | P = P_1] - \\Pr[\\mathcal{A} \\rightarrow 1 | P = P_0]$.\n", "answer": "Explanations: The advantage of an algorithm $\\mathcal{A}$ having to distinguish a distribution $P$ which is either $P_0$ or $P_1$ is given by $\\Pr[\\mathcal{A} \\rightarrow 0 | P = P_0] - \\Pr[\\mathcal{A} \\rightarrow 1 | P = P_1]$. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following are parameters involved in the choice made by an order-1 HMM model for PoS tagging knowing that its output isthis/Pron is/V a/Det good/Adj question/Nand that neither \"is\" nor \"question\" can be adjectives, and that \"question\" can also not be a determiner.(Penalty for wrong ticks.)?\n\nOptions:\nA. P(N|question)\nB. P(question|N)\nC. P(question|Adj N)\nD. P(question|N Adj)\nE. P(this)\nF. P(this is)\nG. P(this V)\nH. P(Pron)\nI. P(Pron V)\nJ. P(Pron is)\nK. P(Det|Adj)\nL. P(Adj|Det)\nM. P(Adj|V Det)\nN. P(Adj|Det V)\nO. P(Det|V Adj)\nP. P(Det|Pron V)\nQ. P(Adj|a)\nR. P(question|Adj)\n", "answer": "Explanations: To determine which parameters are involved in the choice made by an order-1 HMM model for PoS tagging in the given scenario, we need to consider the constraints provided:\n\n1. The given output is \"this/Pron is/V a/Det good/Adj question/N\"\n2. \"is\" and \"question\" cannot be adjectives\n3. \"question\" cannot be a determiner\n\nNow let's go through each option and see if they align with the constraints:\n\na) P(N|question) - This parameter represents the probability of the tag N given the word \"question\". Since \"question\" cannot be an adjective or a determiner, this option is not relevant.\n\nb) P(question|N) - This parameter represents the probability of the word \"question\" given the tag N. Since \"question\" is tagged as N in the given output, this option is relevant.\n\nc) P(question|Adj N) - This parameter represents the probability of the word \"question\" given the tags Adj and N. Since \"question\" cannot be an adjective, this option is not relevant.\n\nd) P(question|N Adj) - This parameter represents the probability of the word \"question\" given the tags N and Adj. Since \"question\" cannot be an adjective, this option is not relevant.\n\ne) P(this) - This parameter represents the probability of the word \"this\". It is not directly related to the constraints provided.\n\nf) P(this is) - This parameter represents the probability of the words \"this is\". It is not directly related to the constraints provided.\n\ng) P(this V) - This parameter represents the probability of the word \"this\" given the tag V. It is not directly related to the constraints provided.\n\nh) P(Pron) - This parameter represents the probability of the tag Pron. It is not directly related to the constraints provided.\n\ni) P(Pron V) - This parameter represents the probability of the tags Pron and V. It is not directly related to the constraints provided.\n\nj) P(Pron is) - This parameter represents the probability of the words \"Pron is\". It is not directly related to the constraints provided.\n\nk) P(Det|Adj) - This parameter represents the probability of the tag Det given the tag Adj. It is not directly related to the constraints provided.\n\nl) P(Adj|Det) - This parameter represents the probability of the tag Adj given the tag Det. It is not directly related to the constraints provided.\n\nm) P(Adj|V Det) - This parameter represents the probability of the tag Adj given the tags V and Det. It is not directly related to the constraints provided.\n\nn) P(Adj|Det V) - This parameter represents the probability of the tag Adj given the tags Det and V. It is not directly related to the constraints provided.\n\no) P(Det|V Adj) - This parameter represents the probability of the tag Det given the tags V and Adj. It is not directly related to the constraints provided.\n\np) P(Det|Pron V) - This parameter represents the probability of the tag Det given the tags Pron and V. It is not directly related to the constraints provided.\n\nq) P(Adj|a) - This parameter represents the probability of the tag Adj given the word \"a\". It is not directly related to the constraints provided.\n\nr) P(question|Adj) - This parameter represents the probability ///Answer: B"}
{"subject": "", "question": "Question: Consider an RSA encryption where the $(p, q)$ are determined as $(53, 61)$. True or false: $(e,d) = (7, 223)$ are valid encoding/decoding exponents.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: To determine if $(e,d) = (7, 223)$ are valid encoding/decoding exponents, we need to check if the following conditions are satisfied:\n\n1. $e$ and $\\phi(n)$ are coprime, where $\\phi(n) = (p-1)(q-1)$.\n2. $ed \\equiv 1 \\pmod{\\phi(n)}$.\n\nGiven that $p = 53$ and $q = 61$, we can calculate $n = pq = 53 \\times 61 = 3233$ and $\\phi(n) = (53-1)(61-1) = 3120$.\n\n1. $e = 7$ and $\\phi(n) = 3120$ are coprime, as their greatest common divisor is 1.\n2. We need to check if $ed \\equiv 1 \\pmod{\\phi(n)}$. In this case, $7 \\times 223 \\equiv 1561 \\not\\equiv 1 \\pmod{3120}$.\n\nTherefore, the statement $(e,d) = (7,223)$ are valid encoding/decoding exponents is false. \n\nThe correct option is:\n- False ///Answer: A"}
{"subject": "", "question": "Question: Select all true statements.A penalty will be applied for any incorrect answers.\n\nOptions:\nA. The k-means algorithm always converges because at each step it minimizes the intra-class variance.\nB. The k-NN algorithm is a non-hierarchical, non-overlapping clustering method.\nC. The k-means algorithm always converges into a global minimum.\nD. In mapping methods used for visualization, the target space is considered a sub-space of the original space.\nE. In textual classification, the objects are always full length documents.\nF. Non-parametric methods for classification does not involve any parameter.\n", "answer": "Explanations: Answer:\n1) False, the k-means algorithm does not always converge to the global minimum as it is sensitive to initialization and can converge to local optima depending on the initial centroids.\n2) True, k-NN is a non-hierarchical, non-overlapping clustering method where each data point belongs to the cluster that has the majority of its k nearest neighbors.\n3) False, the k-means algorithm can converge to a local minimum, not necessarily the global minimum, due to its dependency on initialization.\n4) True, in mapping methods for visualization like t-SNE or PCA, the target space is typically considered a lower-dimensional subspace of the original high-dimensional space.\n5) False, in textual classification, objects can be individual words, phrases, or full-length documents, not restricted to just full-length documents.\n6) False, non-parametric methods for classification do involve parameters, but they are not fixed in number or strictly defined a priori like in parametric methods.\n\nConclusion: The true statements are 2, 4. ///Answer: B"}
{"subject": "", "question": "Question: Which one is \\emph{not} a block cipher?\n\nOptions:\nA. DES\nB. ECB\nC. SAFER\nD. IDEA\n", "answer": "Explanations: ECB\n\nExplanation: ECB (Electronic Codebook) is a mode of operation for block ciphers, not a block cipher itself. It is a method of using a block cipher to encrypt a message by dividing it into blocks and encrypting each block separately. DES (Data Encryption Standard), SAFER, and IDEA are all examples of block ciphers, which are algorithms that encrypt data in fixed-size blocks. ///Answer: B"}
{"subject": "", "question": "Question: Let $\\mathcal{C}$ be the $(6,3)$ linear code on $\\mathbb{F}_3$ whose parity-check matrix is  \begin{equation*}      H = \begin{pmatrix}   2 &0 &1 &1 &1 &0 \\     1 &2 &0 &0 &1 &1 \\     0 &0 &0 &1 &1 &1    \\end{pmatrix}. \\end{equation*} True or false: The minimum distance of $\\mathcal{C}$ is $d_{\\min} = 2$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: To determine the minimum distance of the $(6,3)$ linear code $\\mathcal{C}$ with the given parity-check matrix $H$, we need to use the fact that the minimum distance $d_{\\min}$ of a linear code is equal to the weight of the smallest nonzero codeword in the code.\n\nFirst, let's find the generator matrix $G$ of the code $\\mathcal{C}$. Since $H$ is the parity-check matrix, we know that $G$ is a $(6,3)$ generator matrix such that $GH^T = 0$, where $H^T$ is the transpose of the parity-check matrix.\n\nWe can find the generator matrix $G$ by first finding a basis for the null space of $H^T$. Since $H$ is a $(3,6)$ matrix, its null space will be a $3$-dimensional subspace of $\\mathbb{F}_3^6$.\n\nNext, we construct the parity-check matrix $H$ as follows:\n\n\\[\nH = \\begin{pmatrix}\n2 & 0 & 1 & 1 & 1 & 0 \\\\\n1 & 2 & 0 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 1\n\\end{pmatrix}\n\\]\n\nTaking the transpose of $H$, we get:\n\n\\[\nH^T = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 2 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\]\n\nThen, we need to find a basis for the null space of $H^T$. This can be done by row-reducing $H^T$ to row-echelon form and determining the basis for the null space.\n\nAfter performing row operations, we find that the reduced row-echelon form of $H^T$ is:\n\n\\[\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\\\\n\\end{pmatrix}\n\\]\n\nThis means that the null space of $H^T$ is spanned by the standard basis vectors, which correspond to the columns of the identity matrix. Therefore, the code $\\mathcal{C}$ has a generator matrix $G$ in systematic form with the identity matrix as its first $3 \\times 3$ submatrix.\n\nSince the code $\\mathcal{C}$ has a generator matrix in systematic form with the identity matrix as the first submatrix, the minimum distance of the code is equal to the weight of the first column of the identity matrix, which is $1$. Therefore, the correct answer is:\n\nFalse\n\nThe minimum distance of $\\mathcal{C}$ is $1$, not $2$. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements is \textbf{incorrect} ? Training a model with $L_1$-regularization ...\n\nOptions:\nA. can reduce the storage cost of the final model.\nB. is used to help escaping local minima during training.\nC. can reduce overfitting.\nD. can be named Lasso regression when in combination with an MSE loss function and a linear model.\n", "answer": "Explanations: Let's evaluate each statement:\n\n1. $L_1$-regularization can reduce the storage cost of the final model.\n- This statement is **correct**. $L_1$-regularization introduces sparsity by encouraging some model coefficients to be exactly zero. This sparsity can reduce the number of parameters that need to be stored, thereby reducing the storage cost of the final model.\n\n2. $L_1$-regularization is used to help escape local minima during training.\n- This statement is **incorrect**. $L_1$-regularization is primarily used for feature selection and to induce sparsity in the model parameters. It does not directly help escape local minima during training; techniques like early stopping or different optimization algorithms are more relevant for that purpose.\n\n3. $L_1$-regularization can reduce overfitting.\n- This statement is **correct**. $L_1$-regularization can reduce overfitting by preventing the model from becoming too complex and by promoting simpler models with fewer non-zero coefficients.\n\n4. $L_1$-regularization can be named Lasso regression when in combination with an MSE loss function and a linear model.\n- This statement is **correct**. When $L_1$-regularization is combined with a Mean Squared Error (MSE) loss function and applied to a linear model, it is commonly referred to as Lasso regression. Lasso stands for \"Least Absolute Shrinkage and Selection Operator.\"\n\nTherefore, the **incorrect** statement is:\n2. $L_1$-regularization is used to help escape local minima during training. ///Answer: B"}
{"subject": "", "question": "Question: How many generators do we have in a group of order $13$?\n\nOptions:\nA. 13.\nB. 12.\nC. 6.\nD. 2.\n", "answer": "Explanations: The correct answer is 12.\n\nIn a group of order $n$, the number of generators is given by $\\phi(n)$, where $\\phi$ is Euler's totient function. For a prime number $p$, $\\phi(p) = p-1$. Therefore, in a group of order 13, the number of generators is $\\phi(13) = 13-1 = 12$. ///Answer: B"}
{"subject": "", "question": "Question:  When applying Fagin\u2019s algorithm for a query with three different terms for finding the k top documents, the algorithm will scan?\n\nOptions:\nA. 2 different lists\nB. 3 different lists\nC. k different lists\nD. t depends how many rounds are taken\n", "answer": "Explanations: Fagin's algorithm involves scanning lists of documents to find the top k results. In this case, we have a query with three different terms. \nWhen applying Fagin's algorithm, the algorithm will scan each list corresponding to each term in the query. Therefore, for three different terms, the algorithm will scan 3 different lists.\nSo, the answer is (2) 3 different lists. ///Answer: B"}
{"subject": "", "question": "Question: We define a simplified Gaussian Mixture Model consisting of 2 equally likely Gaussians, i.e. $K=2$ and $\\pi_1 = \\pi_2 = 0.5$, and covariance matrices of the form $\\mathbf{\\Sigma}_i = \\sigma_i \\mathbf{I}_{D\times D}$ for $i\\in \\{1, 2\\}$ with $\\mathbf{I}_{D\times D}$ the identity matrix of size $D$. The dataset consists of only 2 points $\\xv_1$ and $\\xv_2$ that are distinct ($\\xv_1 \not= \\xv_2$). We initialize the model at some finite $\boldsymbol{\\mu}_1^{(0)}, \boldsymbol{\\mu}_2^{(0)}$ and $\\sigma_1^{(0)}, \\sigma_2^{(0)}$. We fit the model by the EM method on these parameters (keeping $\\pi_1$ and $\\pi_2$ fixed to 0.5). After $T\to \\infty$ steps, select the true statement among the following: ?\n\nOptions:\nA. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ converge to 0 for any initializations. \nB. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ diverge to $\\infty$ for any initializations.\nC. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ converge to 0 for some but not all the initializations.\nD. $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ diverge to $\\infty$ for some but not all the initializations.\n", "answer": "Explanations: The correct statement among the options provided is: '$\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ converge to 0 for some but not all the initializations.'\n\nThe reasoning behind this answer is that in a Gaussian Mixture Model with covariance matrices of the form $\\mathbf{\\Sigma}_i = \\sigma_i \\mathbf{I}_{D \\times D}$, the EM algorithm tends to converge to a local minimum. Since the dataset consists of only 2 distinct points $\\xv_1$ and $\\xv_2$, the model is very sensitive to the initializations of $\\mu_1, \\mu_2, \\sigma_1,$ and $\\sigma_2$. Depending on the initializations, it is possible for the algorithm to converge to a solution where $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ converge to 0. However, for some initializations, it is also possible for $\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ to diverge to infinity. Therefore, the statement that '$\\sigma_1^{(T)}$ and $\\sigma_2^{(T)}$ converge to 0 for some but not all the initializations' is the most appropriate choice. ///Answer: C"}
{"subject": "", "question": "Question: The little Fermat theorem states that for a prime $n$ and any $b\\in \\mathbb{Z}_n ^\\star$ we have\\dots?\n\nOptions:\nA. $b^{n-1}\\mod n = 1$.\nB. $b^{n-1}\\mod n = n$.\nC. $b^{n}\\mod n = 1$.\nD. $b^{n-1}\\mod n = b$.\n", "answer": "Explanations: Answer: \na) $b^{n-1}\\mod n = 1$.\n\nExplanation: The little Fermat theorem states that for a prime number $n$ and any integer $b$ that is coprime to $n$, the modular exponentiation of $b$ to the power of $n-1$ (i.e., $b^{n-1}\\mod n$) is congruent to 1 modulo $n$. This theorem is a fundamental result in number theory and modular arithmetic. \n\nOption b) $b^{n-1}\\mod n = n$ is incorrect because the result should be congruent to 1, not equal to $n$.\n\nOption c) $b^{n}\\mod n = 1$ is incorrect because the little Fermat theorem specifically deals with $b^{n-1}$, not $b^n$.\n\nOption d) $b^{n-1}\\mod n = b$ is incorrect because the correct result should be 1, not $b$. ///Answer: A"}
{"subject": "", "question": "Question: What is the complexity of generating an RSA modulus of length $2\\ell$?\n\nOptions:\nA. $O(\\ell)$\nB. $O(\\ell^2)$\nC. $O(\\ell^4)$\nD. $O(\\ell^8)$\n", "answer": "Explanations: The complexity of generating an RSA modulus of length $2\\ell$ is $O(\\ell^2)$. This is because the RSA modulus is typically generated by multiplying two large prime numbers of length $\\ell$, which involves the generation of two $\\ell$-bit prime numbers, their multiplication, and some additional computations. The overall complexity is dominated by the generation of these prime numbers, which is approximately $O(\\ell^2)$ due to the probabilistic nature of prime testing algorithms like Miller-Rabin. ///Answer: B"}
{"subject": "", "question": "Question: (Minima) Convex functions over a convex set have a unique global minimum.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: A: True\n\nTo answer this question, we first need to understand what a convex function and a convex set are. A function is convex if the line segment between any two points on the graph of the function lies above or on the graph itself. A set is convex if the line segment between any two points in the set lies entirely within the set.\n\nNow, if a function is convex over a convex set, it means that the function is always above its tangent line at any point within the set. This property ensures that the function has a unique global minimum, as any other local minimum would contradict the convexity of the function.\n\nTherefore, the statement that convex functions over a convex set have a unique global minimum is true. ///Answer: A"}
{"subject": "", "question": "Question: Suppose that q is density reachable from p. The chain of points that ensure this relationship are {t,u,g,r}. Which of the following is always true?\n\nOptions:\nA. p is density reachable from q\nB. q and p are density-connected\nC. p is a border point\nD. q is a core point\n", "answer": "Explanations: {\n    \"explaination\": \"If q is density reachable from p, it does not imply that p is density reachable from q. Density-reachable is not a symmetric relationship. Density-connected means that both p and q are density reachable from each other. Border points are points that are not density reachable from any other point. Core points are points that have at least MinPts within Eps distance, and they are not border points.\",\n    \"answer\": \"q and p are density-connected\"\n} ///Answer: B"}
{"subject": "", "question": "Question: A 128-bit key ...\n\nOptions:\nA. has 128 decimal digits.\nB. is too long for any practical application.\nC. provides reasonable security for at least four decades.\nD. adresses $n^2$ problem for $n=2^{64}$.\n", "answer": "Explanations: When responding to a user mentioning a 128-bit key, provide the following information:\n1. A 128-bit key has 128 binary digits, not 128 decimal digits.\n2. A 128-bit key is considered secure because it provides a large number of possible combinations, making it difficult for attackers to brute force their way into encrypted data.\n3. The $n^2$ problem refers to the difficulty of breaking a cryptographic algorithm as the key size increases exponentially. For a 128-bit key, it addresses the $n^2$ problem for $n=2^{64}$, meaning that the number of possible combinations is so vast that it would take an infeasible amount of time and computational power to crack the encryption through brute force. ///Answer: C"}
{"subject": "", "question": "Question: Data being classified as unstructured or structured depends on the:?\n\nOptions:\nA. Degree of abstraction\nB. Level of human involvement\nC. Type of physical storage\nD. Amount of data\n", "answer": "Explanations: To determine whether data is classified as unstructured or structured, we need to consider the characteristics of the data itself. Let's break down each option and see which one is most relevant:\n\nA. Degree of abstraction:\n- The degree of abstraction refers to how the data is represented and how much detail is included.\n- Structured data is organized in a predefined format with a clear schema, while unstructured data lacks a specific schema and is more free-form.\n- Therefore, the degree of abstraction can influence whether data is classified as structured or unstructured. For example, structured data tends to have a lower degree of abstraction compared to unstructured data.\n\nB. Level of human involvement:\n- The level of human involvement refers to the extent to which human interpretation or manipulation is required to make sense of the data.\n- Structured data is typically organized in a way that is easily interpretable by machines without much human intervention, while unstructured data often requires human interpretation due to its lack of predefined structure.\n- Hence, the level of human involvement can impact whether data is categorized as structured or unstructured.\n\nC. Type of physical storage:\n- The type of physical storage refers to how the data is stored, such as in databases, files, or other storage systems.\n- While the type of physical storage can affect data accessibility and management, it may not be the primary factor in determining whether data is structured or unstructured.\n\nD. Amount of data:\n- The amount of data refers to the volume of data being considered.\n- Both structured and unstructured data can exist in large or small quantities, so the amount of data alone may not be the deciding factor in classifying data as structured or unstructured.\n\nIn conclusion, the most relevant factor in determining whether data is classified as unstructured or structured among the given options is the degree of abstraction (Option A). The level of human involvement (Option B) is also important, but the degree of abstraction plays a more significant role in distinguishing between structured and unstructured data. ///Answer: A"}
{"subject": "", "question": "Question: An element of the finite field $\\mathrm{GF}(2^8)$ is usually represented by\\dots?\n\nOptions:\nA. \\dots one hexadecimal digit.\nB. \\dots eight bytes.\nC. \\dots two hexadecimal digits.\nD. \\dots an irreducible polynomial of degree 8.\n", "answer": "Explanations: An element of the finite field $\\mathrm{GF}(2^8)$ is usually represented by **two hexadecimal digits**.\n\nCorrect option: Option 3: two hexadecimal digits. ///Answer: C"}
{"subject": "", "question": "Question: (Generalized Linear Models) Deep neural networks with logistic loss for binary classification are generalized linear models.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: In the context provided, it is essential to break down each part of the statement:\n\n1. Generalized linear models (GLMs) predict an outcome based on a linear combination of input features and a link function: This is a fundamental characteristic of GLMs, where the relationship between the input features and the output is modeled as a linear combination passed through a link function to accommodate non-normal error distributions.\n\n2. Logistic regression, a type of GLM, uses a logistic function as its link function: Logistic regression is a specific type of GLM that models binary outcomes using the logistic (sigmoid) function as its link function to map the linear combination of input features to a probability between 0 and 1.\n\n3. Deep neural networks involve multiple layers and non-linear activations, distinguishing them significantly from GLMs: Deep neural networks consist of multiple layers of interconnected nodes and employ non-linear activation functions to model complex relationships in the data, enabling them to learn hierarchical representations, which is a departure from the linear modeling approach of GLMs.\n\nBased on these explanations, the statement \"Deep neural networks with logistic loss for binary classification are generalized linear models\" is False. While deep neural networks with logistic loss for binary classification share the goal of predicting outcomes, their architecture, use of multiple layers, and non-linear activations make them distinctly different from traditional generalized linear models like logistic regression. ///Answer: B"}
{"subject": "", "question": "Question: According to the Kerckhoffs Principle:?\n\nOptions:\nA. The internal design of a cryptosystem should be public.\nB. The internal design of a cryptosystem should \\emph{not} be public.\nC. If there is a single security hole in a cryptosystem, somebody will discover it.\nD. The security of the cryptosystem should \\emph{not} rely on the secrecy of the cryptosystem itself.\n", "answer": "Explanations: The correct answer is D: The security of the cryptosystem should not rely on the secrecy of the cryptosystem itself.\n\nThe Kerckhoffs Principle, also known as Kerckhoffs's Law, is a fundamental concept in the field of cryptography that states that the security of a cryptosystem should not depend on the secrecy of the algorithm or system itself, but rather on the secrecy of the key. In other words, a cryptosystem should remain secure even if everything about the system, except for the key, is public knowledge.\n\nThis principle emphasizes the importance of using strong, well-established encryption algorithms and protocols that have been thoroughly analyzed and tested by the cryptographic community. By relying on the secrecy of the key rather than the secrecy of the system, the security of the system can be more easily maintained and assessed.\n\nOverall, the Kerckhoffs Principle promotes transparency and openness in the design and implementation of cryptographic systems, ensuring that security is based on sound mathematical principles rather than on obscurity. ///Answer: D"}
{"subject": "", "question": "Question: The KNN algorithm needs a notion of distance to assess which points are ``nearest''.\n\t                Identify the distance measures that can be used in the KNN algorithm. \n                    (a) Euclidean Distance : distance associated to the $L_2$ norm $\\|xv\\|_2 := \\sqrt{x_1^2+\\dots+x_D^2}$\n\t\t            (b) Manhattan Distance : distance associated to the $L_1$ norm $\\|xv\\|_1 := |x_1|+\\dots+|x_D|$\n\t\t            (c) Distance associated to the $L_4$ norm $\\|xv\\|_4 := \big(|x_1|^4+\\dots+|x_D|^4\big)^{1/4}$\n\t                ?\n\nOptions:\nA. only a\nB. only b\nC. only c\nD. only a and b\nE. only a and c\nF. only b and c\nG. a, b and c\n", "answer": "Explanations: To identify the distance measures that can be used in the KNN algorithm, we need to consider the different options provided:\n\n(a) Euclidean Distance: This is the most common distance measure used in the KNN algorithm. It calculates the straight-line distance between two points in a D-dimensional space. It is defined using the $L_2$ norm formula $\\|xv\\|_2 := \\sqrt{x_1^2+\\dots+x_D^2}$. This measure is effective in capturing the overall similarity between points in a multi-dimensional space.\n\n(b) Manhattan Distance: This distance measure calculates the distance between two points by summing the absolute differences between their coordinates. It is defined using the $L_1$ norm formula $\\|xv\\|_1 := |x_1|+\\dots+|x_D|$. Manhattan Distance is useful when the dimensions have different scales and when the data is sparse.\n\n(c) Distance associated to the $L_4$ norm: This distance measure is defined using the $L_4$ norm formula $\\|xv\\|_4 := \big(|x_1|^4+\\dots+|x_D|^4\big)^{1/4}$. This measure can be used when we want to emphasize larger differences in the data compared to the Euclidean distance.\n\nConsidering the options provided, the correct answer is: a, b and c. This means that all three distance measures (Euclidean, Manhattan, and $L_4$ norm) can be used in the KNN algorithm to assess the nearest points. Each distance measure has its own strengths and weaknesses, and the choice of distance measure depends on the specific characteristics of the data and the problem at hand. ///Answer: G"}
{"subject": "", "question": "Question: Church booleans are a representation of booleans in the lambda calculus. The Church encoding of true and false are functions of two parameters: Church encoding of tru: t => f => t Church encoding of fls: t => f => f What does the following function implement? b => c => b (not c) c?\n\nOptions:\nA. not c\nB. b xor c\nC. b or c\nD. b and c\nE. not(b and c)\n", "answer": "Explanations: Answer: The correct option is 'b and c'.\n\nExplanation:\n1. The given function is \\( b \\Rightarrow c \\Rightarrow b(\\neg c) c \\).\n2. This function takes two parameters, \\( b \\) and \\( c \\), and returns \\( b \\) if \\( \\neg c \\) is true; otherwise, it returns \\( c \\).\n3. Breaking down the function:\n   - If \\( \\neg c \\) is true, then the function will return \\( b \\).\n   - If \\( \\neg c \\) is false, then the function will return \\( c \\).\n4. This is essentially the implementation of the logical AND operation (conjunction) in boolean logic.\n5. Therefore, the function implements \\( b \\) AND \\( c \\).\n6. Thus, the correct answer is 'b and c'. ///Answer: D"}
{"subject": "", "question": "Question: For binary classification, which of the following methods can achieve perfect training accuracy on \\textbf{all} linearly separable datasets?\n\nOptions:\nA. Decision tree\nB. Hard-margin SVM\nC. 15-nearest neighbors\nD. None of the suggested\n", "answer": "Explanations: To determine which method can achieve perfect training accuracy on all linearly separable datasets, we need to analyze each option:\n\n1. Decision tree: Decision trees can achieve perfect training accuracy on linearly separable datasets if the decision boundaries can perfectly separate the classes. However, decision trees may not always guarantee perfect separation, especially if the data is noisy or has overlapping classes. Therefore, decision trees may not achieve perfect training accuracy on all linearly separable datasets.\n\n2. Hard-margin SVM: Hard-margin SVM aims to find the hyperplane that separates the two classes with the maximum margin. If the dataset is linearly separable, the hard-margin SVM can achieve perfect training accuracy by finding the optimal separating hyperplane. Therefore, hard-margin SVM can achieve perfect training accuracy on all linearly separable datasets.\n\n3. 15-nearest neighbors: The 15-nearest neighbors algorithm classifies data points based on the majority class of its 15 nearest neighbors. While the k-nearest neighbors algorithm can be effective for many classification tasks, it may not always achieve perfect training accuracy on linearly separable datasets, especially if the number of neighbors is not optimal for the dataset.\n\n4. None of the suggested: If none of the suggested methods are capable of achieving perfect training accuracy on all linearly separable datasets, then this would be the correct choice. However, based on the analysis above, the hard-margin SVM is the method that can achieve perfect training accuracy on all linearly separable datasets.\n\nTherefore, the correct answer is 2. Hard-margin SVM, as it is the method that can achieve perfect training accuracy on all linearly separable datasets. ///Answer: B"}
{"subject": "", "question": "Question: Consider the following loaded dice with $6$ faces: $P(S_1=6)=5/6$ and $P(S_1 = x)=1/30$ for $x\\in\\{1,2,3,4,5\\}$. Suppose we throw it indefinitely. Hence, we have a source $S=S_1 S_2 S_3\\ldots$. Then, the source is stationary.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: The correct answer is 0. True. \n\nThe source is considered stationary in this scenario. The probabilities assigned for each outcome ($P(S_n=6) = 5/6$ and $P(S_n = x) = 1/30$ for $x \\in \\{1, 2, 3, 4, 5\\}$) stay constant. This consistency indicates that the source maintains its statistical properties with each new roll despite the indefinite throwing. ///Answer: A"}
{"subject": "", "question": "Question: For the number of times the apriori algorithm and the FPgrowth algorithm for association rule mining are scanning the transaction database the following is true?\n\nOptions:\nA. fpgrowth has always strictly fewer scans than apriori\nB. fpgrowth and apriori can have the same number of scans\nC. apriori cannot have fewer scans than fpgrowth\nD. all three above statements are false\n", "answer": "Explanations: ANSWER:\nThe correct answer is 'fpgrowth has always strictly fewer scans than apriori'. The reason for this is that the Apriori algorithm generates candidate itemsets and scans the database multiple times to check support, which can be computationally expensive. On the other hand, the FPgrowth algorithm constructs a tree structure (FP-tree) from the transaction database and only needs to scan the database twice - once to build the FP-tree and once to mine the frequent itemsets. This makes FPgrowth more efficient in terms of the number of scans needed compared to Apriori. To solve similar questions on his own, the student should understand the underlying concepts of both algorithms, specifically how they scan the transaction database, and compare their efficiencies based on these principles. By grasping the core ideas behind each algorithm, the student can reason through questions related to their performance and make educated choices. ///Answer: A"}
{"subject": "", "question": "Question: How do you compute the output Y of a linear regression using Python and the scientific package Numpy? Recall that W is the weight matrix with dimensionality (number of features x 1), b is the bias with dimensionality (1 x 1) and X is the input matrix with dimensionality (number of samples x number of features). Notation of matrix dimensions: (Rows $\\times$ Columns) (One answer)?\n\nOptions:\nA. \\texttt{import numpy as np} \\\\\n\t\t\\texttt{Y = X.T.dot(W) + b}\nB. \\texttt{import numpy as np} \\\\\n\t\t\\texttt{Y = X.dot(W) + b}\nC. \\texttt{import numpy as np} \\\\\n\t\t\\texttt{Y = W.dot(X) + b}\nD. \\texttt{import numpy as np} \\\\\n\t\t\\texttt{Y = (W +b).dot(X)}\n", "answer": "Explanations: To compute the output Y of a linear regression using Python and Numpy, where W is the weight matrix, b is the bias, and X is the input matrix, the correct code snippet is:\n\n```python\nimport numpy as np\nY = X.dot(W) + b\n```\n\nHere's the explanation behind this choice:\n\n1. **X.dot(W)**: In linear regression, the output Y is calculated by multiplying the input matrix X with the weight matrix W. This operation is achieved using the dot product. The dot product of X and W results in a new matrix where each row corresponds to a sample in X and each column corresponds to a weight in W. This step captures the linear relationship between the input features and the output.\n\n2. **+ b**: After obtaining the dot product of X and W, the bias term b is added element-wise to each sample's result. The bias term allows the linear model to fit the data better by accounting for the offset from the origin.\n\n3. **import numpy as np**: This line imports the Numpy library and assigns it the alias np, which provides efficient array operations and linear algebra functions necessary for matrix computations.\n\nTherefore, the correct way to compute the output Y of a linear regression model in Python using Numpy is by multiplying the input matrix X with the weight matrix W and then adding the bias term b to the result. ///Answer: B"}
{"subject": "", "question": "Question: (Robustness) The $l_{1}$ loss is less sensitive to outliers than $l_{2}$.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: Option 1: True\n\nRationale:\nThe $l_{1}$ loss function, also known as the mean absolute error, is less sensitive to outliers compared to the $l_{2}$ loss function, which is the mean squared error. This is because the $l_{1}$ loss penalizes outliers linearly, while the $l_{2}$ loss penalizes outliers quadratically. As a result, the $l_{1}$ loss function is more robust to outliers in the data. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion. In ElGamal $\\ldots$?\n\nOptions:\nA. the encryption algorithm is deterministic.\nB. the key recovery problem is equivalent to the Computational Diffie Hellman problem.\nC. the size of the ciphertext is always bigger than the size of the corresponding plaintext.\nD. the decryption problem can be hard even if the discrete logarithm is easy to compute in the underlying group.\n", "answer": "Explanations: To tick the correct assertion in ElGamal, you need to choose the option that states that the decryption problem can be hard even if the discrete logarithm is easy to compute in the underlying group. This is because ElGamal encryption is based on the difficulty of the discrete logarithm problem, which means that even if it is easy to compute in the group, the decryption problem can still be hard. ///Answer: D"}
{"subject": "", "question": "Question: Select \\emph{incorrect} statement. The brute force technique against a cipher with key $256$ bits is?\n\nOptions:\nA. impossible even if we can compute without burning an energy.\nB. impossible since the number of possible keys is too high $2^{256} \\approx 10^{77}$.\nC. impossible in future even if we consider Moore's law.\nD. feasible using all clusters at EPFL.\n", "answer": "Explanations: C. impossible in future even if we consider Moore's law.\n\nThis statement is incorrect because Moore's law refers to the observation that the number of transistors on a microchip doubles approximately every two years, leading to an increase in computing power. As computing power increases, the feasibility of brute force attacks against ciphers with large key sizes, such as 256 bits, also increases. Therefore, it is not accurate to say that it will be impossible in the future, as advancements in technology may eventually make it feasible to carry out such attacks. ///Answer: C"}
{"subject": "", "question": "Question: Which of the following is correct regarding Crowdsourcing?\n\nOptions:\nA. Random Spammers give always the same answer for every question\nB. It is applicable only for binary classification problems\nC. Honey Pot discovers all the types of spammers but not the sloppy workers\nD. The output of Majority Decision can be equal to the one of Expectation-Maximization\n", "answer": "Explanations: I apologize for the confusion. In the context of AI and machine learning, the correct answer is: (d) The output of Majority Decision can be equal to the one of Expectation-Maximization.\n\nI chose this answer because in the context of AI and machine learning, both Majority Decision and Expectation-Maximization are algorithms used to make decisions based on input data. The output of Majority Decision, which relies on the most common vote or opinion, can sometimes be equivalent to the output of Expectation-Maximization, which is a statistical method used for clustering and classification. This highlights the potential overlap in results between different algorithms and the importance of understanding their underlying principles and assumptions. ///Answer: D"}
{"subject": "", "question": "Question: When constructing a word embedding, what is true regarding negative samples?\n\nOptions:\nA. They are words that do not appear as context words\nB. They are selected among words which are not stop words\nC. Their frequency is decreased down to its logarithm\nD. They are oversampled if less frequent\n", "answer": "Explanations: When thinking about negative sampling in word embeddings, we need to consider the purpose of negative samples. Negative samples are used to train the model to distinguish between true context words and randomly selected words that are not context words. The goal is for the model to learn to predict that true context words are likely to appear together while negative samples are not.\n\nNow, let's break down each statement and see if it aligns with the concept of negative samples in word embeddings:\n\n1. They are words that do not appear as context words: This statement is true. Negative samples are words that are not the actual context words. By training the model to differentiate between true context words and negative samples, it can better learn the relationships between words.\n\n2. They are selected among words which are not stop words: This statement is not necessarily true. Negative samples can be selected from any words in the vocabulary, not just limited to non-stop words. The key is that they are not the actual context words.\n\n3. Their frequency is decreased down to its logarithm: This statement is not accurate. Negative sampling involves randomly selecting negative samples based on a distribution that may involve word frequency, but there is no specific requirement to decrease the frequency to its logarithm.\n\n4. They are oversampled if less frequent: This statement is not true. Negative samples are typically sampled randomly from the vocabulary, not specifically oversampled based on their frequency.\n\nTherefore, the correct statement regarding negative samples in word embeddings is:\n- They are words that do not appear as context words ///Answer: A"}
{"subject": "", "question": "Question: When representing the adjacency list of a Web page in a connectivity server by using a reference list from another Web page, the reference list is searched only in a neighbouring window of the Web page's URL, because:?\n\nOptions:\nA. subsequent URLs in an adjacency list have typically small differences typically many URLs in a web page are similar to each other\nB. often many URLs among two pages with similar URL are similar\nC. most extra nodes are found in the neighbouring window.\n", "answer": "Explanations: Let's think step-by-step:\n\n1. The adjacency list of a Web page represents the links between different pages.\n2. When representing the adjacency list in a connectivity server using a reference list from another Web page, the reference list is searched in a neighbouring window of the Web page's URL.\n3. A neighbouring window refers to a range of URLs that are close or adjacent to the current URL being analyzed.\n4. This search is limited to the neighbouring window because subsequent URLs in an adjacency list typically have small differences.\n5. Additionally, many URLs on a web page are similar to each other, and often many URLs among two pages with similar URLs are similar.\n6. By focusing the search on the neighbouring window, most extra nodes are found efficiently without having to search through a large number of unrelated URLs. ///Answer: A"}
{"subject": "", "question": "Question: A posting indicates?\n\nOptions:\nA. The frequency of a term in the vocabulary\nB. The frequency of a term in a document\nC. The occurrence of a term in a document\nD. The list of terms occurring in a document\n", "answer": "Explanations: To determine what a posting indicates, we need to understand the concept of a posting in the context of information retrieval or natural language processing.\n\nIn information retrieval, a posting refers to the occurrence of a term in a document along with additional information such as the frequency of that term in the document. Therefore, the most appropriate answer would be:\n\n(c) The occurrence of a term in a document\n\nThis is because a posting typically includes information about where and how many times a term appears in a document, making it a more specific and accurate description compared to the other options. ///Answer: C"}
{"subject": "", "question": "Question: In which of the following cases does JOS acquire the big kernel lock?\n\nOptions:\nA. Processor traps in user mode\nB. Processor traps in kernel mode\nC. Switching from kernel mode to user mode\nD. Initialization of application processor\n", "answer": "Explanations: In the given cases, the acquisition of the big kernel lock by JOS would occur in the following scenarios:\n\n1. Processor traps in kernel mode: When a processor traps in kernel mode, it indicates an exception or interrupt occurred while the processor was already in kernel mode. When this happens, the system needs to ensure that critical sections of the kernel code are protected from concurrent access by multiple processors. Acquiring the big kernel lock in this case ensures that only one processor can execute kernel code at a time, preventing race conditions and maintaining the consistency of the system.\n\n2. Switching from kernel mode to user mode: When a processor switches from kernel mode to user mode, it signifies a context switch from executing kernel code to user code. Before transitioning to user mode, the kernel may need to release resources or perform other critical operations that require exclusive access. Acquiring the big kernel lock during this transition helps prevent conflicts with other processors that may be attempting similar operations, ensuring the integrity of the system state.\n\nTherefore, in the given options, JOS would acquire the big kernel lock in the cases of processor traps in kernel mode and switching from kernel mode to user mode to ensure proper synchronization and mutual exclusion in multi-processor systems. ///Answer: B"}
{"subject": "", "question": "Question: Determine which of the following  compound propositions are satisfiable (more than one answer can be correct):?\n\nOptions:\nA. (p\u2228\u00acq)\u2227(\u00acp\u2228q)\u2227(\u00acp\u2228\u00acq)\nB. (p\u2194q)\u2227(\u00acp\u2194q)\nC. (p \u2192 q)\u2227(p \u2192 \u00acq)\u2227(\u00acp \u2192 q)\nD. None of the other options\n", "answer": "Explanations: To determine which of the compound propositions are satisfiable, we need to check if there exists an assignment of truth values to the propositional variables p and q that make the entire compound proposition true.\n\nLet's evaluate each option:\n\n(1) (p\u2228\u00acq)\u2227(\u00acp\u2228q)\u2227(\u00acp\u2228\u00acq):\nTo satisfy this compound proposition, we need to find values for p and q such that all three sub-propositions are true simultaneously. Let's consider the truth table:\n\n| p | q | (p\u2228\u00acq) | (\u00acp\u2228q) | (\u00acp\u2228\u00acq) | Compound Proposition |\n|---|---|--------|--------|---------|-----------------------|\n| T | T |   T    |   T    |    F    |          F            |\n| T | F |   T    |   F    |    T    |          F            |\n| F | T |   F    |   T    |    T    |          F            |\n| F | F |   T    |   T    |    T    |          T            |\n\nFrom the truth table, we can see that the compound proposition is satisfiable when p is false and q is false. Therefore, option (1) is satisfiable.\n\n(2) (p\u2194q)\u2227(\u00acp\u2194q):\nLet's evaluate this compound proposition using the truth table:\n\n| p | q | (p\u2194q) | (\u00acp\u2194q) | Compound Proposition |\n|---|---|--------|---------|----------------------|\n| T | T |   T    |    F    |           F          |\n| T | F |   F    |    T    |           F          |\n| F | T |   F    |    T    |           F          |\n| F | F |   T    |    F    |           F          |\n\nWe can see from the truth table that there is no assignment of truth values to p and q that makes the entire compound proposition true. Therefore, option (2) is not satisfiable.\n\n(3) (p \u2192 q)\u2227(p \u2192 \u00acq)\u2227(\u00acp \u2192 q):\nLet's evaluate this compound proposition using the truth table:\n\n| p | q | (p \u2192 q) | (p \u2192 \u00acq) | (\u00acp \u2192 q) | Compound Proposition |\n|---|---|---------|---------|----------|----------------------|\n| T | T |    T    |    F    |    T     |          F           |\n| T | F |    F    |    T    |    F     |          F           |\n| F | T |    T    |    T    |    T     |          T           |\n| F | F |    T    |    T    |    T     |          T           |\n\nFrom the truth table, we see that the compound proposition is satisfiable when p is false and q is true. Therefore, option (3) is satisfiable.\n\nIn conclusion, options (1) and (3) are satisfiable, while options (2) is not satisfiable. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. $C(P_0,P_1)\\ldots$?\n\nOptions:\nA. $\\ldots\\displaystyle\\sim-\\log{\\sum_{x}\\sqrt{P_0(x)P_1(x)}}$\nB. $\\ldots=\\displaystyle\\sum_{x}\\left|P_1(x)-P_0(x)\\right|$\nC. $\\ldots\\displaystyle\\sim\\frac{\\mbox{SEI}(P_0,P_1)}{8\\mbox{ln}2}$\nD. is the Chernoff information between $P_0$ ans $P_1$.\n", "answer": "Explanations: The false assertion among the options provided is:\n\n$\\ldots\\displaystyle\\sim-\\log{\\sum_{x}\\sqrt{P_0(x)P_1(x)}}$\n\nThe Chernoff information between two probability distributions $P_0$ and $P_1$ is defined as:\n\n$C(P_0, P_1) = \\sup_{0 \\leq \\lambda \\leq 1} \\left\\{-\\log \\sum_x P_0(x)^\\lambda P_1(x)^{1-\\lambda}\\right\\}$\n\nTherefore, the correct expression for the Chernoff information is the one given by $\\ldots\\displaystyle\\sim-\\log{\\sum_{x} P_0(x)^\\lambda P_1(x)^{1-\\lambda}}$, not $\\ldots\\displaystyle\\sim-\\log{\\sum_{x}\\sqrt{P_0(x)P_1(x)}}$. ///Answer: A"}
{"subject": "", "question": "Question: Let $f: \\mathbb{Z}_{m n} \\rightarrow \\mathbb{Z}_m \\times \\mathbb{Z}_n$ be defined by $f (x) = (x \\bmod m,x \\bmod n)$. Then $f$ is a ring isomorphism between $\\mathbb{Z}_{180}$ and:?\n\nOptions:\nA. $\\mathbb{Z}_{2} \\times \\mathbb{Z}_{90}$.\nB. $\\mathbb{Z}_{4} \\times \\mathbb{Z}_{45}$.\nC. $\\mathbb{Z}_{10} \\times \\mathbb{Z}_{18}$.\nD. $\\mathbb{Z}_{6} \\times \\mathbb{Z}_{30}$.\n", "answer": "Explanations: To find the ring that $\\mathbb{Z}_{180}$ is isomorphic to under $f$, we need to consider the Chinese Remainder Theorem.\n\nGiven that $f(x) = (x \\bmod m, x \\bmod n)$ for $x \\in \\mathbb{Z}_{mn}$, we can see that $f$ is a ring homomorphism.\n\nNow, let's calculate the possible choices for the target ring:\n\n1. $\\mathbb{Z}_{2} \\times \\mathbb{Z}_{90}$:\nFor this choice, we have $m = 2$ and $n = 90$. So, $f(x) = (x \\bmod 2, x \\bmod 90)$. However, since $90$ is not coprime with $2$, this choice is not valid.\n\n2. $\\mathbb{Z}_{4} \\times \\mathbb{Z}_{45}$:\nFor this choice, we have $m = 4$ and $n = 45$. So, $f(x) = (x \\bmod 4, x \\bmod 45)$. However, since $45$ is not coprime with $4$, this choice is not valid.\n\n3. $\\mathbb{Z}_{10} \\times \\mathbb{Z}_{18}$:\nFor this choice, we have $m = 10$ and $n = 18$. So, $f(x) = (x \\bmod 10, x \\bmod 18)$. Since $10$ and $18$ are coprime, this choice is valid.\n\n4. $\\mathbb{Z}_{6} \\times \\mathbb{Z}_{30}$:\nFor this choice, we have $m = 6$ and $n = 30$. So, $f(x) = (x \\bmod 6, x \\bmod 30)$. However, since $30$ is not coprime with $6$, this choice is not valid.\n\nTherefore, the correct answer is $\\mathbb{Z}_{10} \\times \\mathbb{Z}_{18}$. ///Answer: C"}
{"subject": "", "question": "Question: 10 itemsets out of 100 contain item A, of which 5 also contain B. The rule B -> A has:?\n\nOptions:\nA. unknown support and 50% confidence\nB. unknown support and unknown confidence\nC. 5% support and 50% confidence\nD. 5% support and unknown confidence\n", "answer": "Explanations: To find the support and confidence values for the rule B -> A, we can use the given information:\n\n- Support: This is the percentage of transactions that contain both items A and B. In this case, 5 out of 100 itemsets contain both A and B, so the support is 5%.\n\n- Confidence: This is the percentage of itemsets containing B that also contain A. Since 5 out of the 10 itemsets containing A also contain B, the confidence is 50%.\n\nNow, let's match these values with the given options:\n\n- 'unknown support and 50% confidence': This option is incorrect as we have calculated the support to be 5%.\n\n- 'unknown support and unknown confidence': This option is incorrect as we have calculated both support (5%) and confidence (50%).\n\n- '5% support and 50% confidence': This option matches our calculations and is correct.\n\n- '5% support and unknown confidence': This option is incorrect as we have calculated the confidence to be 50%.\n\nTherefore, the correct answer is '5% support and 50% confidence'. ///Answer: C"}
{"subject": "", "question": "Question: (Weight initialization) The choice of weight initialization will not impact the optimization behavior of the neural network.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: This statement is not accurate. The choice of weight initialization can have a significant impact on the optimization behavior of a neural network. Proper weight initialization can help the network converge faster and achieve better performance, while poor initialization can lead to issues like vanishing or exploding gradients, which can hinder the optimization process. Common weight initialization techniques include using small random values drawn from a Gaussian or uniform distribution, or using techniques like Xavier or He initialization. Choosing the right weight initialization method is an important aspect of training a neural network effectively. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following is equivalent to \\((10001)_2\\) ? (Multiple answers can be correct)?\n\nOptions:\nA. \\(  (17)_{10}  \\)\nB. \\(  (F0)_{16}  \\)\nC. \\(  (23)_{8}  \\)\nD. \\(  (101)_{4}  \\)\n", "answer": "Explanations: To convert the binary number \\((10001)_2\\) to decimal, we can use the positional notation method. \n\n\\((10001)_2 = 1*2^4 + 0*2^3 + 0*2^2 + 0*2^1 + 1*2^0\\)\n\\((10001)_2 = 16 + 1 = 17\\)\n\nSo, \\((10001)_2\\) in decimal is \\( (17)_{10} \\).\n\nTherefore, the correct option is \\( (17)_{10} \\). ///Answer: A"}
{"subject": "", "question": "Question: Let $\\mathbb F$ be a field of cardinality $q$ and let $0<k<n\\leq q$ be unspecified integers. As seen in the lecture, we generate a $(n,k,d_{min})$ Reed-Solomon code with the following mapping: $$\\mathbb F^k \rightarrow \\mathbb F^n ~~,~~ \\vec u \\mapsto \\vec c =(P_{\\vec u}(a_1),P_{\\vec u}(a_2),\\ldots,P_{\\vec u}(a_n))$$ for $a_i \\in \\mathbb F$ all distinct and $P$ a polynomial of degree $k-1$ with coefficient vector $\\vec u\\in\\mathbb F^k$.  Now, we construct a $(n,k',d'_{min})$ code $\\mathcal C'$ similarly to the above one by assigning $a_1\\leftarrow a_2$ while leaving $n,P$ and $a_2,\\ldots,a_n$ unchanged. As before, the code is generated by evaluating $P_{\\vec u}(a_2,a_2,a_3,\\dots,a_n)$ over all possible coefficients vectors $\\vec u \\in \\mathbb F^k$. This is by definition not an RS code, however it is still a well-defined linear block code. True or false: We know for certain that $k'=k-1$.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: The statement is false.\n\nReasoning: When constructing the code $\\mathcal C'$, even though we are changing the mapping by assigning $a_1\\leftarrow a_2$, the degree of the polynomial $P$ remains the same, i.e., $k-1$. Since the degree of the polynomial determines the dimension of the code, $k'$ will remain the same as $k$, i.e., $k'=k$. ///Answer: A:"}
{"subject": "", "question": "Question: Which of the following statement(s) is/are true about Safe Exception Handling (SEH)?\n\nOptions:\nA. Neither SafeSEH nor SeHOP checks the order and number of exception handlers.\nB. The implementation of SEH is compiler specific.\nC. SEH is a defense that protects C/C++ programs against control-flow hijack attacks through changing exception data structures.\nD. SafeSEH provides stronger protection than SeHOP.\n", "answer": "Explanations: A: Neither SafeSEH nor SeHOP checks the order and number of exception handlers. - This statement is true. SafeSEH and SeHOP are focused on improving the security of exception handling mechanisms but do not specifically target the order and number of exception handlers.\n\nB: The implementation of SEH is compiler specific. - This statement is also true. The implementation of SEH can vary between different compilers, as they may have different ways of handling exceptions and implementing security features.\n\nC: SEH is a defense that protects C/C++ programs against control-flow hijack attacks through changing exception data structures. - This statement is not entirely accurate. SEH (Structured Exception Handling) is a mechanism for handling exceptions in Windows systems, but it is not specifically designed to protect against control-flow hijack attacks.\n\nD: SafeSEH provides stronger protection than SeHOP. - This statement is false. SafeSEH and SeHOP are both security features related to exception handling in Windows systems, but they focus on different aspects. SafeSEH is designed to prevent certain types of buffer overflow attacks, while SeHOP (SEH Overwrite Protection) is designed to protect against overwriting exception handlers. The level of protection provided by each may vary depending on the specific attack being targeted. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{incorrect} assertion. For a cipher $C$, decorrelation theory says that \\ldots?\n\nOptions:\nA. A decorrelation $0$ of order $1$ means perfect secrecy when used once.\nB. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\cdot\\right|\\right|_a}(C)$.\nC. A decorrelation $0$ of order $1$ always protects against linear cryptanalysis.\nD. $\\mathsf{Dec}^n(C_1\\circ C_2) \\leq \\mathsf{Dec}^n(C_1) \\times \\mathsf{Dec}^n(C_2)$, for $C_1$ and $C_2$ two independent random permutations.\n", "answer": "Explanations: 1. A decorrelation of order $0$ means perfect secrecy when used once. - This is incorrect because a decorrelation of order $1$ means perfect secrecy when used once, not order $0$.\n\n2. $\\mathsf{BestAdv}_n(C,C^\\ast)=\\frac{1}{2}\\mathsf{Dec}^n_{\\left|\\left|\\cdot\\right|\\right|_a}(C)$. - This assertion is correct. \n\n3. A decorrelation of order $0$ always protects against linear cryptanalysis. - This is incorrect because a decorrelation of order $1$, not order $0, always protects against linear cryptanalysis.\n\n4. $\\mathsf{Dec}^n(C_1\\circ C_2) \\leq \\mathsf{Dec}^n(C_1) \\times \\mathsf{Dec}^n(C_2)$, for $C_1$ and $C_2$ two independent random permutations. - This assertion is correct. \n\nTherefore, the incorrect assertion is: 1. A decorrelation $0$ of order $1$ means perfect secrecy when used once. ///Answer: A:"}
{"subject": "", "question": "Question: Let us define the relation R = {(1, 1)} on the set A = {1, 2, 3}. Which of the following properties does R satisfy ? (multiple answers)?\n\nOptions:\nA. Reflexive\nB. Symmetric\nC. Anti-symmetric\nD. Transitive\nE. None of the other statements are valid\n", "answer": "Explanations: The relation R = {(1, 1)} on the set A = {1, 2, 3} satisfies the following properties:\n1. Reflexive: R is reflexive if for every element a in A, (a, a) is in R. In this case, (1, 1) is in R, so R is reflexive.\n\n2. Symmetric: R is symmetric if for every (a, b) in R, (b, a) is also in R. Since there is only one pair in R, (1, 1), and (1, 1) = (1, 1), R is symmetric.\n\n3. Anti-symmetric: R is anti-symmetric if for every (a, b) and (b, a) in R, a = b. Since there is only one pair in R, (1, 1), it vacuously satisfies the anti-symmetric property.\n\n4. Transitive: R is transitive if for every (a, b) and (b, c) in R, (a, c) is also in R. Since there is only one pair in R, (1, 1), R vacuously satisfies the transitive property.\n\nTherefore, the properties that R = {(1, 1)} on the set A = {1, 2, 3} satisfies are Reflexive, Symmetric, Anti-symmetric, and Transitive. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is true?\n\nOptions:\nA. High precision implies low recall\nB. High precision hurts recall\nC. High recall hurts precision\nD. High recall implies low precision\n", "answer": "Explanations: a) High precision implies low recall - False\nPrecision and recall are independent of each other. It is possible to have high precision and high recall at the same time. Therefore, this statement is false.\n\nb) High precision hurts recall - False\nHigh precision does not necessarily hurt recall. It is possible to have high precision and high recall at the same time. Therefore, this statement is false.\n\nc) High recall hurts precision - True\nHigh recall means capturing a large portion of relevant instances, which can lead to including some irrelevant instances as well, thus potentially reducing precision. Therefore, this statement is true.\n\nd) High recall implies low precision - False\nHigh recall does not necessarily imply low precision. It is possible to have high recall and high precision at the same time. Therefore, this statement is false.\n\nTherefore, the correct statement is:\nc) High recall hurts precision ///Answer: C"}
{"subject": "", "question": "Question: In terms of the \\textbf{bias-variance} decomposition, a 1-nearest neighbor classifier has \\rule{2cm}{0.15mm} than a 3-nearest neighbor classifier.\n\nOptions:\nA. higher variance\nB. lower variance\nC. higher bias\nD. lower bias\n", "answer": "Explanations: In terms of the bias-variance decomposition, a 1-nearest neighbor classifier has \\textbf{higher variance} than a 3-nearest neighbor classifier.\n\nTherefore, the correct option is:\n1 - higher variance ///Answer: A"}
{"subject": "", "question": "Question: Your friend Charlie was training a neural net, and observed something very curious. During training, the same network, with the same exact weights, doesn't always give the same prediction for a given example. What can cause this behavior?\n\nOptions:\nA. Residual layers (skip-connections)\nB. Convolutional layers\nC. Batch normalization\nD. SGD with momentum\nE. Adam\nF. Dropout\nG. L1 regularization\nH. L2 regularization\nI. This is not possible, Charlie made a mistake\n", "answer": "Explanations: A possible reason for the neural network with the same weights not giving the same prediction for a given example could be the presence of Dropout. Dropout is a regularization technique used during training where randomly selected neurons are ignored, or \"dropped out\", which can lead to different predictions as different subsets of neurons are active during each forward pass. Therefore, the presence of Dropout could cause this behavior. ///Answer: F"}
{"subject": "", "question": "Question: Tick the \\textbf{false} assertion. Assume that $C$ is a random permutation.\n\nOptions:\nA. BestAdv$_n(C,C^\\ast)=\\frac{1}{2}Dec^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_a}(C)$\nB. BestAdv$_n^{n.a.}(C,C^\\ast)=\\frac{1}{2}Dec^n_{\\left|\\left|\\left|\\cdot\\right|\\right|\\right|_\\infty}(C)$\nC. $E(LP^{C}(a,b))\\leq 1$\nD. $Dec^n(C\\circ C)\\leq Dec^n(C)^2$.\n", "answer": "Explanations: To determine the false assertion, let's analyze each statement:\n\n(a) BestAdv$_n(C,C^\\ast)=\\frac{1}{2}Dec^n_{||| \\cdot |||_a}(C)$: This assertion is true. It states that the best advantage of a random permutation $C$ against itself is equal to half the $a$-norm of the decryption of $C$.\n\n(b) BestAdv$_n^{n.a.}(C,C^\\ast)=\\frac{1}{2}Dec^n_{||| \\cdot |||_\\infty}(C)$: This assertion is also true. It claims that the best advantage of a non-adaptive adversary against a random permutation $C$ is half the infinity norm of the decryption of $C$.\n\n(c) $E(LP^{C}(a,b))\\leq 1$: This assertion is true as well. It states that the expected value of the leakage profile for a permutation $C$ with inputs $a$ and $b$ is less than or equal to 1.\n\n(d) $Dec^n(C\\circ C)\\leq Dec^n(C)^2$: This assertion is false. It suggests that the decryption of the composition of $C$ with itself is less than or equal to the square of the decryption of $C$, which is not generally true for permutations.\n\nTherefore, the false assertion is (d) $Dec^n(C\\circ C)\\leq Dec^n(C)^2$. ///Answer: D"}
{"subject": "", "question": "Question: Select the \\emph{incorrect} statement.\n\nOptions:\nA. The Discrete Logarithm can be solved in polynomial time on a quantum computer.\nB. The ElGamal cryptosystem is based on the Discrete Logarithm problem.\nC. The Computational Diffie-Hellman problem reduces to the Discrete Logarithm problem.\nD. The Discrete Logarithm is hard to compute for the additive group $\\mathbf{Z}_{n}$.\n", "answer": "Explanations: The incorrect statement from the options provided is: \"The Discrete Logarithm can be solved in polynomial time on a quantum computer.\"\n\nLet's break down each statement to explain why the incorrect one is indeed incorrect:\n\n1. \"The Discrete Logarithm can be solved in polynomial time on a quantum computer.\"\n   - This statement is incorrect. The Discrete Logarithm problem is one of the fundamental problems in cryptography, and it is known to be hard to solve efficiently on classical computers. While quantum computers have the potential to solve certain problems more efficiently than classical computers, the Discrete Logarithm problem is not one of them. In fact, several cryptographic schemes rely on the assumption that the Discrete Logarithm problem is computationally hard, making this statement false.\n\n2. \"The ElGamal cryptosystem is based on the Discrete Logarithm problem.\"\n   - This statement is correct. The ElGamal cryptosystem is a public-key cryptosystem based on the difficulty of solving the Discrete Logarithm problem in a specific mathematical group, such as a cyclic group modulo a prime number.\n\n3. \"The Computational Diffie-Hellman problem reduces to the Discrete Logarithm problem.\"\n   - This statement is correct. The Computational Diffie-Hellman problem is closely related to the Discrete Logarithm problem. In fact, the security of many cryptographic protocols, including the Diffie-Hellman key exchange protocol, relies on the assumption that the Discrete Logarithm problem is hard to solve.\n\n4. \"The Discrete Logarithm is hard to compute for the additive group $\\mathbf{Z}_{n}$.\"\n   - This statement is correct. The Discrete Logarithm problem is indeed hard to compute in the additive group modulo $n$ ($\\mathbf{Z}_{n}$), especially when $n$ is a large prime number. This forms the basis for many cryptographic systems that rely on the hardness of solving the Discrete Logarithm problem in such groups.\n\nTherefore, the incorrect statement is the first one claiming that the Discrete Logarithm can be solved in polynomial time on a quantum computer. ///Answer: A"}
{"subject": "", "question": "Question: Which of the following Diffie-Hellman instance is believed to be secure?\n\nOptions:\nA. Diffie-Hellman in a subgroup of $\\left\\{ 0,\\dots,n \\right\\}$ (with the addition) of prime order $q$ with $q$ a $200$-bit prime and $n$ a $2048$-bit integer.\nB. Diffie-Hellman over a subgroup of a good Elliptic curve over $Z_p$ of prime order $q$, with $q$ a $200$-bit prime and $p$ a $2048$-bit prime.\nC. Diffie-Hellman over a subgroup of $Z_p^*$ of order $q$, with $q$ a $30$-bit prime and $p$ a $2048$-bit prime.\nD. Diffie-Hellman over a subgroup of $Z_p^*$ of order $q=2^{128}(127)^{40}$, with $p$ a $2048$-bit prime.\n", "answer": "Explanations: The Diffie-Hellman key exchange protocol is a fundamental cryptographic algorithm that relies on the difficulty of solving the discrete logarithm problem. In order to ensure the security of the key exchange, it is crucial to select appropriate parameters for the Diffie-Hellman instance.\n\nOption 1: Diffie-Hellman in a subgroup of $\\left\\{ 0,\\dots,n \\right\\}$ (with the addition) of prime order $q$ with $q$ a 200-bit prime and $n$ a 2048-bit integer.\nThis option lacks specificity in terms of the prime order of the subgroup, leaving it potentially vulnerable to attacks.\n\nOption 2: Diffie-Hellman over a subgroup of a good Elliptic curve over $Z_p$ of prime order $q$, with $q$ a 200-bit prime and $p$ a 2048-bit prime.\nElliptic curve cryptography is renowned for its security and efficiency. The specification of a prime order for the subgroup in this option enhances its security and makes it a robust choice for Diffie-Hellman key exchange.\n\nOption 3: Diffie-Hellman over a subgroup of $Z_p^*$ of order $q$, with $q$ a 30-bit prime and $p$ a 2048-bit prime.\nThe use of a small 30-bit prime for the order of the subgroup in this option makes it vulnerable to attacks due to the limited key space.\n\nOption 4: Diffie-Hellman over a subgroup of $Z_p^*$ of order $q=2^{128}(127)^{40}$, with $p$ a 2048-bit prime.\nThe specification of a large prime order for the subgroup in this option significantly enhances the security of the Diffie-Hellman instance, making it a strong and secure choice for key exchange.\n\nTherefore, the most secure Diffie-Hellman instances among the options provided are Option 2 and Option 4. These options stand out for their use of Elliptic curve cryptography and the specification of a large prime order for the subgroup, ensuring robust security in the key exchange process. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements about fuzzing is/are correct?\n\nOptions:\nA. Generational fuzzing requires more manual work (to specify the\n            generator policies) than mutational fuzzing, but can generate\n            high-quality seeds.\nB. Blackbox fuzzers can make use of initial seeds.\nC. Greybox fuzzing is always the better alternative to\n            blackbox fuzzing.\nD. Greybox fuzzing keeps track of concrete program paths to\n            abstract behavior.\n", "answer": "Explanations: 1) **True**. Generational fuzzing indeed requires more manual work to specify the generator policies compared to mutational fuzzing. However, it can generate high-quality seeds by carefully defining how to mutate the initial seeds.\n\n2) **True**. Blackbox fuzzers can make use of initial seeds to guide the fuzzing process. These seeds help in providing an initial set of inputs for the fuzzer to mutate and test.\n\n3) **False**. Greybox fuzzing is not always the better alternative to blackbox fuzzing. The choice between greybox and blackbox fuzzing depends on the specific requirements of the testing scenario. Blackbox fuzzing may be more suitable when the internal details of the program are not accessible or not required for the testing.\n\n4) **True**. Greybox fuzzing does keep track of concrete program paths to abstract behavior. By monitoring the code coverage and program paths taken during execution, greybox fuzzing can guide the generation of new test cases. ///Answer: B"}
{"subject": "", "question": "Question: One-hot encoding:?\n\nOptions:\nA. encode a state or category, with a group of bits whose unique representation is with a single high (1) and others low (0).  \nB. encode continuous values into a unique temperature representation between 0 and 1.\nC. encode a state or category, with a group of bits whose unique representation is with a single low (0) and others high (1).\nD. encode continuous values into unique multi-dimensional temperature representations between 0 and 1 \n", "answer": "Explanations: Problem: What does one-hot encoding refer to?\n\nSolution:\nOne-hot encoding is a technique commonly used in machine learning and artificial intelligence to represent categorical data. It is particularly useful when working with categorical variables that do not have an inherent numeric order or relationship between them. \n\nThe correct option is:\n1: encode a state or category, with a group of bits whose unique representation is with a single high (1) and others low (0).\n\nExplanation:\n1. One-hot encoding involves converting categorical variables into a binary format where each category is converted into a binary vector with all zeros except for one element, which is marked with a one (hence \"one-hot\").\n   \n2. For instance, consider a variable \"Color\" with three categories: Red, Blue, Green. After one-hot encoding, the representation would be as follows:\n   - Red: [1, 0, 0]\n   - Blue: [0, 1, 0]\n   - Green: [0, 0, 1]\n\n3. In this representation, each category is independent of the others and can be considered a separate binary feature for machine learning algorithms. It ensures that no ordinal relationship is assumed between the categories.\n\n4. One-hot encoding helps prevent the model from misinterpreting the categorical variables as having some sort of ordered relationship, which is crucial when dealing with nominal (non-ordinal) data.\n\n5. This technique is widely used in various machine learning models such as logistic regression, neural networks, decision trees, and support vector machines to effectively handle categorical data during training and prediction phases.\n\nTherefore, the correct description of one-hot encoding is option 1: encode a state or category, with a group of bits whose unique representation is with a single high (1) and others low (0). ///Answer: A"}
{"subject": "", "question": "Question: Which of the following measures will always improve fuzzing executions per second?\n\nOptions:\nA. Reducing overheads imposed by the fuzzing framework.\nB. Performing structure-aware input generation.\nC. Providing dictionaries for input generation.\nD. Collecting code coverage as feedback.\n", "answer": "Explanations: Certainly! Here is a more detailed standalone answer exploring the impact of different measures on improving fuzzing executions per second:\n\n1. Reducing overheads imposed by the fuzzing framework:\n- Minimizing overheads imposed by the fuzzing framework can lead to improved fuzzing executions per second by optimizing resource utilization and eliminating unnecessary processing delays. By reducing the computational burden and streamlining the execution flow within the fuzzing framework, more resources can be dedicated to actual fuzzing tasks, enhancing the speed and efficiency of discovering vulnerabilities. Efficient resource allocation and overhead reduction are crucial factors in boosting the performance of fuzzing campaigns and accelerating the rate of executions per second.\n\n2. Performing structure-aware input generation:\n- Structure-aware input generation plays a pivotal role in enhancing the quality and effectiveness of fuzzing by generating inputs tailored to target specific code paths or vulnerabilities. While this measure can significantly improve the thoroughness and coverage of the fuzzing process, its direct impact on increasing fuzzing executions per second may not be guaranteed. The emphasis here is on generating inputs that are more likely to trigger critical behaviors or uncover hidden flaws, which may result in more impactful findings but may not always translate into a faster rate of executions per second.\n\n3. Providing dictionaries for input generation:\n- Supplying dictionaries for input generation can streamline the fuzzing process by offering predefined sets of values or templates for creating inputs. While dictionaries can improve the efficiency and diversity of generated inputs, their influence on boosting fuzzing executions per second may vary. The effectiveness of dictionaries in guiding input generation and exploring different code paths can contribute to the overall efficiency of the fuzzing campaign but may not always directly correlate with a significant increase in the speed of executions per second. \n\n4. Collecting code coverage as feedback:\n- Incorporating code coverage feedback into the fuzzing process enables the fuzzer to gain insights into the areas of code exercised by the generated inputs. By leveraging code coverage data as feedback, the fuzzer can prioritize inputs that explore new code paths, potentially leading to more efficient and targeted fuzzing. While collecting code coverage feedback enhances the overall effectiveness of fuzzing campaigns by focusing on unexplored areas of the codebase, its direct impact on improving fuzzing executions per second is contingent on how effectively the fuzzer utilizes this information to guide input generation and maximize code coverage exploration.\n\nIn conclusion, while reducing overheads imposed by the fuzzing framework is more likely to consistently improve fuzzing executions per second by optimizing resource utilization and streamlining processing, other measures such as structure-aware input generation, providing dictionaries, and collecting code coverage feedback also play essential roles in enhancing the effectiveness and efficiency of fuzzing campaigns, albeit with varying impacts on the speed of executions per second. It is essential to consider a holistic approach that balances optimization strategies to achieve both quality and speed in fuzzing executions. ///Answer: A"}
{"subject": "", "question": "Question: Given the 2-itemsets {1,2}, {1,5}, {2,5}, {1,4}, {1,3}, when generating the 3-itemsets we\nwill?\n\nOptions:\nA. Generate 5 3-itemsets after the join and 2 3-itemsets after the prune\nB. Generate 6 3-itemsets after the join and 1 3-itemsets after the prune\nC. Generate 4 3-itemsets after the join and 1 3-itemsets after the prune\nD. Generate 4 3-itemsets after the join and 2 3-itemsets after the prune\n", "answer": "Explanations: To generate the 3-itemsets from the given 2-itemsets {1,2}, {1,5}, {2,5}, {1,4}, {1,3}, we need to perform join and prune operations based on the Apriori algorithm for frequent itemset mining.\n\nGiven 2-itemsets:\n- {1,2}\n- {1,5}\n- {2,5}\n- {1,4}\n- {1,3}\n\nLet's first perform the join step to generate candidate 3-itemsets by combining the 2-itemsets:\nAfter the join:\n- {1,2,5}\n- {1,2,4}\n- {1,2,3}\n- {1,5,4}\n- {1,5,3}\n\nNext, we need to prune the candidate 3-itemsets based on the downward closure property, which states that if any subset of a frequent itemset is infrequent, the itemset itself cannot be frequent.\n\nAfter the prune:\n- {1,2,5} (Frequent)\n- {1,2,4} (Infrequent)\n- {1,2,3} (Infrequent)\n- {1,5,4} (Infrequent)\n- {1,5,3} (Infrequent)\n\nFrom the above, we see that only {1,2,5} is a frequent 3-itemset after the prune operation.\n\nTherefore, the correct option is: Generate 1 3-itemset after the join and 0 3-itemsets after the prune. ///Answer: B"}
{"subject": "", "question": "Question: How do you split your data? (One or multiple answers)?\n\nOptions:\nA. 60\\% of the oldest bridges into training, the next 20\\% into validation and the newest 20\\% of bridges into the test set\nB. randomly sample with a $60\\%:20\\%:20\\%$ split for training, validation and test set\nC. use steel bridges for training and concrete bridges for the test set\nD. use long bridges for training and short bridges for the test set\n", "answer": "Explanations: The correct answer is Option 2: randomly sample with a $60\\%:20\\%:20\\%$ split for training, validation, and test set.\n\nThis option is the most appropriate because it ensures a random and unbiased distribution of data across the training, validation, and test sets. Random sampling helps to prevent any potential biases in the data distribution, which can affect the performance and generalizability of the model.\n\nOptions 1, 3, and 4 involve specific criteria for splitting the data based on certain characteristics of the bridges (e.g., age, material, length). While these criteria may seem reasonable, they can introduce biases in the model if the selected characteristics have a significant impact on the target variable. For example, if the age of the bridge is strongly correlated with its structural integrity, splitting the data based on age could result in an overfit model that performs poorly on new data.\n\nIn contrast, random sampling ensures that the data is divided in a way that is representative of the overall dataset, allowing the model to learn from a diverse range of examples and make more accurate predictions on unseen data. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements is wrong regarding RDF?\n\nOptions:\nA. An RDF statement would be expressed in SQL as a tuple in a table\nB. Blank nodes in RDF graphs correspond to the special value NULL in SQL\nC. The object value of a type statement corresponds to a table name in SQL\nD. RDF graphs can be encoded as SQL databases\n", "answer": "Explanations: Let's break down each statement and determine which one is wrong:\n\n1. An RDF statement would be expressed in SQL as a tuple in a table:\n- This statement is correct. In SQL, a tuple in a table can represent an RDF statement.\n\n2. Blank nodes in RDF graphs correspond to the special value NULL in SQL:\n- This statement is incorrect. Blank nodes in RDF graphs do not directly correspond to the special value NULL in SQL. Blank nodes are used in RDF to represent entities without a specific identifier, while NULL in SQL represents a missing or unknown value.\n\n3. The object value of a type statement corresponds to a table name in SQL:\n- This statement is incorrect. In RDF, the object value of a type statement refers to the data type or class of the subject entity, not a table name in SQL.\n\n4. RDF graphs can be encoded as SQL databases:\n- This statement is correct. RDF graphs can be encoded as SQL databases by representing the subject-predicate-object triples as rows in tables.\n\nTherefore, the wrong statement regarding RDF is:\n- 3. The object value of a type statement corresponds to a table name in SQL. ///Answer: C"}
{"subject": "", "question": "Question: Consider a biased coin that lands on 'head' twice as much as it lands on 'tail'. You flip this coin 5 times. What is the probability that you get at least one head?\n\nOptions:\nA. $$\frac{242}{243}$$\nB. $$\frac{211}{243}$$\nC. $$\frac{31}{32}$$\nD. $$\frac{1}{243}$$\n", "answer": "Explanations: To solve this problem, we first need to calculate the probability of getting tails and heads for the biased coin.\n\nLet's denote the probability of landing on head as \\( p(H) \\) and the probability of landing on tail as \\( p(T) \\).\n\nGiven that the biased coin lands on 'head' twice as much as it lands on 'tail', we have:\n\\[ p(H) = 2p(T) \\]\n\nSince the total probability must equal 1, we have:\n\\[ p(H) + p(T) = 1 \\]\n\\[ 2p(T) + p(T) = 1 \\]\n\\[ 3p(T) = 1 \\]\n\\[ p(T) = \\frac{1}{3} \\]\n\\[ p(H) = 2 \\times \\frac{1}{3} = \\frac{2}{3} \\]\n\nNow, we need to calculate the probability of getting at least one head in 5 flips.\n\nThe probability of getting no heads in 5 flips is:\n\\[ P(\\text{no heads}) = (1 - p(H))^5 = \\left(\\frac{1}{3}\\right)^5 \\]\n\nTherefore, the probability of getting at least one head in 5 flips is:\n\\[ P(\\text{at least one head}) = 1 - P(\\text{no heads}) = 1 - \\left(\\frac{1}{3}\\right)^5 \\]\n\nCalculating this value gives us:\n\\[ P(\\text{at least one head}) = 1 - \\left(\\frac{1}{243}\\right) = \\frac{242}{243} \\]\n\nSo, the correct option is (1) \\( \\frac{242}{243} \\). ///Answer: A"}
{"subject": "", "question": "Question: Which of the following is/are true about fuzzing?\n\nOptions:\nA. Having too many initial seeds might harm fuzzing performance.\nB. Fuzzing open-source software allows the analyst to modify the\n               target software to remove parts where the fuzzer might get stuck\n               (such as checksums).\nC. When fuzzing open-source software, recompiling it with\n                  mitigations disabled will improve the fuzzing process.\nD. Fuzzing can only be applied to C/C++ programs.\n", "answer": "Explanations: - Having too many initial seeds can harm fuzzing performance.\n- Fuzzing open-source software allows for modifications to remove obstacles like checksums.\n- Recompiling open-source software with mitigations disabled can improve fuzzing.\n- Fuzzing is not limited to C/C++ programs. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following integers has the square roots $\\{2,3\\}$ when taken modulo $5$ \\textbf{and} the square roots $\\{3,10\\}$ when taken modulo $13$.\n\nOptions:\nA. $4$.\nB. $9$.\nC. $6$.\nD. $5$.\n", "answer": "Explanations: I choose option 2, considering insertion, deletion and substitution.\n\nExplanation:\n\nFirst, let's consider the modulo $5$ condition. We need to find integers $x$ such that $x^2 \\equiv 2 \\pmod{5}$ and $x^2 \\equiv 3 \\pmod{5}$. The squares modulo $5$ are $0^2 \\equiv 0, 1^2 \\equiv 1, 2^2 \\equiv 4, 3^2 \\equiv 4, 4^2 \\equiv 1$. We see that $2$ has no square root modulo $5$, and $3$ has no square root modulo $5$. Therefore, there are no solutions modulo $5$.\n\nNext, let's consider the modulo $13$ condition. We need to find integers $x$ such that $x^2 \\equiv 3 \\pmod{13}$ and $x^2 \\equiv 10 \\pmod{13}$. The squares modulo $13$ are $0^2 \\equiv 0, 1^2 \\equiv 1, 2^2 \\equiv 4, 3^2 \\equiv 9, 4^2 \\equiv 3, 5^2 \\equiv 12, 6^2 \\equiv 10, 7^2 \\equiv 9, 8^2 \\equiv 3, 9^2 \\equiv 12, 10^2 \\equiv 1, 11^2 \\equiv 4, 12^2 \\equiv 0$. We see that $3$ has the square roots $6$ and $9$, and $10$ has the square roots $6$ and $9$ modulo $13$.\n\nSince the only integer that satisfies both conditions is $9$, which is equivalent to $9$ modulo $13$, we choose option 2, considering insertion, deletion, and substitution. ///Answer: B:"}
{"subject": "", "question": "Question: For each of the sub-questions of this question (next page), tick/check the corresponding box if the presented sentence is correct\nat the corresponding level (for a human). There will be a penalty for wrong boxes ticked/checked.The Times is a British daily national\nnewspaper based in London.\n\nOptions:\nA. lexical\nB. syntactic\nC. semantic\nD. pragmatic\nE. none of the above is correct\n", "answer": "Explanations: A: lexical - Correct\nB: syntactic - Correct\nC: semantic - Correct\nD: pragmatic - Correct\nE: none of the above is correct - Incorrect\n\nExplanation:\n- Lexical level: The sentence is correct at the lexical level as it contains words that are correctly spelled and make sense.\n- Syntactic level: The sentence is correct at the syntactic level as the words are arranged in a grammatically correct manner.\n- Semantic level: The sentence is correct at the semantic level as it conveys a clear meaning about The Times being a British daily national newspaper based in London.\n- Pragmatic level: The sentence is correct at the pragmatic level as it follows the appropriate conventions and expectations for conveying information about a newspaper. ///Answer: E"}
{"subject": "", "question": "Question: Tick the \\emph{correct} assertion. The maximum advantage of an \\textbf{adaptive} distinguisher limited to $q$ queries between two random functions $F$ and $F^*$ is always\\dots?\n\nOptions:\nA. $\\frac{1}{2}|||[F]^q - [F^*]^q |||_{\\infty}$.\nB. $\\frac{1}{2}|||[F]^q - [F^*]^q |||_{a}$.\nC. $1$ when $F = F^*$.\nD. lower than the advantage of the best \\textbf{non-adaptive} distinguisher.\n", "answer": "Explanations: Let's analyze each option:\n\n1) The expression $\\frac{1}{2}|||[F]^q - [F^*]^q |||_{\\infty}$ represents the advantage of a non-adaptive distinguisher. Since the question asks for the maximum advantage of an adaptive distinguisher, this is not the correct assertion.\n\n2) The expression $\\frac{1}{2}|||[F]^q - [F^*]^q |||_{a}$ is not a standard notation for the advantage of a distinguisher, so this is not the correct assertion.\n\n3) When $F = F^*$, the advantage of any distinguisher (adaptive or non-adaptive) is always $0$ because the two functions are identical. Therefore, this is not the correct assertion.\n\n4) The correct assertion is: the maximum advantage of an adaptive distinguisher limited to $q$ queries between two random functions $F$ and $F^*$ is lower than the advantage of the best non-adaptive distinguisher. This statement is true because adaptive distinguishers have more power and flexibility than non-adaptive ones due to their ability to adapt based on previous queries.\n\nTherefore, the correct assertion is:\n4) lower than the advantage of the best \\textbf{non-adaptive} distinguisher. ///Answer: D"}
{"subject": "", "question": "Question: Consider a learning algorithm that has the property that it depends only very weakly on the input data. E.g., this could be SGD where we choose a very small step size and only run for very few iterations. To go to the extreme, you can imagine a learning algorithm that always outputs the same model irrespective of the training set. Presumably such a learning algorithm will not give us good results. Why is that?\n\nOptions:\nA. (a)  Such a learning algorithm typically has a much larger generalization error than training error.\nB. (b)  Such a learning algorithm typically has a large bias.\nC. (c)  Such a learning algorithm is prone to overfitting.\n", "answer": "Explanations: The correct answer is Option 2: (b) Such a learning algorithm typically has a large bias.\n\nWhen a learning algorithm always outputs the same model irrespective of the training set, it indicates that the algorithm is not able to learn from the data provided. This lack of flexibility in the model leads to high bias, meaning that the model is not able to capture the true relationship between the input features and the target variable. As a result, the model will consistently make incorrect predictions, leading to poor performance on both the training and test data.\n\nIn contrast, overfitting (Option 3) occurs when a model is too complex and fits the training data too closely, leading to poor generalization to new, unseen data. In this case, the learning algorithm is not overfitting as it is not learning anything from the data at all, resulting in high bias and poor generalization performance.\n\nOption 1 is also incorrect as generalization error is typically higher than training error for models that overfit, rather than for models with high bias. ///Answer: B"}
{"subject": "", "question": "Question: A passive adversary can \\ldots?\n\nOptions:\nA. do nothing.\nB. only listen to communications.\nC. only interfere with client or server communications.\nD. only replace some communication messages by others.\n", "answer": "Explanations: A passive adversary can intercept and monitor communications between a client and server without modifying the messages. This type of adversary engages in eavesdropping activities, capturing data exchanged between the communicating parties. By passively listening to the communication flow, the adversary can potentially gather sensitive information without alerting the legitimate users to their presence. This passive approach allows the adversary to analyze the transmitted data for patterns, vulnerabilities, or valuable insights without directly influencing the communication process. The passive nature of this adversary distinguishes it from active adversaries, who engage in more intrusive actions such as modifying or blocking messages. ///Answer: B"}
{"subject": "", "question": "Question: The statistical distance between two distributions is \\dots?\n\nOptions:\nA. unrelated to the advantage of a distinguisher.\nB. a lower bound on the advantage of \\emph{all} distinguishers (with a unique sample).\nC. an upper bound on the advantage of \\emph{all} distinguishers (with a unique sample).\nD. an upper bound on the advantage of all distinguishers making statistics on the obtained samples.\n", "answer": "Explanations: The correct statement is : a lower bound on the advantage of \\emph{all} distinguishers (with a unique sample). The statistical distance between two distributions is a measure of how far apart the two distributions are. It provides a lower bound on the advantage of all possible distinguishers when given a single sample. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements are true about DEP?\n\nOptions:\nA. DEP prohibits code injection but not code reuse.\nB. A single bit is used to indicate that a memory region is DEP protected.\nC. Even if it is called Enhanced Virus Protection on AMD hardware, it does not protect against viruses.\nD. Data Execution Prevention is usually handled at byte-level granularity.\nE. A region marked as DEP protected cannot be read.\n", "answer": "Explanations: 1. DEP prohibits code injection but not code reuse.\n2. A single bit is used to indicate that a memory region is DEP protected.\n4. Data Execution Prevention is usually handled at byte-level granularity. \n\nExplanation:\n1. This statement is true. DEP helps prevent attacks that involve injecting malicious code into memory regions that are meant for data storage.\n2. This statement is also true. DEP uses a single bit to indicate whether a memory region is protected by DEP or not.\n4. This statement is true as well. DEP operates at the byte-level granularity, allowing it to protect specific memory regions from being executed. ///Answer: A"}
{"subject": "", "question": "Question: How is data integrity ensured in WEP?\n\nOptions:\nA. there is no integrity.\nB. A3.\nC. Michael.\nD. HMAC.\n", "answer": "Explanations: The final answer is:\n\n4. HMAC.\n\nExplanation: While options like \"A3\" and \"Michael\" are related to other aspects of WEP, data integrity in WEP is ensured by using the CRC32 checksum in the frame check sequence (FCS). Although HMAC is a method that can also be used for data integrity, in WEP specifically, CRC32 is utilized for this purpose. ///Answer: D"}
{"subject": "", "question": "Question: Let $\\mathcal{C}$ be a binary $(5,2)$ linear code with generator matrix  \begin{equation*}    G = \begin{pmatrix}     1 &0 &1 &0 &1 \\      0 &1 &0 &1 &1      \\end{pmatrix}      \\end{equation*}  and consider a minimum-distance decoder obtained by choosing the coset leaders of the standard array of $\\mathcal{C}$ so that the error probability is minimized under a binary symmetric channel with bit-flip probability $\\epsilon < \frac{1}{2}$. True or false: The decoder can correct all errors of weight 1.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: True, the decoder obtained by choosing the coset leaders of the standard array of a binary code is a minimum-distance decoder. For a binary $(5,2)$ linear code $\\mathcal{C}$ with generator matrix $G = \\begin{pmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 1 & 1 \\end{pmatrix}$, the minimum distance of the code can be calculated as the minimum weight of the non-zero codewords. In this case, the minimum distance of the code is 2, which means the decoder can correct all errors of weight 1, making the statement True. ///Answer: A"}
{"subject": "", "question": "Question: Consider a blockcipher $\\mathsf{B}:\\mathcal{K}\\times\\{0,1\\}^n \\rightarrow \\{0,1\\}^n$ with $|\\mathcal{K}|=N$ for which we choose a secret key $K\\in\\mathcal{K}$ uniformly at random. Tick the \\textit{correct} assertion.\n\nOptions:\nA. If $\\mathsf{B}$ is secure against known-plaintext decryption attack, then it is surely secure against ciphertext-only key recovery attack.\nB. If $\\mathsf{B}$ is secure against known-plaintext decryption attack, then it is surely secure against chosen-ciphertext decryption attack.\nC. It is possible to recover $K$ with precomputation: $O(N^{2/3})$, memory: $O(N^{2/3})$, time: $O(1)$.\nD. It is possible to recover $K$ key with precomputation: $O({1})$, memory: $O(N)$, time: $O(1)$.\n", "answer": "Explanations: **Hint**: Consider the different types of attacks and their implications on the security of the block cipher. Think about the resources required for each type of attack and how they relate to the security guarantees.\n\n**Solution**:\nThe correct assertion is:\n\n**2) If $\\mathsf{B}$ is secure against known-plaintext decryption attack, then it is surely secure against chosen-ciphertext decryption attack.**\n\nExplanation:\n1. Known-plaintext attack: In this attack scenario, the adversary has access to plaintext-ciphertext pairs and aims to recover the secret key $K$.\n   \n2. Chosen-ciphertext attack: In this attack scenario, the adversary can choose ciphertexts and obtain their corresponding decryptions. The goal is still to recover the secret key $K$.\n\nIf a block cipher is secure against a known-plaintext decryption attack, it implies that the adversary cannot easily recover the key when provided with plaintext-ciphertext pairs. Since a chosen-ciphertext attack is more powerful (as the adversary can choose ciphertexts for decryption), if the block cipher is secure against known-plaintext attacks, it is also secure against chosen-ciphertext attacks.\n\nThe other options are incorrect:\n1. If a block cipher is secure against known-plaintext attacks, it does not guarantee security against ciphertext-only attacks because in that scenario, the attacker has only access to the ciphertexts.\n   \n3. Recovering the key with precomputation and memory/time complexity $O(N^{2/3})$ does not align with typical key recovery scenarios for secure block ciphers.\n\n4. Recovering the key with precomputation and memory complexity $O(N)$ is not feasible in real-world scenarios for secure block ciphers due to the large memory requirement. ///Answer: B"}
{"subject": "", "question": "Question: For each of the sub-questions of this question (next page), tick/check the corresponding box if the presented sentence is correct\nat the corresponding level (for a human). There will be a penalty for wrong boxes ticked/checked.The mouse lost a feather as it took off.\n\nOptions:\nA. lexical\nB. syntactic\nC. semantic\nD. pragmatic\nE. none of the above is correct\n", "answer": "Explanations: Pragmatic - This is the best answer because pragmatics deals with the study of language in context, including how language is used to achieve goals. In this sentence, the action of a mouse losing a feather as it takes off implies a cause-effect relationship that is understood in a pragmatic context. ///Answer: D"}
{"subject": "", "question": "Question: What is the order of 11 in $\\mathbb{Z}_{37}$?\n\nOptions:\nA. 1\nB. 27\nC. 36\nD. 37\n", "answer": "Explanations: To find the order of 11 in $\\mathbb{Z}_{37}$, we need to find the smallest positive integer $k$ such that $11^k \\equiv 1 \\pmod{37}$.\n\nWe can start by calculating the powers of 11 modulo 37:\n\n$11^1 \\equiv 11 \\pmod{37}$ \\\\\n$11^2 \\equiv 22 \\pmod{37}$ \\\\\n$11^3 \\equiv 5 \\pmod{37}$ \\\\\n$11^4 \\equiv 2 \\pmod{37}$ \\\\\n$11^5 \\equiv 7 \\pmod{37}$ \\\\\n$11^6 \\equiv 13 \\pmod{37}$ \\\\\n$11^7 \\equiv 26 \\pmod{37}$ \\\\\n$11^8 \\equiv 20 \\pmod{37}$ \\\\\n$11^9 \\equiv 17 \\pmod{37}$ \\\\\n$11^{10} \\equiv 34 \\pmod{37}$ \\\\\n$11^{11} \\equiv 25 \\pmod{37}$ \\\\\n$11^{12} \\equiv 32 \\pmod{37}$ \\\\\n$11^{13} \\equiv 16 \\pmod{37}$ \\\\\n$11^{14} \\equiv 31 \\pmod{37}$ \\\\\n$11^{15} \\equiv 29 \\pmod{37}$ \\\\\n$11^{16} \\equiv 23 \\pmod{37}$ \\\\\n$11^{17} \\equiv 8 \\pmod{37}$ \\\\\n$11^{18} \\equiv 4 \\pmod{37}$ \\\\\n$11^{19} \\equiv 10 \\pmod{37}$ \\\\\n$11^{20} \\equiv 35 \\pmod{37}$ \\\\\n$11^{21} \\equiv 28 \\pmod{37}$ \\\\\n$11^{22} \\equiv 19 \\pmod{37}$ \\\\\n$11^{23} \\equiv 14 \\pmod{37}$ \\\\\n$11^{24} \\equiv 1 \\pmod{37}$\n\nTherefore, the order of 11 in $\\mathbb{Z}_{37}$ is 24. So, the correct option is '36'. ///Answer: C"}
{"subject": "", "question": "Question: Which algorithm can be typically used in order to generate a prime number?\n\nOptions:\nA. The Left to Right Algorithm\nB. The Extended Euclidean Algorithm\nC. The Miller-Rabin Test\nD. The Tonelli Algorithm\n", "answer": "Explanations: C: The Miller-Rabin Test\n\nExplanation: The Miller-Rabin Test is a probabilistic algorithm commonly used to determine whether a given number is prime or composite. It is efficient and reliable for generating prime numbers. ///Answer: C"}
{"subject": "", "question": "Question: Consider a $(k+1,k)$ block code that to a binary sequence $x_1,\\dots,x_k$ associates the codeword $x_1,\\dots,x_k,x_{k+1}$, where $x_{k+1}= x_1+\\ldots+x_k$ mod $2$. This code can detect all the errors of odd weight.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: Answer: True\n\nExplanation:\nTo determine if the statement is true, we need to understand how the $(k+1,k)$ block code detects errors of odd weight. In this specific code, the additional bit $x_{k+1}$ is calculated as the sum of all the preceding bits modulo 2.\n\nWhen an error occurs and changes an odd number of bits in the codeword, it will cause the parity of the codeword to change. Since the additional bit is calculated as the sum of all preceding bits modulo 2, any error that changes an odd number of bits will result in an error in the calculated parity bit $x_{k+1}$.\n\nBy checking the parity of the received codeword, if the parity calculated using the received bits does not match the calculated parity bit $x_{k+1}$, it indicates that an error of odd weight has occurred. Therefore, the $(k+1,k)$ block code can detect errors of odd weight.\n\nIn conclusion, the statement that the given code can detect all errors of odd weight is true, based on the way the parity bit is calculated and how errors affecting an odd number of bits will result in a detectable change in parity. ///Answer: A"}
{"subject": "", "question": "Question: Suppose that you possess a $D$-ary encoding $\\Gamma$ for the source $S$ that does not satisfy Kraft's Inequality. Specifically, in this problem, we assume that our encoding satisfies $\\sum_{i=1}^n D^{-l_i} = k+1 $ with $k>0$. What can you infer on the average code-word length $L(S,\\Gamma)$?\n\nOptions:\nA. $L(S,\\Gamma) \\geq H_D(S)-\\log_D(e^k)$.\nB. $L(S,\\Gamma) \\geq k H_D(S)$.\nC. $L(S,\\Gamma) \\geq \frac{H_D(S)}{k}$.\nD. The code would not be uniquely-decodable and thus we can't infer anything on its expected length.\n", "answer": "Explanations: The correct answer is:\n\n1. $L(S,\\Gamma) \\geq H_D(S)-\\log_D(e^k)$.\n\nThis is because when Kraft's Inequality is violated and $\\sum_{i=1}^n D^{-l_i} = k+1 $ with $k>0$, it implies that the average code-word length $L(S,\\Gamma)$ must be greater than or equal to the entropy of the source $S$ in base $D$ minus the logarithm base $D$ of the exponential function of $k$. This is a consequence of the violation of Kraft's Inequality and the relationship between code efficiency and entropy. ///Answer: A"}
{"subject": "", "question": "Question: What are the drawbacks of non-preemptive scheduling compared to preemptive scheduling?\n\nOptions:\nA. It can lead to starvation especially for those real-time tasks\nB. Less computational resources need for scheduling and takes shorted time to suspend the running task and switch the context.\nC. Bugs in one process can cause a machine to freeze up\nD. It can lead to poor response time for processes\n", "answer": "Explanations: Non-preemptive scheduling has several drawbacks compared to preemptive scheduling. Some of these drawbacks include:\n\n1. Starvation: Non-preemptive scheduling can lead to starvation, especially for real-time tasks. This means that certain processes may never get a chance to execute if higher priority tasks are constantly being scheduled.\n\n2. Poor response time: Non-preemptive scheduling can lead to poor response time for processes, as the scheduler does not have the ability to interrupt a running task to allocate resources to more urgent tasks.\n\n3. Machine freeze-up: Bugs in one process can cause the entire system to freeze up in non-preemptive scheduling. Since the scheduler cannot intervene and switch to a different process, a bug in one process can bring the entire system to a halt.\n\nOverall, while non-preemptive scheduling may require fewer computational resources and take less time to suspend running tasks and switch contexts, it is not as efficient or effective as preemptive scheduling in terms of fairness, responsiveness, and system stability. ///Answer: A"}
{"subject": "", "question": "Question: How many necessary and sufficient queries do we need to distinguish the distribution $P$ from the uniform distribution $U$ with significant advantage?\n\nOptions:\nA. $d(P,U)^{-1}$\nB. $\\infty$\nC. $\\Theta(SEI(P)^{-1})$\nD. $\\Theta(C(P,U))$\n", "answer": "Explanations: The correct answer is $\\Theta(C(P,U))$.\n\nExplanation:\nIn order to distinguish the distribution $P$ from the uniform distribution $U$ with significant advantage, we need to consider the statistical distance between the two distributions. The statistical distance between two distributions $P$ and $Q$ is defined as $d(P,Q) = \\frac{1}{2}\\sum_{x} |P(x) - Q(x)|$. If this distance is significant, then we can use queries to distinguish $P$ from $U$.\n\nThe number of necessary and sufficient queries required to distinguish $P$ from $U$ with significant advantage is given by the complexity of the problem, denoted as $C(P,U)$. This complexity captures the minimum number of queries required to distinguish $P$ from $U$ with significant advantage.\n\nTherefore, the correct answer is $\\Theta(C(P,U))$. ///Answer: D"}
{"subject": "", "question": "Question: Que dit la loi d'Ohm g\u00e9n\u00e9ralis\u00e9e \u00e0 propos de la tension au bornes d'une bobine travers\u00e9e par un courant ? (une seule r\u00e9ponse possible)?\n\nOptions:\nA. rien\nB. qu'elle est \u00e9gale \u00e0 la r\u00e9sistance du fil multipli\u00e9 par le courant\nC. qu'elle est \u00e9gale \u00e0 la r\u00e9sistance du fil multipli\u00e9 par le courant, plus la variation du flux totalis\u00e9\nD. qu'elle est \u00e9gale \u00e0 la r\u00e9sistance du fil multipli\u00e9 par le courant, mois la variation du flux totalis\u00e9\n", "answer": "Explanations: To understand the transformation from binary MRSW safe registers to binary MRSW regular registers, we first need to define these terms:\n\n- Binary MRSW safe registers: Multiple readers, single writer registers that allow for concurrent reads by multiple processes and ensure consistency when a write operation is performed.\n- Binary MRSW regular registers: Multiple readers, single writer registers that allow for concurrent reads by multiple processes, but do not necessarily ensure consistency when a write operation is performed.\n\nNow, let's consider a counterexample to prove that the transformation does not generate multi-valued MRSW regular registers:\n\nCounterexample for regularity:\nSuppose we have two processes, P1 and P2, reading from a binary MRSW regular register after a write operation has occurred. The register initially contains the value '0'. Process P1 reads the register and observes the value '1', while process P2 reads the register at the same time and observes the value '0'. This inconsistency in the observed values by different processes violates the regularity property of the register.\n\nNext, let's consider a counterexample to prove that the resulting registers are not binary atomic:\n\nCounterexample for atomicity:\nSuppose we have two processes, P1 and P2, attempting to write to a binary MRSW regular register simultaneously. The register initially contains the value '0'. Process P1 writes the value '1' to the register, and process P2 writes the value '0' to the register at the same time. As a result, the final value of the register is non-deterministic and depends on the order in which the write operations are applied. This non-deterministic behavior violates the atomicity property of the register.\n\nIn conclusion, the transformation from binary MRSW safe registers to binary MRSW regular registers can lead to inconsistencies in values observed by different processes and non-deterministic behavior in write operations, thereby breaking the regularity and atomicity properties of the resulting registers. ///Answer: C"}
{"subject": "", "question": "Question: In a second-preimage attack \\dots?\n\nOptions:\nA. you are given a hash function $h$ and have to find two messages $m\\neq m'$ such that $h(m) = h(m')$.\nB. you are given access to $\\text{MAC}_K$ as a blackbox and a tag $\\text{MAC}_K(m)$ for a message $m$ and you have to recover $m$.\nC. you are given a hash function $h$ and a message $m$ and you have to find $m'\\neq m$ such that $h(m) = h(m')$.\nD. you are given a hash function $h$ and $c :=h(h(m))$ for a message $m$ and you have to find $m'$ such that $h(h(m'))=c$.\n", "answer": "Explanations: The correct option for a second-preimage attack is: \"you are given a hash function $h$ and have to find two messages $m\\neq m\\'$ such that $h(m) = h(m\\')$.\"\n\nJustification:\nIn a second-preimage attack, the goal is to find a different input message that hashes to the same output hash value as a given message. This is different from a collision attack, where the goal is to find any two messages that hash to the same value. In a second-preimage attack, the attacker is specifically targeting a particular message and looking for another message that produces the same hash value. ///Answer: A"}
{"subject": "", "question": "Question: Select which statements are true regarding SCFGs.A penalty will be applied for any incorrect answers.\n\nOptions:\nA. The sum over all the probabilities of the rules of a SCFG that share the\u00a0same left hand side\u00a0should sum up to 1.\nB. The\u00a0probabilities of lexical rules of a SCFG\u00a0correspond to\u00a0emission probabilities of HMMs\u00a0for PoS tagging.\nC. The probabilities of all parse trees in a SCFG sums up to 1.\nD. A stochastic grammar is consistent if the sum of all parse trees is equal to 1.\nE. Probabilities of parse trees can be used to compare parse trees from\u00a0any two different SCFGs.\nF. The inside-outside algorithm is used to derive the stochastic coefficients of a SCFG in a supervised manner.\n", "answer": "Explanations: Answer: B\n\nExplanation: \nA) This statement is not true. The sum over all the probabilities of the rules of a SCFG that share the same left-hand side does not have to sum up to 1. The probabilities of rules for a given non-terminal symbol (left-hand side) in a SCFG do not necessarily have to sum up to 1.\nB) This statement is true. In a SCFG, the probabilities of lexical rules correspond to emission probabilities in Hidden Markov Models (HMMs) used for Part-of-Speech (PoS) tagging. This connection allows for the use of SCFGs in various Natural Language Processing tasks.\nC) This statement is not true. The probabilities of all parse trees in a SCFG do not necessarily sum up to 1. Each parse tree has its own probability, and the sum of probabilities of all possible parse trees may not equal 1.\nD) This statement is not true. A stochastic grammar being consistent does not depend on the sum of probabilities of all parse trees being equal to 1. Consistency of a stochastic grammar is evaluated based on other criteria such as the ability to generate valid structures and probabilities.\nE) This statement is not true. Probabilities of parse trees in SCFGs are specific to the grammar they belong to and cannot be directly compared between different SCFGs unless they share the same underlying structure and parameters.\nF) This statement is not true. The inside-outside algorithm is used in unsupervised learning to estimate the production probabilities of rules in a SCFG from a given set of parse trees. It is not a supervised learning algorithm for deriving stochastic coefficients. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following functions reaches a global maximum on the set $I$? (Note that $[.,.]$ and $(.,.)$ denote closed and open intervals respectively)\n                    $f_1(x) = -x^4$, $I = [-5,5]$\n\t\t            $f_2(x) = \u0007rccos(x)$, $I = (-1,1)$\n\t\t            $f_3(x) = x \\exp(-x)$, $I = (-\\infty, 0)$\n\t\t            $f_4(x) = \\sin(\\cos(x))\\sin(x)$, $I= \\R_+$?\n\nOptions:\nA. $f_1, f_2, f_3, f_4\nB. $f_1, f_4$\nC. $f_1, f_3, f_4$\nD. $f_1, f_2, f_4$\n", "answer": "Explanations: To determine which of the functions reaches a global maximum on the given interval, we need to analyze each function individually.\n\n1. $f_1(x) = -x^4$, $I = [-5,5]$:\nThe function $f_1(x) = -x^4$ is a polynomial function. Since $f_1(x)$ is a continuous function on the closed interval $[-5,5]$, by the Extreme Value Theorem, it must attain both a global maximum and a global minimum on the interval. In this case, $f_1(x)$ reaches a global maximum at $x = -5$ and a global minimum at $x = 5$. Therefore, $f_1(x)$ reaches a global maximum on the set $I$.\n\n2. $f_2(x) = \\arccos(x)$, $I = (-1,1)$:\nThe function $f_2(x) = \\arccos(x)$ is the inverse cosine function. The range of $\\arccos(x)$ is $[0, \\pi]$ for $x \\in [-1, 1]$. Since $\\arccos(x)$ is strictly decreasing on $(-1, 1)$, its global maximum occurs at the left endpoint $x = -1$, which is $\\pi$. Therefore, $f_2(x)$ does not reach a global maximum on the set $I$.\n\n3. $f_3(x) = x \\exp(-x)$, $I = (-\\infty, 0)$:\nThe function $f_3(x) = x \\exp(-x)$ is defined for $x \\in (-\\infty, 0)$. As $x$ approaches $-\\infty$, both $x$ and $\\exp(-x)$ approach $0$, so $f_3(x)$ approaches $0$. However, $f_3(x)$ does not have a global maximum on the interval $(-\\infty, 0)$ as it decreases indefinitely as $x$ approaches $-\\infty$.\n\n4. $f_4(x) = \\sin(\\cos(x))\\sin(x)$, $I= \\R_+$:\nThe function $f_4(x) = \\sin(\\cos(x))\\sin(x)$ is a periodic function. Since the interval $I = \\R_+$ is the set of positive real numbers, $f_4(x)$ does not have a global maximum on this interval. It oscillates between $-1$ and $1$ indefinitely.\n\nTherefore, the correct option is $\\boxed{\\text{B. } f_1, f_4}$, as only $f_1(x)$ and $f_4(x)$ reach a global maximum on their respective intervals. ///Answer: B"}
{"subject": "", "question": "Question: In general, what is true regarding Fagin's algorithm?\n\nOptions:\nA. It performs a complete scan over the posting files\nB. It provably returns the k documents with the largest aggregate scores\nC. Posting files need to be indexed by the TF-IDF weights\nD. It never reads more than (kn)\u00bd entries from a posting list\n", "answer": "Explanations: Fagin's algorithm is a technique used in information retrieval to find the top-k documents based on a query. Let's analyze the statements regarding Fagin's algorithm:\n\n1. It performs a complete scan over the posting files: This statement is true. Fagin's algorithm involves scanning through the posting files to find the relevant documents based on the query terms.\n\n2. It provably returns the k documents with the largest aggregate scores: This statement is true. Fagin's algorithm is designed to return the top-k documents with the highest scores based on the query.\n\n3. Posting files need to be indexed by the TF-IDF weights: This statement is not necessarily true. While TF-IDF weights are commonly used in information retrieval, Fagin's algorithm can work with various scoring mechanisms and does not specifically require TF-IDF indexing.\n\n4. It never reads more than (kn)^\u00bd entries from a posting list: This statement is true. Fagin's algorithm is efficient in that it only reads a square root of the total number of entries in the posting lists, which helps reduce the computational complexity of the algorithm.\n\nTherefore, the correct statements regarding Fagin's algorithm are:\n- It performs a complete scan over the posting files.\n- It provably returns the k documents with the largest aggregate scores.\n- It never reads more than (kn)^\u00bd entries from a posting list. ///Answer: B"}
{"subject": "", "question": "Question: Let $n \\geq 2$ be a positive integer, and $M$ a uniformly distributed binary message of length $2n$. Let $P_K(M)$ denote the one-time pad encryption of $M$ with key $K$. Let $K_1$ be a uniformly distributed binary key length $n$. Let $K_2$ be the complement of $K_1$. Let $K_3$ be the reverse of $K_1$. Let $K_i||K_j$ denote the concatenation of the two keys. Let $K_7$ be a key that is either equal to $K_2$ or $K_3$ with uniform probability. True or false: Encryption with the key $K_8 = ( K_1|| K_7 ) $, $P_{K_8}(M)$ provides perfect secrecy.\n\nOptions:\nA. False\nB. True\n", "answer": "Explanations: The encryption with the key $K_8 = ( K_1|| K_7 )$, denoted by $P_{K_8}(M)$, does not provide perfect secrecy. The keys $K_1$ and $K_7$ are randomly generated keys that do not guarantee perfect secrecy for the encryption process. The use of concatenation in forming the key $K_8$ may introduce vulnerabilities in the encryption scheme, making it susceptible to attacks.\n\nTherefore, the statement \"False\" is correct. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\emph{true} assertion among the followings:?\n\nOptions:\nA. Visual cryptography is perfectly secure (at an unreasonable cost).\nB. The Vernam cipher was invented by Kerckoff.\nC. Just like coding theory, cryptography usually faces random noise.\nD. Enigma has never been broken.\n", "answer": "Explanations: To identify the true assertion among the following options, let's analyze each statement:\n\n1. Visual cryptography is perfectly secure (at an unreasonable cost): This statement is not true as visual cryptography may not always be perfectly secure due to practical constraints. \\textbf{False}\n\n2. The Vernam cipher was invented by Kerckhoff: This statement is not true as the Vernam cipher was invented by Gilbert Vernam. \\textbf{False}\n\n3. Just like coding theory, cryptography usually faces random noise: This statement is generally true as both coding theory and cryptography may face challenges due to random noise or errors. \\textbf{True}\n\n4. Enigma has never been broken: This statement is not true as the Enigma machine used during World War II was eventually broken by cryptanalysts, including Alan Turing and his team at Bletchley Park. \\textbf{False}\n\nTherefore, the true assertion among the options is:\n3. Just like coding theory, cryptography usually faces random noise. ///Answer: C"}
{"subject": "", "question": "Question: You are given the task of choosing the parameters of a hash function. What value of the output will you recommend in order to be minimal and secure against second preimage attacks?\n\nOptions:\nA. 40 bits\nB. 80 bits\nC. 160 bits\nD. 320 bits\n", "answer": "Explanations: The recommended output size to be minimal and secure against second preimage attacks is 160 bits. \n\nEliminate:\n- 40 bits: Too short and insecure.\n- 80 bits: Insufficient for good security.\n- 320 bits: Overkill and unnecessarily large for most applications. ///Answer: C"}
{"subject": "", "question": "Question: Let the first four retrieved documents be N N R R, where N denotes a non-relevant and R a relevant document. Then the MAP (Mean Average Precision) is:?\n\nOptions:\nA. 1/2\nB. 5/12\nC. 3/4\nD. 7/24\n", "answer": "Explanations: To calculate the Mean Average Precision (MAP), we need to first find the Average Precision (AP) for each query and then take the mean of these average precisions.\n\nGiven that the first four retrieved documents are N N R R, where N denotes a non-relevant document and R a relevant document, the relevant documents are at positions 3 and 4.\n\nTo find the Average Precision (AP) for this query, we calculate the precision at each relevant document position and then take the average.\n\nPrecision at position 1: 0/1 = 0\nPrecision at position 2: 0/2 = 0\nPrecision at position 3: 1/3 = 1/3\nPrecision at position 4: 2/4 = 1/2\n\nAverage Precision (AP) = (0 + 0 + 1/3 + 1/2) / 2 = 5/12\n\nTherefore, the correct answer is:\n2. 5/12 ///Answer: B"}
{"subject": "", "question": "Question: Which of the following statements about the $\\mathrm{SVD}$ of an $N \\times D$ matrix $\\mathbf{X}$ are correct?\n\nOptions:\nA. We can compute the singular values of $\\mathbf{X}$ by computing the eigenvalues of $\\mathbf{X X}^{\\top}$. This has complexity $O\\left(N^{3}\\right)$.\nB. We can compute the singular values of $\\mathbf{X}$ by computing the eigenvalues of $\\mathbf{X X}^{\\top}$. This has complexity $O\\left(D^{3}\\right)$.\nC. We can compute the singular values of $\\mathbf{X}$ by computing the eigenvalues of $\\mathbf{X}^{\\top} \\mathbf{X}$. This has complexity $O\\left(N^{3}\\right)$.\nD. We can compute the singular values of $\\mathbf{X}$ by computing the eigenvalues of $\\mathbf{X}^{\\top} \\mathbf{X}$. This has complexity $O\\left(D^{3}\\right)$.\nE. We can compute the singular values of $\\mathbf{X}$ by computing the eigenvalues of $\\mathbf{X} \\mathbf{X}^{\\top}$ if only if $\\mathbf{X}$ is a square matrix. This has complexity $O\\left(D^{3}\\right)=O\\left(N^{3}\\right)$.\n", "answer": "Explanations: Another correct statement about the SVD of an $N \\times D$ matrix $\\mathbf{X}$ is that we can compute the singular values of $\\mathbf{X}$ by computing the eigenvalues of $\\mathbf{X}^{T} \\mathbf{X}$. This has complexity $O(D^{3})$. This is because the number of columns in $\\mathbf{X}$ influences the complexity of the computation, and therefore the complexity is $O(D^{3})$. ///Answer: D."}
{"subject": "", "question": "Question: Let $n$ be any positive integer. Three of the following assertions are equivalent. Tick the remaining one.\n\nOptions:\nA. $\\mathbb{Z}_n$ is a field.\nB. $\\varphi(n)=n-1 $, where $\\varphi$ denotes the Euler totient function.\nC. $n$ is a prime power.\nD. Any element $x \\in \\mathbb{Z}_n \\backslash \\{0\\}$ is invertible.\n", "answer": "Explanations: $\\mathbb{Z}_n$ is a field.\n\nExample: Let's consider $n = 5$. The set $\\mathbb{Z}_5 = \\{0, 1, 2, 3, 4\\}$ under addition and multiplication modulo 5 forms a field. Each non-zero element in $\\mathbb{Z}_5$ has an inverse such that multiplication is commutative, associative, has distributive property, and has an identity element. ///Answer: A"}
{"subject": "", "question": "Question: Consider a binary classification problem with classifier $f(\\mathbf{x})$ given by $$ f(\\mathbf{x})= \\begin{cases}1, & g(\\mathbf{x}) \\geq 0 \\\\ -1, & g(\\mathbf{x})<0\\end{cases} $$ and $\\mathbf{x} \\in \\mathbb{R}^{6}$. Consider a specific pair $(\\mathbf{x}, y=1)$ and assume that $g(\\mathbf{x})=8$. In particular this means that this point is classified correctly by $f$. Assume further that we have computed the gradient of $g$ at $\\mathbf{x}$ to be $\\nabla_{\\mathbf{x}} g(\\mathbf{x})=(+1,-2,+3,-4,+5,-6)$. You are allowed to make one step in order to (hopefully) find an adversarial example. In the following four questions, assume $\\epsilon=1$. Which offset $\\delta$ with $\\|\\delta\\|_{1} \\leq 1$ yields the smallest value for $g(\\mathbf{x}+\\delta)$, assuming that $g$ is (locally) linear?\n\nOptions:\nA. $(0,0,0,0,0,1)$\nB. $(+1,-1,+1,-1,+1,-1)$\nC. $(+1,-2,+3,-4,+5,-6)$\nD. $(+1,+1,+1,+1,+1,+1)$\nE. $(-1,+2,-3,+4,-5,+6)$\nF. $(0,0,0,0,0,1)$\nG. $(-1,+1,-1,+1,-1,+1)$\nH. $(-1,-1,-1,-1,-1,-1)$\n", "answer": "Explanations: To find an adversarial example, we need to perturb the input $\\mathbf{x}$ in such a way that the classification changes. Given that the current point $\\mathbf{x}$ is correctly classified as $y=1$ with $g(\\mathbf{x})=8$, we want to change this classification by adding a perturbation to $\\mathbf{x}$.\n\nThe concept of adversarial examples in machine learning involves finding small perturbations to the input data that cause a model to misclassify the example. In this case, we want to find an offset $\\delta$ that results in the smallest value for $g(\\mathbf{x}+\\delta)$, assuming that $g$ is locally linear around $\\mathbf{x}$.\n\nGiven that $\\nabla_{\\mathbf{x}} g(\\mathbf{x})=(+1,-2,+3,-4,+5,-6)$, we can view this as the gradient of $g$ at $\\mathbf{x}$. To minimize $g(\\mathbf{x}+\\delta)$, we need to move in the direction opposite to the gradient. This means we should choose the offset $\\delta$ that aligns with the negative of the gradient.\n\nAmong the options provided, the one that aligns with the negative gradient is $(-1,+2,-3,+4,-5,+6)$. By adding this perturbation to $\\mathbf{x}$, we are essentially moving in the direction that decreases the value of $g(\\mathbf{x})$ the most, potentially leading to a change in classification.\n\nTherefore, the correct choice is: $(-1,+2,-3,+4,-5,+6)$. ///Answer: E"}
{"subject": "", "question": "Question: Which is true?\n\nOptions:\nA. Hand-written patterns are in general more precise than classifiers\nB. Hand-written patterns cannot exploit syntactic features\nC. Supervised classifiers do not require any human input\nD. Supervised classifiers can only detect typed statements\n", "answer": "Explanations: The true statement is: Hand-written patterns cannot exploit syntactic features. Hand-written patterns are generally less precise than classifiers because they rely on manual rules and heuristics rather than statistical learning. Supervised classifiers do require human input in the form of labeled training data to learn patterns and make predictions. Supervised classifiers can detect various patterns and features beyond just typed statements. ///Answer: B"}
{"subject": "", "question": "Question: Tick the \\textbf{false} statement. Enigma ...\n\nOptions:\nA. was broken mainly because of design flaws in the patented documents.\nB. was used by the German armies in World War 2.\nC. relies on Kerchkoffs's Principle.\nD. could be plugged into a radio transmitter.\n", "answer": "Explanations: 1. Enigma was not broken mainly because of design flaws in the patented documents. The breaking of Enigma was primarily due to the efforts of codebreakers like Alan Turing and the team at Bletchley Park who used mathematical and analytical methods to decrypt the messages.\n\n2. Enigma was indeed used by the German armies in World War 2. This statement is true.\n\n3. Enigma does rely on Kerchkoffs's Principle, which states that the security of a cryptographic system should not depend on the secrecy of the algorithm but rather on the secrecy of the key.\n\n4. Enigma could not be plugged into a radio transmitter. Enigma machines were standalone devices that were used to encrypt and decrypt messages, not directly connected to radio transmitters.\n\nTherefore, the false statement is 1. Enigma was not broken mainly because of design flaws in the patented documents. ///Answer: A"}
{"subject": "", "question": "Question: Tick the \\textbf{incorrect} assumption. A language $L$ is in NP if\\dots?\n\nOptions:\nA. $x \\in L$ can be decided in polynomial time.\nB. $x \\in L$ can be decided in polynomial time given a witness $w$.\nC. $L$ is NP-hard.\nD. $L$ (Turing-)reduces to a language $L_2$ with $L_2$ in $P$, i.e., if there is a polynomial deterministic Turing machine which recognizes $L$ when plugged to an oracle recognizing $L_2$.\n", "answer": "Explanations: Let's analyze each option regarding the incorrect assumption for a language \\( L \\) to be in NP:\n\n1. \\( x \\in L \\) can be decided in polynomial time: This statement is correct. For a language to be in NP, membership should be verifiable in polynomial time.\n\n2. \\( x \\in L \\) can be decided in polynomial time given a witness \\( w \\): This statement is correct. NP is defined based on the existence of polynomial-time verifiable solutions or witnesses.\n\n3. \\( L \\) is NP-hard: This statement is incorrect. NP-hardness is a property of languages to which all problems in NP can be reduced to in polynomial time.\n\n4. \\( L \\) (Turing-)reduces to a language \\( L_2 \\) with \\( L_2 \\) in P: This statement is correct. If a language \\( L \\) can be reduced to a language \\( L_2 \\) in P, then \\( L \\) is in NP.\n\nTherefore, the incorrect assumption is:\n3. \\( L \\) is NP-hard. ///Answer: C"}
{"subject": "", "question": "Question: $\\mathbb{Z}_{37}^*$ denotes ...\n\nOptions:\nA. a field.\nB. an additive group.\nC. a multiplicative group.\nD. a ring.\n", "answer": "Explanations: To answer this question thoroughly, let's delve into the definitions and concepts involved:\n\n1. **Definition of $\\mathbb{Z}_{37}^*$:**\n   - $\\mathbb{Z}_{37}^*$ refers to the set of integers modulo 37 that are relatively prime to 37. In other words, it consists of all integers from 1 to 36 that do not share any common factors with 37 except 1.\n\n2. **Field, Additive Group, Multiplicative Group, and Ring:**\n   - **Field:** A field is a set where addition, subtraction, multiplication (excluding division by 0), and division (excluding division by 0) are well-defined operations. It satisfies properties like closure, associativity, commutativity, distributivity, existence of additive and multiplicative inverses (except for 0), and the existence of additive and multiplicative identities.\n   - **Additive Group:** An additive group is a set equipped with an addition operation that is closed (the sum of any two elements is in the set), associative, has an identity element (0), and each element has an additive inverse.\n   - **Multiplicative Group:** A multiplicative group is a set with a multiplication operation that is closed, associative, has an identity element (1), and each non-zero element has a multiplicative inverse.\n   - **Ring:** A ring is a set with two operations, addition, and multiplication, that satisfy closure, associativity, distributivity, and have an identity element for addition. Rings may lack multiplicative inverses for all non-zero elements.\n\n3. **Analysis:**\n   - In the context of $\\mathbb{Z}_{37}^*$, the set consists of integers relatively prime to 37, forming a multiplicative group under modulo 37 multiplication.\n   - Since $\\mathbb{Z}_{37}^*$ includes only numbers coprime to 37, it meets the criteria of a multiplicative group by having closure, associativity, an identity element (1), and every element (except 0) possessing a multiplicative inverse within the set.\n\n4. **Conclusion:**\n   - Therefore, $\\mathbb{Z}_{37}^*$ is best described as a multiplicative group due to its adherence to the group properties under multiplication within the set of integers modulo 37 that are relatively prime to 37.\n   - Understanding the algebraic structures and properties of sets like $\\mathbb{Z}_{37}^*$ is crucial in various fields, including computer science, cryptography, and number theory, where modular arithmetic plays a significant role.\n\nIn summary, $\\mathbb{Z}_{37}^*$ denotes a multiplicative group, showcasing the interplay between number theory and algebraic structures within the realm of computer science and mathematics. ///Answer: C"}
{"subject": "", "question": "Question: L'unit\u00e9 du flux magn\u00e9tique est en : (plusieurs r\u00e9ponses possibles)?\n\nOptions:\nA. volt/seconde (V/s)\nB. volt*seconde (V*s)\nC. Weber (Wb)\nD. Tesla (T)\n", "answer": "Explanations: Path coverage is a type of code coverage metric that measures the percentage of unique paths through the code that are executed by a test suite. While achieving a high level of code coverage is generally a good practice for ensuring the quality and reliability of software, setting a strict threshold of 80% path coverage for all new code may not be the most effective approach.\n\nFirstly, achieving 100% path coverage is often not feasible, especially in complex software systems with a large number of possible paths. In some cases, reaching 80% path coverage may require a significant amount of additional testing effort without necessarily providing a proportionate increase in bug detection.\n\nInstead of focusing solely on path coverage, the CTO could consider a more holistic approach to improving software quality. This could include a combination of techniques such as unit testing, integration testing, code reviews, static code analysis, and continuous integration/continuous deployment (CI/CD) practices.\n\nOne alternative suggestion is to prioritize testing critical paths and functionalities that are more likely to contain bugs or have a higher impact on the system. This targeted approach can help ensure that the most important parts of the code are thoroughly tested, while still allowing for flexibility in terms of overall code coverage.\n\nUltimately, while path coverage can be a useful metric for assessing test coverage, it should be considered as part of a broader testing strategy rather than as a strict requirement for all new code. By implementing a balanced approach to testing and quality assurance, the CTO can help minimize bugs in the product while also optimizing development efficiency and resource allocation. ///Answer: C."}
{"subject": "", "question": "Question: The blockcipher AES \\ldots?\n\nOptions:\nA. is based on a Feistel scheme.\nB. can have a variable block size.\nC. should replace DES, as it is more secure than DES.\nD. has a round function that consists of the following operations: Expansion, AddBytes, ShiftRows, MixColumns.\n", "answer": "Explanations: Answer: The blockcipher AES has a round function that consists of the following operations: Expansion, AddBytes, ShiftRows, MixColumns.\n\nExplanation: In the AES encryption process, a round function is applied multiple times to transform the plaintext into ciphertext. This round function consists of several key operations. \n\n1. Expansion: In this step, the input data is expanded to create a larger matrix by adding additional bytes. This expansion increases the complexity of the encryption process and enhances security.\n\n2. AddBytes: In this operation, each byte of the input data is combined with a corresponding byte from the encryption key using a bitwise XOR operation. This helps to introduce randomness and variability into the encryption process.\n\n3. ShiftRows: In this operation, the bytes in each row of the matrix are shifted cyclically to the left. This permutation operation ensures that the data is mixed and shuffled, making it more resistant to attacks such as differential cryptanalysis.\n\n4. MixColumns: In this step, the columns of the matrix are mixed by applying a linear transformation that combines the values in each column. This operation further confuses the relationship between the input data and the encryption key, increasing the overall security of the encryption process.\n\nBy understanding and implementing these operations in the round function of AES, you can successfully encrypt data and protect it from unauthorized access and cyber threats. This thorough understanding of the AES encryption process is essential for ensuring the security of sensitive information in various applications and systems. ///Answer: D"}
{"subject": "", "question": "Question: What is WRONG\u00a0regarding the Transformer model?\n\nOptions:\nA. It uses a self-attention mechanism to compute representations of the input and output.\nB. Its computation cannot be parallelized compared to LSTMs and other sequential models.\nC. Its complexity is quadratic to the input size.\nD. It captures the semantic context of the input.\n", "answer": "Explanations: The correct statement is: \\\"Its computation cannot be parallelized compared to LSTMs and other sequential models.\\\"\\n\\nJustification: This statement is actually incorrect. The Transformer model is known for its ability to be highly parallelized, unlike traditional sequential models like LSTMs. This parallelization is achieved through the use of self-attention mechanisms, allowing the model to process all tokens in the input sequence simultaneously. ///Answer: B"}
{"subject": "", "question": "Question: Let $n$ be an integer such that $n\\geq 2$ and let  $A \\in \\R^{n\times n}$, and $xv \\in \\R^n$, consider the function $f(xv) = xv^\top A xv$ defined over $\\R^n$. Which of the following is the gradient of the function $f$? ?\n\nOptions:\nA. $2 xv^\top A$\nB. $2Axv$\nC. $A^\top xv + Axv$\nD. $2A^\top xv$\n", "answer": "Explanations: To find the gradient of the function $f(xv) = xv^T A xv$, we need to compute the partial derivatives of $f$ with respect to each component of $xv$. \n\nGiven $f(xv) = xv^T A xv$, let's first expand the function using matrix multiplication:\n\n$f(xv) = (x_1, x_2, \\ldots, x_n) \\begin{pmatrix} a_{11} & a_{12} & \\ldots & a_{1n} \\\\ a_{21} & a_{22} & \\ldots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{n1} & a_{n2} & \\ldots & a_{nn} \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}$\n\nExpanding this further, we get:\n\n$f(xv) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{ij} x_i x_j$\n\nNow, let's compute the partial derivative of $f$ with respect to $x_k$:\n\n$\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{ij} x_i x_j \\right)$\n\n$\\frac{\\partial f}{\\partial x_k} = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{ij} \\frac{\\partial}{\\partial x_k} (x_i x_j)$\n\n$\\frac{\\partial f}{\\partial x_k} = \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{ij} \\left( \\delta_{ik} x_j + x_i \\delta_{jk} \\right)$  [using product rule]\n\n$\\frac{\\partial f}{\\partial x_k} = \\sum_{i=1}^{n} a_{ik} x_i + \\sum_{j=1}^{n} a_{kj} x_j$\n\nThis is a summation over $x_i$ and $x_j$, so we can rewrite this as matrix multiplication:\n\n$\\frac{\\partial f}{\\partial x_k} = \\sum_{i=1}^{n} a_{ik} x_i + \\sum_{j=1}^{n} a_{kj} x_j = (Ax)_k + (A^T x)_k$\n\nTherefore, the gradient of the function $f(xv) = xv^T A xv$ is $\\nabla f = 2Ax$. \n\nHence, the correct choice from the given options is $2Axv$. ///Answer: B"}
{"subject": "", "question": "Question: Let $C_1$, $C_2$ and $C^*$ be three independent random permutations over a set $A$, $C^*$ being uniformaly distributed. Tick the \\textbf{false} assertion.\n\nOptions:\nA. $[C_2\\circ C_1]^n=[C_1]^n\\circ [C_2]^n$\nB. $[C^*]^n\\times [C_2]^n=[C^*]^n$\nC. $[C_1]^n\\times [C^*]^n=[C^*]^n$\nD. $[C^*]^n$ is neutral for $x$\n", "answer": "Explanations: The false assertion is: $[C^*]^n$ is neutral for $x$.\n\nTo see why, let's break down each of the given statements:\n\n1. $[C_2\\circ C_1]^n=[C_1]^n\\circ [C_2]^n$: This is true by the definition of composition of permutations, as the composition of $C_1$ and $C_2$ raised to the power $n$ is the same as raising $C_1$ and $C_2$ to the power $n$ individually and then composing them.\n\n2. $[C^*]^n\\times [C_2]^n=[C^*]^n$: This is true because $C^*$ is uniformly distributed, so raising it to the power $n$ does not change its distribution, and multiplying it by $[C_2]^n$ does not affect the distribution of $C^*$.\n\n3. $[C_1]^n\\times [C^*]^n=[C^*]^n$: This is true for the same reason as statement 2, as $C^*$ being uniformly distributed remains unchanged when multiplied by $[C_1]^n$.\n\n4. $[C^*]^n$ is neutral for $x$: This statement is false. $[C^*]^n$ is not necessarily neutral for $x$ because $C^*$ is a randomly chosen permutation, and raising it to the power $n$ does not guarantee that it will be neutral for any specific element $x$ in $A$. \n\nThus, the false statement is $[C^*]^n$ is neutral for $x$. ///Answer: D"}
{"subject": "", "question": "Question: We saw in class that we can quickly decrease the spatial size of the representation using pooling layers. Is there another way to do this without pooling?\n\nOptions:\nA. Yes, by increasing the amount of padding.\nB. Yes, by increasing the stride.\nC. Yes, by increasing the number of filters.\nD. No, pooling is necessary.\n", "answer": "Explanations: To decrease the spatial size of the representation without using pooling layers, let's analyze each possible answer:\n\n1. Yes, by increasing the amount of padding:\n   Increasing the amount of padding will only add zeros around the input data to maintain the spatial dimensions of the output. It does not inherently decrease the spatial size of the representation.\n\n2. Yes, by increasing the stride:\n   Increasing the stride will result in the output volume having a reduced spatial size, as the filter will move across the input with larger steps, effectively reducing the number of output units. Therefore, increasing the stride is a correct way to decrease the spatial size without using pooling layers.\n\n3. Yes, by increasing the number of filters:\n   Increasing the number of filters in a convolutional layer will not directly lead to a decrease in the spatial size of the representation. It will only increase the depth of the output volume.\n\n4. No, pooling is necessary:\n   While pooling layers are commonly used to decrease spatial size in convolutional neural networks, they are not the only way to achieve this. As discussed in the second point about increasing the stride, adjusting the stride parameter can also reduce the spatial dimensions of the output.\n\nBased on the analysis above, the correct option is: Yes, by increasing the stride. ///Answer: B"}
{"subject": "", "question": "Question: Let $\\mathcal{C}$ be a binary $(5,2)$ linear code with generator matrix  \begin{equation*}    G = \begin{pmatrix}     1 &0 &1 &0 &1 \\      0 &1 &0 &1 &1      \\end{pmatrix}      \\end{equation*}  and consider a minimum-distance decoder obtained by choosing the coset leaders of the standard array of $\\mathcal{C}$ so that the error probability is minimized under a binary symmetric channel with bit-flip probability $\\epsilon < \frac{1}{2}$. True or false: The word $00100$ must be one of the coset leaders.\n\nOptions:\nA. True\nB. False\n", "answer": "Explanations: False\n\nExplanation: In a minimum-distance decoder, the word $00100$ does not necessarily have to be one of the coset leaders. The coset leaders are chosen based on the minimum distance between codewords in the code $\\mathcal{C$. The minimum distance of a binary $(n,k)$ linear code is the minimum number of positions in which any two distinct codewords differ. In this case, the code $\\mathcal{C}$ is a binary $(5,2)$ linear code, so the minimum distance must be at least 3. The word $00100$ may or may not have a distance of at least 3 from the other codewords in $\\mathcal{C$. Therefore, it is not a requirement for $00100$ to be one of the coset leaders in the minimum-distance decoder. ///Answer: B"}
{"subject": "", "question": "Question: Consider 3 regular expressions \\(A\\), \\(B\\), and \\(C\\), such that:the sets of strings recognized by each of the regular expressions is non empty;the set of strings recognized by \\(B\\) is included in the set of strings recognized by \\(A\\);some strings are recognized simultaneously by \\(A\\) and by \\(C\\); andno string is recognized simultaneously by \\(B\\) and \\(C\\).Which of the following statements are true?(where, for a regular expression \\(X\\),\u00a0 \\((X)\\) denotes the transducer which associates every string recognized by \\(X\\) to itself)(Penalty for wrong ticks)?\n\nOptions:\nA. Any string recognized by \\(A\\) but not by \\(B\\) is a left string in an association recognized by the transducer \\((A)(C)\\)\nB. Any string recognized by \\(B\\) is (at least) associated to itself by the transducer \\(A\\otimes B\\)\nC. \\((A\\otimes B)\\circ (C)\\) recognizes a non empty set of string associations\nD. \\((B\\otimes A)\\circ (C)\\) recognizes a non empty set of string associations\n", "answer": "Explanations: Given that the set of strings recognized by \\(B\\) is included in the set of strings recognized by \\(A\\), we can say that any string recognized by \\(B\\) is also recognized by \\(A\\). This means that any string recognized by \\(B\\) is associated to itself by the transducer \\((A)\\). Therefore, statement 2 is true.\n\nSince some strings are recognized simultaneously by \\(A\\) and by \\(C\\), the transducer \\((A)(C)\\) recognizes a non-empty set of string associations. This means that any string recognized by \\(A\\) is associated with itself by the transducer \\((A)(C)\\), regardless of whether it is recognized by \\(B\\) or not. Therefore, statement 1 is true.\n\nGiven that no string is recognized simultaneously by \\(B\\) and \\(C\\), the transducer \\((B)(C)\\) does not recognize any string associations. This means that no string recognized by \\(B\\) is associated with itself by the transducer \\((B)(C)\\). Therefore, statements 3 and 4 are false.\n\nTherefore, the correct statements are 1 and 2. ///Answer: B"}
{"subject": "", "question": "Question: Which of the following in Linux x86-64 assembly snippets can be used as a\n    gadget AND can be chained with more gadgets (e.g., in a ROP/JOP chain)?\n\nOptions:\nA. \\texttt{pop rbx; pop rax; jmp rax}\nB. \\texttt{pop rbx; pop rax; ret}\nC. \\texttt{mov eax, -1; call rax}\nD. \\texttt{xor rbx, rbx; xor rbx, -1; push rbx; ret}\n", "answer": "Explanations: The most appropriate answer is b) \\\\texttt{pop rbx; pop rax; ret}.\\n\\nExplanation:\\nIn a ROP/JOP chain, gadgets are small pieces of code that end with a return instruction (\\\\texttt{ret}). This allows the chain to continue executing by returning to the next gadget in the chain. The gadget \\\\texttt{pop rbx; pop rax; ret} is a valid gadget because it loads values into the registers \\\\texttt{rbx} and \\\\texttt{rax} and then returns, allowing for chaining with more gadgets.\\n\\na) \\\\texttt{pop rbx; pop rax; jmp rax} ends with a jump instruction instead of a return, so it cannot be easily chained with more gadgets.\\nc) \\\\texttt{mov eax, -1; call rax} does not end with a return instruction, so it cannot be used as a gadget in a ROP/JOP chain.\\nd) \\\\texttt{xor rbx, rbx; xor rbx, -1; push rbx; ret} does not load values into the registers \\\\texttt{rbx} and \\\\texttt{rax}, so it cannot be used as a gadget in a ROP/JOP chain. ///Answer: B"}
{"subject": "", "question": "Question: Combien vaut la perm\u00e9abilit\u00e9 magn\u00e9tique du vide ? (une seule r\u00e9ponse possible)?\n\nOptions:\nA. 4*\u03c0*10\nB. 2*\u03c0*10\nC. 4*\u03c0*10\nD. 2*\u03c0*10\n", "answer": "Explanations: To find the minimum distance of $\\mathcal{C}'$, we need to consider the minimum number of positions at which any two distinct codewords of $\\mathcal{C}'$ differ. \n\nSince each codeword of $\\mathcal{C}'$ is a repetition of a codeword of $\\mathcal{C}$, the minimum distance of $\\mathcal{C}'$ will be at least the minimum distance of $\\mathcal{C}$. \n\nThe minimum distance of a Reed-Solomon code $\\mathcal{C}$ is $2n - 2k + 1$. \n\nTherefore, the correct answer is:\n3. $2n-2k+1$ ///Answer: C"}
